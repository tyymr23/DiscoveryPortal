CREATE DATABASE  IF NOT EXISTS `portal` /*!40100 DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci */ /*!80016 DEFAULT ENCRYPTION='N' */;
USE `portal`;
-- MySQL dump 10.13  Distrib 8.0.38, for Win64 (x86_64)
--
-- Host: localhost    Database: portal
-- ------------------------------------------------------
-- Server version	8.0.39

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!50503 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `admins`
--

DROP TABLE IF EXISTS `admins`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `admins` (
  `username` varchar(45) NOT NULL,
  `password` varchar(100) NOT NULL,
  PRIMARY KEY (`username`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `admins`
--

LOCK TABLES `admins` WRITE;
/*!40000 ALTER TABLE `admins` DISABLE KEYS */;
INSERT INTO `admins` VALUES ('VT_ADMIN','$2a$12$tnn6nhdwv7hQM9TQkUHprOrOWyoU9swoH78HTmrfbtRWi4e9jHB/O');
/*!40000 ALTER TABLE `admins` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `clients`
--

DROP TABLE IF EXISTS `clients`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `clients` (
  `username` varchar(45) NOT NULL,
  `first_name` varchar(45) NOT NULL,
  `last_name` varchar(45) NOT NULL,
  `password` varchar(100) NOT NULL,
  `owned_projects` json DEFAULT NULL,
  PRIMARY KEY (`username`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `clients`
--

LOCK TABLES `clients` WRITE;
/*!40000 ALTER TABLE `clients` DISABLE KEYS */;
INSERT INTO `clients` VALUES ('client','client','client','$2b$12$.vLQkbmoD5XuHOUdbw90jOHCZazmt8YKj.HHHh4hZuZjSRk42I4fG','[]');
/*!40000 ALTER TABLE `clients` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `projects`
--

DROP TABLE IF EXISTS `projects`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `projects` (
  `project_id` int NOT NULL AUTO_INCREMENT,
  `project_name` varchar(255) DEFAULT NULL,
  `abstract` text,
  `keywords` json DEFAULT NULL,
  `publisher` varchar(255) DEFAULT NULL,
  `semester_id` int DEFAULT NULL,
  `description` text,
  `client_id` int DEFAULT NULL,
  `deliverables` text,
  `impact` text,
  `skills` text,
  `project_type` text,
  PRIMARY KEY (`project_id`)
) ENGINE=InnoDB AUTO_INCREMENT=295 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

CREATE TABLE `project_files` (
  `id` INT AUTO_INCREMENT PRIMARY KEY,
  `project_id` INT NOT NULL,
  `file_name` VARCHAR(255) NOT NULL,
  `project_file` LONGBLOB NOT NULL,
  FOREIGN KEY (`project_id`) REFERENCES projects(`project_id`) ON DELETE CASCADE
);

--
-- Dumping data for table `projects`
--

LOCK TABLES `projects` WRITE;
/*!40000 ALTER TABLE `projects` DISABLE KEYS */;
INSERT INTO `projects` VALUES (1,'CS4624 Projects Discovery Portal','CS4624 is one of many capstone courses a student within the CS curriculum is able to take. The multimedia and hypertext course digs into the diverse range of multimedia content such as images, audio, video, and any information retrieval and access relating to it. With this comes the capstone project which is a semester long project given to us students to allow a display of mastery within our discipline. It has been a pleasure to have Dr. Farag and Vedant Shah guide and assist us with the project. An insight into real-world applications as well as a diverse approach to different problems has allowed us to grow as both people and developers.\nThe current discovery portal for CS4624 student projects serves as a platform for students working within the course to submit and hold their projects. Details included within the pages on the discovery portal consist of abstract, date, author, presentation, final report, source code, and collections. Additionally, the discovery portal contains filtering features to allow users to specifically search for any project dependent on; recent, issue date, author, title, subject, content type, and department. This allows teachers to easily access desired projects as well as a safe holding for semester long projects that students worked hard on.\nWith this comes the purpose of the project. After reviewing the functionality and appearance of the discovery portal, there were many things that needed to be improved on. The first very noticeable issue was that the search features did not either function properly or at all. For starters, the ‘By Issue Date’ filter would not allow filtering with either month or year. It was required that both were specified along with the open search requiring an exact formatting of date (xx-xx-xxxx). The ‘By Author’ filter expected that the full name was typed out, including the comma separating the first and last name. All open search features expected a prompt that would produce an exact match. It also came to our attention that some features produced over a thousand categories for only 269 projects. This should never be the case as the purpose of filtering is to narrow the search for projects.\nAfter analyzing the discovery portal, our main focus became improving upon the search and filtering features. This was something that would require us to completely recreate the discovery portal due to existing source code being unavailable. With this, we first needed to create a front-end and back-end that would relay any information requests to have a web page display. To replicate the discovery portal further, we also implemented an authentication aspect. Accounts would be divided into ‘admin’, ‘professor’, and ‘user’, each having distinct permissions on what they are able to insert, delete, and modify. Following this was the development of our database to store all of the projects’ files as well as a schema that the search and filtering features would utilize. Finally, we implemented a search api for the back-end to access, a completed schema for each project, and created a function search and filter function. Upon completion, we hope to provide future CS4624 students and staff with a more convenient tool to guide them in their journey of completing their capstone projects.','[\"Discovery Portal\", \"CS4624\", \"Multimedia\", \"Hypertext\", \"Capstone\", \"Discovery Page\", \"Webpage\", \"Filter\", \"Functionality\"]','Virginia Tech',18,'This project was done in fulfilling a requirement for the CS 4624 Capstone Course. The Team is tasked with creating a web application that displays information and files of all previous projects that were originally uploaded to the VTechWorks website under the tag Multimedia, Hypertext & Information Access. This project is to be usable on all OS types and support user authentication and differing permission levels and accesses based on said authentication. Furthermore the web application should allow for users to search the database for projects based on specified search criteria. It will also allow users of higher permissions to create and upload projects to the database based on a form that requests information and files regarding the project.',NULL,NULL,NULL,NULL,NULL),(2,'Crisis Events Information Extraction','Unfortunately, crises occur quite frequently throughout the world. In an increasingly digital age, where most news outlets post articles about events online, there are often tens or even hundreds of articles about the same event. Although the information found in each article is often similar, some information may be specific to a certain article or news outlet. And, as each news outlet usually writes a lengthy article for each crisis event that happens, it can be hard to quickly locate and learn the basic, important information about a given crisis event.\nThis web app project aims to expedite this lengthy process by consolidating any number of articles about a crisis event into who, what, where, when, and how (WWWWH). This information extraction is accomplished using machine learning for named entity recognition and dependency parsing. The extracted WWWWH info is displayed to the user in an easily digestible table, which allows for users to quickly learn the essential information regarding any given crisis event. Both the user’s input and the output data will be saved to a database, so that users can see their previous usages of the program again at any time. While users must manually input web articles into the program, whether as links or .txt files, there is potential in the future to use a web crawler to automate this initial article gathering.\nThe stack for this applications utilizes the MERN Stack. MongoDB was chosen due to its flexible document structure. For the back-end features such as natural language processing and our server we utilized Python and Express/Node.js. The front-end consists of React which is used to fetch our data and utilizes component libraries such as MUI for consistent design language.\nThe deliverables for this project include our Final Presentation and Final Report which show our progress throughout the development stages, and finally our code for the application which are submitted to our professor and client, Mohamed Farag.','[\"Extraction\", \"Natural Language Processing\", \"WARC\", \"Webpages\", \"Archive\", \"Python\", \"JavaScript\", \"React\", \"MongoDB\"]','Virginia Tech',18,'The documents titled \"Crisis Events Information Extraction Presentation\" in both PDF and PPTX formats represent our final presentations. These presentations portray the progress achieved throughout the development stages, provide an overview of the problem, outline our solution strategy, and detail the functionality of our application.\n\nThe documents tiled \"Crisis Events Information Extraction Report\" in both PDF and DOCX formats are our in-depth reports that portray everything that needs to be known about this project. This includes our abstract, introduction, requirements, design, implementation, testing, user\'s and developer\'s manuals, and lessons learned.\n\nThe \"Crisis Events Extraction Information Extraction Code\" is a zip file that contains all the code needed for our application to run.',NULL,NULL,NULL,NULL,NULL),(3,'Integrated Web App for Traffic Simulator',NULL,NULL,'N/A',18,'N/A',NULL,NULL,NULL,NULL,NULL),(4,'Crisis Events Text Summarization','From mass shootings to public health emergencies, crisis events have unfortunately become a prevalent part of today’s world. This project contributes to the advancement of crisis response capabilities by providing an accessible tool for extracting key insights from diverse sources of information. The system allows users to create collections to aggregate articles and data relevant to specific crisis events. Users can upload files in various formats, including text files of links, zip files containing articles in text format, or zip files with HTML content. The program extracts and organizes information from these sources, storing it efficiently in a SQLite database for future retrieval and analysis. One of the key features of the system is its flexibility in text summarization. The current summarizers available are BERT, T5, and NLTK, but it would be relatively easy to add new summarizers at a later date. Currently, the NLTK and T5 summarizers work relatively quickly, but the BERT summarizer takes minutes before it finishes summarizing. This is because the BERT summarizer is the most powerful, being a larger model and requiring more processing. The front-end of the application is written in React.js using JavaScript. The back-end is composed of the database, the scraper, and the summarizers. The code for accessing the database is written in Python. The Flask framework facilitates back-end operations, allowing seamless integration between frontend and database functionalities. The code for the summarizers is also written using Python. The libraries used in the summarizer code are NLTK, Transformers, PyTorch, and Summarizer. The code for the web scraper is also written using Python and utilizes the BeautifulSoup4 library for parsing HTML. Overall, this project aims to empower users with a crisis information management tool that efficiently aggregates, extracts, and summarizes data to aid in crisis response and decision-making.',NULL,'N/A',18,'N/A',NULL,NULL,NULL,NULL,NULL),(5,'CS4624: Crisis Events Knowledge Graph Generation','In a world inundated by information during crisis events, the challenge isn’t just finding data, it’s making sense of it. Knowledge graphs rise to this challenge by structuring disparate data into interconnected insights, enabling a clearer understanding of complex situations through visual relationships and contextual analysis. This report presents a web-based application for generating and managing knowledge graphs and details the process taken to create it. The application integrates React with Material-UI for the frontend, Flask for the backend, and MongoDB and Neo4j for data storage. Users input multi document collections which are processed using Beautiful Soup, Stanford’s Core NLP, NLTK and SpaCy to extract and analyze data, forming triplestores and named entities. These elements are then used to generate knowledge graphs that are stored in Neo4j and rendered on the web via Sigma.js and Graphology. This report addresses development processes, features, testing, step-by step guidance for users and developers, the lessons we learned working on this project, and potential enhancements that can be implemented by future student groups picking up our project.',NULL,'N/A',18,'N/A',NULL,NULL,NULL,NULL,NULL),(6,'Creating a Website for Election Predictions','This project, Creating a Website for Election Predictions, aims to present 2024 presiden-\ntial election predictions at the county level using demographic variables. By leveraging\nmachine learning techniques on historical survey data, our website offers an interactive\nmap and data visualization tools for public access. This non-academic approach seeks\nto provide a more accurate and representative analysis of election predictions, diverging\nfrom traditional poll-based methods. Additionally, it serves as a user-friendly platform\nfor policymakers and the public to gain insightful, data-driven perspectives on election\noutcomes.','[\"Election Prediction\", \"Website\"]','N/A',18,'Election Prediction',NULL,NULL,NULL,NULL,NULL),(7,'Crisis Events News Coverage Analysis','Analysis of the coverage of crisis events by news agencies provides important information for historians, activists, and the general public. Detecting bias in news coverage is a direct benefit. Thus, there is a need for an automated tool that, given a set of crisis events and a dataset of webpages about these events, can extract the set of news media outlets that reported about these events and how frequently, the types of events covered by each media outlet, and how each news media outlet links to other outlets, if any. Bias detection and sentiment analysis can then be applied to each media outlet to discover hidden patterns.\nThe web application we have designed will allow users to provide a collection of URLs or HTML files for webpages reporting on a crisis event. The program will provide a thorough analysis of the provided collection, detecting bias in news coverage as well as linkage between different domains. The results of this analysis will then be returned to the user, offering insights into their provided collection of news articles in a way that is accurate, informative, and easy to understand.\nOur team is optimistic that the application we have developed will assist users in navigating the complexities of news reporting during periods of uncertainty. In today\'s increasingly divided and turbulent political landscape, discerning the truth from misinformation is more crucial than ever. We believe that our application will empower individuals to make more informed decisions through enhancing the transparency of online news organizations, ultimately contributing to a culture of more responsible journalism and improved civic discourse.',NULL,'N/A',18,'N/A',NULL,NULL,NULL,NULL,NULL),(8,'Building an Intelligent QA/Chatbot with LangChain and Open Source LLMs','We have created a web application enabling access to Intelligent Q/A chatbots, where the end user has access to query language learning models to retrieve context specific information.\nThis web application will provide a collection-based interface, where documents uploaded by the user provide the context for responses by the language learning model to user input. This is accomplished through retrieval augmented generation (RAG) pipeline. As to reduce inaccuracies and fulfill the user needs of the client, the language learning model will notify the user if a query cannot be sufficiently answered given the documents in a collection.\nAs such, our application emphasizes collection management with the functionality to upload (in .txt, .html or .zip format) and delete documents as well as select specific collections, while providing a familiar interface not much different from the web interface for established AI chatbot services such as OpenAI’s ChatGPT or Anthropic’s Claude. The final product also currently encompasses a landing page and user login, with accessibility to a document upload portal for creating document collections.',NULL,'N/A',18,'N/A',NULL,NULL,NULL,NULL,NULL),(9,'Discovery Portal for Twitter Collection',NULL,NULL,'Team 10',18,'N/A',NULL,NULL,NULL,NULL,NULL),(10,'Integrated Web App for Crisis Events Crawling','The integration of a web crawler and a text classifier into a unified web application is a practical advancement in digital tools for crisis event information retrieval and parsing. This project combines HTML text processing techniques and a priority-based web crawling algorithm into a system capable of gathering and classifying web content with high relevance to specific crisis events. Utilizing the classifier project’s model trained with targeted data, the application enhances the crawler\'s capability to identify and prioritize content that is most pertinent to the crisis at hand.\nThe transition from Firebase to MongoDB for backend services provides a much more flexible, accessible, and permanent database solution. As well as this, the system’s backend is further supported by a Flask API, which facilitates the interaction between the frontend, the machine learning model, and the database. This setup not only streamlines the data flow within the application but also simplifies the maintenance and scalability of the system.\nThis integrated web app aims to serve as  a valuable tool for stakeholders involved in crisis management, such as journalists, first responders, and policy makers, enabling them to access timely and relevant information swiftly. During development of this project there were many challenges with fixing the two projects; out of the box neither was functional when they were obtained from their respective repositories. As well as this, the projects had incomplete documentation, leaving a lot for our team to figure out on our own. The results of our team is a redesigned frontend, backend, and MongoDB local database together into a cohesive, full application.','[\"text classifier\", \"text classification\", \"web crawler\", \"information retrieval\", \"web crawling\", \"crisis event\"]','Virginia Tech',18,'Two previous projects built two web apps for retrieving webpages about a crisis event. The first web app provided a nice web interface for building and using one-class classification to judge if a webpage is related to a crisis event or not and the second web app provided a nice web interface for crawling the WWW about webpages related to a crisis event. In this project, we would like to merge these two web apps into one, where we will have an integrated web interface for preparing the one class classifier and then using it to crawl the web.',NULL,NULL,NULL,NULL,NULL),(11,'ScrapingGenAI','AI has been widely used for many years and has been a constant front-page news topic. The recent but fast development of generative AI inspired many conversations, from concerns to aspirations. Understanding how the topic develops and when people become more supportive of generative AI is critical for social scientists to pinpoint which developments inspire public discussions. The use of generative AI is relatively new. The data and insight gathered could be used to determine if use in a commercial setting (like in Travel/Hospitality) is viable and what the potential feedback from the public might look like.\nWe developed two specialized web scrapers. The first targets specific keywords within Reddit subreddits to gauge public opinion, and the second extracts discussions from corporate earnings calls to capture the business perspective. The collected data were then processed and analyzed using Python libraries, with visualizations created in Matplotlib, Pandas, and Tkinter to depict trends through line charts, pie charts, and bar charts. We limited our analysis period from August 2022 to March 2024, which is significant as ChatGPT was released in November 2022, allowing us to observe notable changes. These tools not only show changes in public interest and sentiment but also provide a graphical representation of temporal shifts in the perception of AI technologies over time.\nThe final product is designed for anyone interested in company transcripts and in comparing them to the public perspective. The product offers users access to detailed data representations, including numerical trends and visual summaries to further understand the correlation between the company and the public. This comprehensive overview assists in understanding how public and corporate sentiments towards AI have shifted during a recent 20-month period.\nA significant hurdle was using the PRAW API for Reddit data scraping. Through review of documentation, tutorials, and additional support from a teaching assistant, we successfully implemented the functionality needed to extract and process the data from subreddits effectively.\nTo make our findings more accessible and engaging, future additional work transforming this product into a fully functional website would be beneficial. This platform would make the insights more readily available to a wider audience, including the general public and industry stakeholders. Doing so could enhance the impact and usefulness of our project.','[\"Travel\", \"Web-Scraping\", \"Reddit\", \"MarketBeat\"]','N/A',18,'ScrapingGenAIpresentation.pdf: PDF file for the ScrapingGenAI presentation.\nScrapingGenAIpresentation.pptx: Powerpoint file for the ScrapingGenAI presentation.\nScrapingGenAIreport.docx: Word document for the ScrapingGenAI report.\nScrapingGenAIreport.pdf: PDF file for the ScrapingGenAI report.',NULL,NULL,NULL,NULL,NULL),(12,'Assistive Voice Assistant','This project is an extension of work that has been done in previous years on the sharkPulse website. sharkPulse was created due to the escalating exploitation of shark species and the difficulty of classifying shark sightings. Due to sharks’ low population dynamics, exploitation has only exacerbated the issue and made sharks the most endangered group of marine animals.\nsharkPulse retrieves sightings from several sources such as Flickr, Instagram, and user submissions to generate shark population data. The website utilizes WordPress , HTML, and CSS for the front end and R-Shiny, PostgreSQL, and PHP to connect the website to the back end database. The team was tasked with improving the general usability of the site by integrating dynamic data-informed visualizations. The major clients of the project are Assistant Professor Franceso Ferreti from the Virginia Tech Department of Fish and Wildlife Conservation and Graduate Research Assistant Jeremy Jenrette.\nThe team established regular contact through Slack, scheduled weekly meetings online with both clients, and acquired access to all major code repositories and relevant databases. The team was tasked with creating dynamic and data-informed visualizations, general UI/UX improvements, and stretch goals for improving miscellaneous pages throughout the site. The team developed PHP scripts to model a variety of statistics by dynamically querying the database. These scripts were then sourced directly through the site via the Elementor WordPress module.\nAll original requirements from the clients have been met as well as some stretch goals established later in the semester. The team created a Leaflet global network map of affiliate links which dynamically sourced the sharkPulse social network groups from an Excel spreadsheet and generated country border markers and links to each country’s social network sites as well as a Taxonomic Accuracy Table for the Shark Detector AI. The team created and distributed a survey form to collect user feedback on the general usability of the site which was compiled and sent to the client for future work.','[\"LLM-Assistant\", \"Voicebot\", \"User-Interaction\", \"Drive-Through-Automation\", \"Drivingo\", \"CS4624 Multimedia and Hypertext\", \"Python\", \"GPT-4\", \"OpenAI\", \"Automation Bot\"]','N/A',18,'The final report appears in PDF and Word versions in files AssistiveVoiceAssistantReport.pdf and AssistiveVoiceAssistantReport.docx. \nThe final presentation appears in PDF and PowerPoint versions in files AssistiveVoiceAssistantPresentation.pdf and AssistiveVoiceAssistantPresentation.pptx.',NULL,NULL,NULL,NULL,NULL),(13,'SharkPulse App','This project is an extension of work that has been done in previous years on the sharkPulse website. sharkPulse was created due to the escalating exploitation of shark species and the difficulty of classifying shark sightings. Due to sharks’ low population dynamics, exploitation has only exacerbated the issue and made sharks the most endangered group of marine animals.\nsharkPulse retrieves sightings from several sources such as Flickr, Instagram, and user submissions to generate shark population data. The website utilizes WordPress , HTML, and CSS for the front end and R-Shiny, PostgreSQL, and PHP to connect the website to the back end database. The team was tasked with improving the general usability of the site by integrating dynamic data-informed visualizations. The major clients of the project are Assistant Professor Franceso Ferreti from the Virginia Tech Department of Fish and Wildlife Conservation and Graduate Research Assistant Jeremy Jenrette.\nThe team established regular contact through Slack, scheduled weekly meetings online with both clients, and acquired access to all major code repositories and relevant databases. The team was tasked with creating dynamic and data-informed visualizations, general UI/UX improvements, and stretch goals for improving miscellaneous pages throughout the site. The team developed PHP scripts to model a variety of statistics by dynamically querying the database. These scripts were then sourced directly through the site via the Elementor WordPress module.\nAll original requirements from the clients have been met as well as some stretch goals established later in the semester. The team created a Leaflet global network map of affiliate links which dynamically sourced the sharkPulse social network groups from an Excel spreadsheet and generated country border markers and links to each country’s social network sites as well as a Taxonomic Accuracy Table for the Shark Detector AI.  The team created and distributed a survey form to collect user feedback on the general usability of the site which was compiled and sent to the client for future work.','[\"Sharks\", \"Web Application\", \"Conservation\", \"SharkPulse\", \"Classification\", \"Data Visualization\", \"User Experience\"]','N/A',18,'SharkPulsePresentation.pdf PDF of our final presentation\nSharkPulsePresentation.pptx Powerpoint of our final presentation\nSharkPulseReport.pdf PDF of our final report\nShrakPulseReport.docx Word Document of our final report \nSharkPulseScripts.zip - Finalized scripts for dynamic visualizations',NULL,NULL,NULL,NULL,NULL),(14,'Case Studies Library','The purpose of this project is to create an online repository for the CS 3604 course at Virginia Tech. This course, Professionalism in Computing, has students complete case studies on various ethical issues. The issues range from historical supreme court cases to ongoing struggles. Each year nearly 300 such studies are conducted. There should be a mechanism in place to store these studies so the repository is easy to navigate and search.\nPrevious work attempted to use a preexisting digital library tool hosted on AWS to implement this repository. Over time, the CS 3604 copy became out of sync and out of date, leading to a mountain of issues. Initially, this group sought to overcome those issues and stay with the previous approach. After attempting to resolve those issues, the group met with a software engineer from the team supporting the original digital library platform. This resulted in a switch to a custom website, built from scratch, to host the CS 3604 repository.\nThe new full stack website used React.js, Express.js, Node.js, and MongoDB to accomplish this goal. Due to the late start, the group created a preliminary website architecture, before breaking into tasks of frontend development, application development, backend work, and authentication.\nThe new repository offers a user profile to each student in the capstone class that is accessed via a Microsoft login linked to their Virginia Tech account. Each user can upload a title, list of tags, and PDF document showcasing their case study. The rest of the site is publicly accessible and can be searched by title and tags. The searching features are less sophisticated compared to the prior website. However, the new website has the advantages of user login, linking of case studies to users via login, and easier maintainability.','[\"Website\", \"Ethics\", \"Professionalism\"]','N/A',18,'Includes the final report in a PDF file, and as a ZIP dump of a LaTeX project exported from Overleaf. Also includes PDF and PowerPoint versions of the final presentation.',NULL,NULL,NULL,NULL,NULL),(15,'Language and Sentiment Analysis of Extremist Behavior in Online Game Communities','Language and Sentiment Analysis of Extremist Behavior in Online Game Communities was a Multimedia, Hypertext, and Information Access capstone project to assist the VT Gamer Lab in gathering more data to analyze links between online video game communities and extremist behavior. Specifically military simulation games (referred to as milsims) were analyzed due to the inherently political and violent nature of the gameplay. The deliverables for the project were a community forum and YouTube scraper, cleaned data, visualizations, and sentiment analysis. We collected large datasets from both the community forums and YouTube, successfully cleaned the data, and did an analysis to create interesting visualizations. Sentiment analysis was originally going to be conducted with the client but was delayed past the submission of our report so we created our own analysis methods to produce interesting visualizations. Two of these visualizations showed us that the potentially extremist language on both platforms is very similar, both in word choice and frequency. This suggests that the communities define the language more than the platform they exist on.','[\"Scraping\", \"War Thunder\", \"Online Extremism\", \"Python\"]','N/A',18,'Scraping',NULL,NULL,NULL,NULL,NULL),(16,'Knowledge Graph Building','Our team’s main objective was to expand the Virtuoso database by integrating a comprehensive dataset of 500,000 enriched Electronic Theses and Dissertations (ETDs). We built upon the preliminary framework of 200 XML records used for initial testing. This database expansion would enable the developers to deploy more robust testing and analysis of the current Knowledge Graph database. Additionally, our team focused on standardizing the data expansion process, ensuring that future developers have a consistent and reliable foundation for their work.\nThe current Knowledge Graph was established with the Virtuoso graph database system. We primarily worked on four steps to expand the KG database, including inserting Object IDs into each element in XML files, converting XML files to RDF triples, uploading RDF triples to the Virtuoso database, and URI resolution. We leveraged the power of Python, along with its robust libraries (rdflib, sparqlwarpper, requests, xmltodict, Node.js, NPM, tkinter) and tools (REST API, Docker) to execute these steps. Initially, our team successfully tested the data expansion process on a local Virtuoso instance to ensure the functionality and correctness of the expanding procedure. We prepared to deploy the process on the Virtuoso database within the Endeavour cluster upon confirmation. Although we successfully expanded the database by 333 ETDs, we were unable to reach our target of 500,000 ETDs due to a shortage of XML data. This limitation made us refocus our efforts on refining the data expansion process for better standardization and future scalability. We streamlined the data expansion process by integrating the Object ID insertion, data conversion, and data uploading processes into a single GUI application, creating a more straightforward and compact workflow. This visual interface would enhance usability for future developers and teams.','[\"RDF triples\", \"Virtuoso\", \"ETDs\", \"Electronic Theses and Dissertations\", \"XML\", \"URI resolution\"]','N/A',18,'KGBuildingReport.pdf: PDF version of the presentation for the KG Building project. \nKGBuildingReport.docx: Microsoft Word version of the final report for the KG Building project. \nKGBuildingPresentation.pdf: PDF version of the presentation for the KG Building project. \nKGBuildingPresentation.pptx: Microsoft PowerPoint version of the presentation for the KG Building project.',NULL,NULL,NULL,NULL,NULL),(17,'AgInsuranceLLMs','Our project is to develop a conversational assistant to aid users in understanding and choosing appropriate agricultural insurance policies. The assistant leverages a Large Language Model (LLM) trained on datasets from the Rainfall Index Insurance Standards Handbook and USDA site information. It is designed to provide clear, easily understood explanations and guidance, helping users navigate their insurance options. The project encompasses the development of an accessible chat interface, backend integration with a Flask API, and the deployment of the assistant on Virginia Tech\'s Endeavour cluster. Through personalized recommendations and visualizations, the assistant empowers users to make well-informed decisions regarding their insurance needs. Our project report and presentation outline the project\'s objectives, design, implementation, and lessons learned, highlighting the potential impact of this interactive conversational assistant in simplifying the complex process of selecting agricultural insurance policies.','[\"agriculture\", \"insurance\", \"drought\", \"LLMs\", \"RAG\"]','N/A',18,'A web application to provide recommendations and answers to questions related to agricultural insurance using Large Language Models.\n\n The final report is available in Word and PDF versions in files\n  AgricultureInsuranceLLMsReport.docx and AgricultureInsuranceLLMsReport.pdf.\n\n The final presentation is available in PowerPoint and PDF versions in files\n  AgricultureInsuranceLLMsPresentation.pptx and AgricultureInsuranceLLMsPresentation.pdf.\n\nLicense: GNU General Public License v3.0, https://choosealicense.com/licenses/gpl-3.0/',NULL,NULL,NULL,NULL,NULL),(18,'Tweet Collections','For a series of various Virginia Tech research projects related to Dr. Andrea Kavanaugh, more than six billion tweets between the years 2009-2024 were collected to be used for research purposes. These tweets cover many topics, but primarily focus on trends and important events that occurred during the time period. These tweets were collected in three different formats: Social Feed Manager (SFM), yourTwapperKeeper (YTK), and Digital Methods Initiative Twitter Capture and Analysis Toolset (DMI-TCAT). The original focus of the project was to convert these tweets into a singular format (JSON) to make tweet access easier and simplify the research process.\nThe team in the Fall of 2021 consisting of Yash Bhargava, Daniel Burdisso, Pranav Dhakal, Anna Herms, and Kenneth Powell were the first to take on this project and managed to finish the process of writing the initial Python scripts used to convert the three tweet formats to JSON. They originally provided six different Python scripts, two for each of the three tweet formats, one for the individual schema and the other for the collection level schema. However, large parts of these Python scripts were highly unoptimized and would take an unreasonably long time to run. Thus, the team in Spring of 2022 consisting of Matt Gonley, Ryan Nicholas, Nicole Fitz, Griffin Knock, and Derek Bruce took on the project and managed to optimize a portion of the original Python scripts in addition to implementing a BERT-based machine learning model used to classify the tweets. They adjusted the scripts to better accommodate scale and were able to begin the tweet conversion process, getting through about 800 million of the roughly 6 billion tweets collected.\nThis project was taken over again in Spring of 2024, and began by writing additional automation scripts to simplify the process and reduce the amount of work that had to be done manually for the SFM conversion process. In addition to writing new scripts, our team updated some of the scripts done by the past team, to better suit our uses. We exported 45 collections from the SFM machine and were able to convert 9,744,468 tweets from SFM. Regarding DMI_TCAT and YTK, the raw SQL files needed to be transferred to a new database in order to convert the remaining tweets. This process was begun for DMI and YTK at the Digital Library Research Laboratory, located in room 2030 at Torgerson Hall, and will be continued into Summer 2024. Regarding the machine learning aspects of the project, we implemented a new hate speech classifier, due to the prevalence of hate speech on the internet. We ran a test with both a GloVe model and a BERT model with a Naive Bayes classifier, before ultimately settling on the GloVe model due to the speed being significantly faster while still providing enough accuracy to be useful.','[\"tweet\", \"collection\", \"JSON\", \"Python\", \"Machine Learning\", \"Classification\", \"Data Conversion\", \"Data Processing\"]','Virginia Tech',18,'tweet',NULL,NULL,NULL,NULL,NULL),(19,'Chapter Classification and Summarization','The US corpus of Electronic Theses and Dissertations (ETDs), partly captured in our research collection numbering over 500,000, is a valuable resource for education and research. Unfortunately, as the average length of these documents is around 100 pages, finding specific research information is not a simple task. Our project aims to tackle this issue by segmenting our sample of 500,000 ETDs, and providing a web interface that provides users with an application that summarizes individual chapters from the previously segmented sample.\nThe first step of the project was to verify that the automatic segmentation process, performed in advance by our client, could be relied upon. This required each team member to analyze 50 segmented documents and verify their integrity by confirming that each chapter was correctly identified and separated into a PDF. During this process, we noted any peculiarities, to identify recurring issues and improve the segmentation process. The rest of our time and effort went into creating an efficient web interface that would allow users to upload ETD chapters and display said chapter’s summary and classification results.\nWe were able to complete a web interface that allows a user to upload an ETD chapter PDF from the sampled ETD database and view the summary of the PDF along with all of the metadata (author, title, publication date, etc.) of the associated ETD. Additionally, the group verified approximately 60 of the automatically segmented documents and detailed any errors or peculiarities thoroughly. Our group delivered both the web interface as a GitHub repository and an Excel spreadsheet detailing the complete results of our segmentation verification process.\nThe interface was designed to be used in aiding research on ETDs. Although this application won’t be available publicly, researchers may use it privately to assist with any ETD research projects they participate in.\nThe web interface uses Streamlit, which is a Python framework for web development. This was the first time anyone in the group had used Streamlit, so we had to learn each feature that we used, which caused quite a few issues. However, quickly searching and accessing the metadata database, which was originally an Excel sheet with 500,000 entries, posed the biggest threat to the usability of our interface. Luckily, we were able to solve all issues through the use of API documentation, our client, Bipasha Banerjee,  and our extremely helpful instructor, Professor Edward A. Fox.\nIn terms of technical skills, we have learned how to operate a Streamlit web interface as well as how to use MySQL. However, we also learned a few life lessons. Firstly, do not use the first tool available when attempting to solve a solution. It is wise to take extra time to search for the best tool for a given situation instead of wasting time compensating for using the wrong tool. Secondly, life happens without regard and without warning, but the best move is to reanalyze the situation and push forward to complete the work that must be done.','[\"Theses\", \"Electronic Theses and Dissertations\", \"ETD\", \"ETDs\", \"Summarization\", \"Classification\", \"Dissertations\", \"ChapterClassSumm\"]','N/A',18,'ChapterClassSummReport.pdf - PDF version of the final report.\n\nChapterClassSummReport.docx - Word version of the final report.\n\nChapterClassSummPresentation.pptx - PowerPoint version of the final presentation. \n\nChapterClassSummPresentation.pdf - PDF version of the final presentation.\n\nChapterClassSummCode.zip - All of the files created and used in the running of this project.',NULL,NULL,NULL,NULL,NULL),(20,'CTE Website','The Computational Tissue Engineering program at Virginia Tech is an interdisciplinary program that allows graduate students to learn about the following fields: Tissue Engineering, Computational Science, and Molecular and Cell Biology. The vision of the CTE program is for students to feel better equipped about these disciplines and act as trained professionals that can both develop and help push the boundaries of these disciplines.\nThe current CTE website was created a decade ago with a software system called Basecamp by our client Dr. Murali. Throughout the years, it was established that the CTE website became more difficult to update due to newer releases and versions of Basecamp and PHP. Our goal for this project was to update the current CTE website to a modern framework that would allow for an easier to update interface.\nOur methodology to update the current CTE website started with choosing a web development system that would best fit the needs and requirements of this CTE website. The capabilities of WordPress and additive functionalities led us to choose WordPress as our web development system.\nOur methodology to update Dr. Murali’s research website involved understanding the layout and overview of how the website currently looks, researching the UI of other research websites, and creating the research website on WordPress.\nThe outlined project deliverables involved understanding the pros and cons of choosing a web development system that would leverage the capabilities of easier maintenance with a refined layout, implementing a bare bones CTE website, and implementing additional features including building Dr. Murali’s research website.\nThroughout the project, group members worked together to understand the front-end and back-end aspects of the project including researching specific plugins to use that would best fit the feasibility of the website, building of Figma wireframes, form creation, migration of past CTE website pages to the new CTE website, and testing both the functionalities of the CTE website and Dr. Murali’s research website.\nFinal URL for CTE Website:https://wordpress.cs.vt.edu/cteigep/Final URL for Dr. Murali\'s Personal Website:https://wordpress.cs.vt.edu/tmmurali/research/','[\"Computational Tissue Engineering\", \"WordPress\", \"Website\"]','Virginia Tech',18,'Four files were uploaded for the final capstone project. These included two versions of the final report: \"CTEWebsiteReport.docx\" and \"CTEWebsiteReport.pdf.\" Additionally, two versions of the final presentation were provided: \"CTEWebsitePresentation.pptx\" and \"CTEWebsitePresentation.pdf.\" Each document was made available in both its original format and as a PDF to ensure accessibility and compatibility.',NULL,NULL,NULL,NULL,NULL),(21,'Support BRANCH','Support BRANCH, formerly known as Mobile Parenting App, is the outcome of a project initially planned for migrating the Treks mobile app to AWS. Financial challenges at Thrust Interactive led to the transfer of data of the Treks App to Virginia Tech. However, further challenges at Thrust Interactive rendered the project defunct. So, Support BRANCH came about to attempt to achieve the same goals as Treks.\nSupport BRANCH is a web application that helps parents manage behavioral disorders in children, including ASD, ADHD, ODD, and general behavior problems. Parents complete weekly educational adventures that reward parents with badges upon completion.\nThe technical setup includes a front-end TALL stack—Tailwind CSS, Alpine.js, Laravel, and Livewire—and a back-end LAMP stack composed of Linux, Apache, MySQL, and PHP. Laravel is used as the main PHP framework, with Laravel Blade for building web pages. The environment runs on Docker Compose.','[\"Parental\", \"Website\", \"Psychology\"]','N/A',18,'The four uploaded files are SupportBRANCHpresentation.pdf, SupportBRANCHpresentation.pptx, SupportBRANCHreport.docx, and SupportBRANCHreport.pdf. The SupportBRANCHpresentation files contain figures and descriptions of our website used for an in-person presentation. The SupportBRANCHreport files contain a well explained report about our entire project and how we were able to accomplish this final product.',NULL,NULL,NULL,NULL,NULL),(22,'NeuroVeTele','The nervous system, a complex anatomical structure, is crucial for sensory perception in animals, including sight, hearing, smell, taste, touch, and pain. When issues occur in regards to these senses in pets, veterinarians often utilize an examination called neurolocalization to pinpoint these issues to specific parts of a pet\'s nervous system. Understanding neurolocalizations is vital in veterinary medicine for diagnosing nervous system disorders in pets. To streamline this process, a new mobile and desktop application called NeuroVeTele was developed under the guidance of Dr. Richard Shinn, a neurology professor at Virginia Tech. This innovative tool assists veterinarians by using a weighted system to provide accurate neurolocalizations based on user inputs. NeuroVeTele features a easily usable front-end for input selection and a back-end that calculates the best neurolocalization based on these inputs. The front-end and back-end are connected by a Model-View-Controller (MVC) architecture to provide dynamic feedback to users. This application helps veterinarians transition to a paperless medium and aids in devising prognosis plans. The latest version includes a point system for behavior based on Dr. Shinn\'s research and expertise. The front-end has a basic user interface, and there is a beta version of a customizable point system for user adjustments.','[\"Neurolocalization\", \"Veterinary Science\", \"Model-View-Controller\", \"Flutter/Dart\", \"Nervous System\"]','N/A',18,'There are five files included. NeuroVeTeleReport.pdf is a record of the latest update of NeuroVeTele showcasing design, implementation, and manuals of the application. NeuroVeTele.zip contains all the images used in NeuroVeTeleReport.pdf along with the LaTeX code. NeuroVeTelePresentation.pdf and NeuroVeTelePresentation.ppt are the presentation slides used to exhibit the key developments of creating NeuroVeTele. NeuroVeTeleDemo.mp4 is a quick video demo displaying the main sequence when using the app. This demo accompanies all the files above.',NULL,NULL,NULL,NULL,NULL),(23,'Practice 10k Music App','Most musicians strive to practice their instrument every day, which warrants a comprehensive companion app. Such an application should allow users to log their practice sessions and keep them motivated, among other useful features. The Practice 10k App continues in its development to bring these features to aspiring musicians. As of the prior development team, users can currently use the app to create customizable profiles, log their practice sessions, and plan future practice sessions. Our team has continued development by adding a metronome feature, switching the backend service, and fixing database issues. Practice 10k will teach beginner, intermediate, and professional musicians new ways to practice and hold themselves accountable in their musical studies.','[\"Supabase\", \"Metronome\", \"Mobile Application\", \"React\", \"JUCE\", \"Music\", \"Practice\"]','Virginia Tech',18,'Uploaded Files: \n\nFinal Report Documents:\nMusicPracticeAppReport.pdf\nMusicPracticeAppReport.docx \n\nFinal Presentation: \nMusicPracticeAppPresentation.pdf\nMusicPracticeAppPresentation.pptx\n\nDemo Video of the Metronome:\nMetronomeDemoVideo.mp4',NULL,NULL,NULL,NULL,NULL),(24,'PromptLibrary','The rapid advancement and integration of Large Language Models (LLMs) in academic research underscores the critical need for specialized, context-rich training datasets. The PromptLibrary project is designed to address this gap by establishing a comprehensive library of prompts tailored for academic libraries. This initiative aims to amass a wide array of instructional prompts, thereby forming an essential instruction dataset to enhance the utility and relevance of LLMs within scholarly domains. By encapsulating real-world, academic-specific inquiries and scenarios, this dataset is poised to significantly improve LLMs\' learning capabilities and adaptability. The project features a web-based, searchable repository that allows for the submission and retrieval of high-quality prompts, ensuring a robust quality assurance mechanism for prompt validation. This repository not only serves as a critical resource for prompt tuning LLMs but also fosters a collaborative environment for librarians, educators, and researchers, thereby advancing the narrative and utility of LLMs in academic settings.','[\"Web Application\", \"Database\", \"LLM\", \"AI\"]','N/A',18,'The files that are being uploaded to this entry in VTechWorks:\n\nPromptLibraryData.zip - The .zip file that contains the project code files. The project\'s code is also available at https://github.com/narwhalle/PromptLibrary.\n\nPromptLibraryReport.pdf and PromptLibraryReport.docx - The PDF and DOCX file that documents what the project is, how to use the application from the user\'s perspective, the overall structure of the project, specification for future developments, and references to the technologies used for the project. \n\nPromptLibraryPresentation.pptx and PromptLibraryPresentation.pdf - The PPTX and PDF files of the PromptLibrary presentation that detail the overall project visually in terms of used technology, frontend walkthrough, backend walkthrough, lessons learned from development, and future work for further development.',NULL,NULL,NULL,NULL,NULL),(25,'Episodic Future Thinking Chatbot','In healthcare, addressing lifestyle diseases such as obesity and type 2 diabetes through innovative methods is crucial. This project introduces a user interface (UI) for an artificial intelligence (AI) chatbot designed to enhance episodic future thinking (EFT). EFT encourages patients to visualize future scenarios, aiding in decision-making processes that favor long-term health benefits over immediate pleasures. Utilizing AI, this chatbot aims to deliver accessible, high-quality care, helping users focus on significant health goals.\nInitially, the project intended to adapt an existing codebase for UI development. However, it became clear that creating a new UI from scratch would better meet our specific needs. This new interface includes four main pages: Login, Chatbot Interaction, Usability Assessment, and an About page. Each page is carefully crafted using React JavaScript to ensure dynamic user interactions, complemented by Cascading Style Sheets for aesthetic design. Python is used to facilitate connectivity with the client’s chatbot backend and to manage a database that stores user information.\nUsers begin their journey on the website by logging in with credentials obtained after completing a demographic survey via Qualtrics, where they also consent to participate in the study. Subsequently, users interact with a GPT-4 powered chatbot, which guides them through personalized future-thinking scenarios based on their inputs. After the interaction, users assess the vividness of their imagined scenarios and the quality of their chat experience, rating their responses. This data, along with chat logs, is stored for analysis and further enhancements. Finally, participants provide detailed feedback through a structured numerical assessment, contributing to continuous improvement of the chatbot’s effectiveness.\nThrough a meticulous design and iterative testing process, this project not only addresses critical health issues but also sets a new standard for integrating AI with user-centric healthcare solutions.','[\"Episodic Future Thinking\", \"EFT\", \"GPT-4\", \"Website\"]','Virginia Tech',18,'AIChatbotWebsiteReport.docx\nThis document provides a comprehensive overview of the development process, use cases, and the functionalities implemented in the Episodic Future Thinking (EFT) chatbot user interface. It details the design decisions, user interaction models, and the technological framework utilized throughout the project.\n\nAIChatbotWebsitePresentation.pdf\nThis presentation outlines the motivations behind the EFT chatbot project, details the specific requirements, and summarizes the completed work on the user interface. Additionally, it provides insights into the database structure and the operational flow of the site post-credential acquisition from Qualtrics.\n\nAIChatbotWebsitePresentation.pptx\nSimilar to the PDF version, this PowerPoint presentation emphasizes the motivations, requirements, and the developmental accomplishments of the EFT chatbot\'s user interface. It also illustrates the database structure and navigational flow of the site after users obtain their credentials via Qualtrics.\n\nAIChatbotWebsiteReport.pdf\nThis report documents the developmental journey, relevant use cases, and the comprehensive work accomplished on the EFT chatbot user interface. It elaborates on the project\'s scope, user engagement strategies, and technical infrastructure.',NULL,NULL,NULL,NULL,NULL),(26,'Parking Spaces Occupancy Prediction','Across Virginia Tech’s campus, finding parking is consistently a source of frustration for students and faculty. During peak hours, locating free parking spots becomes a challenging task; leading to significant delays and increased traffic around campus. Leveraging modern data-driven technologies such as Smart City infrastructure and Intelligent Transportation, we can alleviate some of the school’s congestion and enhance the parking experience for Virginia Tech residents. The proposed solution is a web app that users can integrate into their daily commute. With the help of live data, the app will give real-time parking recommendations as well various other helpful insights. It will analyze the live data at each of the garages, to predict the occupancy of the garages at a given time of arrival. Machine learning will allow us to estimate the occupancy of each of the garages a given time into the future, depending on the distance to each garage, and provide a recommendation for which garage to target. The application will also allow for more effective collection of data for parking services and could eventually take into account more factors such as schedules and live traffic.','[\"Parking\", \"Machine Learning\", \"Garage\", \"Prediction\", \"Parking Prediction\"]','N/A',16,'Parking',NULL,NULL,NULL,NULL,NULL),(27,'Visualizing eTextbook Study Sessions','OpenDSA is an online platform that allows professors to create e-Textbooks with fundamental CS courses. Our project seeks to enhance OpenDSA by providing instructors with a user-friendly web interface and visualization tool. This tool allows them to understand student interactions during study sessions, in the areas of: Reading, Visualizations, and Exercises. The tool could lead to improvements in a student’s learning process. OpenDSA is heavily used at Virginia Tech and other universities for CS courses. It records student interactions with learning materials but doesn’t have an efficient way for instructors to understand these interactions. Our project tackles this issue by developing a web interface that visualizes student interactions. We expand upon past research by sorting interactions into Reading, Visualizations, and Exercises, displaying detailed study session data. These visualizations will give insight into whether students are active learners or credit-seekers.',NULL,'N/A',16,'N/A',NULL,NULL,NULL,NULL,NULL),(28,'Mass Shooting Digital Library','In light of the escalating prevalence of mass shootings in the U.S., there is an urgent need for a structured digital repository to centralize, categorize, and offer detailed analyses of these events. This project aims to develop a comprehensive website functioning as a digital library. This library will house mass shooting objects where each object symbolizes a specific mass shooting event, elaborating on who, what, when, where, why, and how. The website\'s central features will include the ability to visualize and compare various mass shooting incidents, facilitating a broader understanding of trends, patterns, and anomalies. Users will be able to explore the data via geographic visualizations, timelines, and more, providing an immersive and informative experience. Underpinning the platform, our backend system will utilize Python, Flask, and MongoDB, ensuring robust data collection and management. This data includes information fields, URL sources associated with each event, and more. On the front end, technologies like NextJS, React, and Javascript will drive the user interface, supported by essential libraries such as React Chrono and Leaflet.js for advanced visualization. Deployment will be executed via Firebase or AWS for the frontend and Heroku for the backend. Two primary user categories have been identified: general users, who can view the data, and administrators, who can modify the contents. Ensuring the integrity of the data input, admin access will be safeguarded by authentication processes. In summary, this digital library emerges as a timely and crucial initiative in response to the rising tide of mass shootings in the U.S. This project aims to provide comprehensive insights into the tragic events that have marked the nation. Beyond its functional capabilities, the digital library strives to improve understanding, awareness, and ultimately, change in the narrative surrounding mass shootings.','[\"Mass Shootings\", \"Website\", \"Library\"]','N/A',16,'Mass Shootings',NULL,NULL,NULL,NULL,NULL),(29,'Automated Students\' short answers assessment','The objective of this innovative project was to create an automated web application for the assessment and scoring of computer science-related short answers. This solution directly addresses the often labor-intensive and time-consuming process of manually grading written responses, a challenge that educators across various academic disciplines frequently encounter. The developed web application stands out not just for its efficiency but also for its versatility, being applicable to a wide range of subjects beyond computer science, provided that appropriate teacher answer files are supplied.\nAt the heart of the application lies a user-friendly interface created using ReactJS. This frontend allows educators to seamlessly upload \'teacher\' and \'student\' files in .tsv format. Following the upload, the application\'s backend, developed using Flask, takes over. It processes these submissions by comparing student responses against predefined model answers. The scoring mechanism of the application is particularly noteworthy. It employs an advanced semantic analysis approach, utilizing a pre-existing deep learning model, RoBERTa Large. This model is integral to the AutoGrader class, which is responsible for the semantic evaluation of the text.\nThe grading logic embedded within the AutoGrader class is both innovative and sophisticated. It assesses student responses by breaking them down into phrases and then computing the semantic similarity between each phrase and the concepts outlined in the model answers. The process employs SentenceTransformer to generate text embeddings, allowing for a nuanced evaluation based on cosine similarity between vector representations. This method ensures a grading system that transcends simple keyword matching, delving into the semantic content and understanding of the student answers.\nThe application boasts several key features that enhance user experience and provide educators with comprehensive insights into student performance. These include the ability to display scores and grades directly on the web application, download detailed Grade Reports that include each question, student\'s response, the grade awarded, and the model answer. Additionally, the application allows for the viewing of previous submissions and the downloading of historical documents such as past versions of \'teacher file\', \'student file\', and grade reports.\nIn terms of future development, the project team has outlined several ambitious goals. These include implementing a dataset-driven strategy for enhancing the training of deep learning models, thereby significantly advancing the current framework. Another focus will be on allowing for a variety of file types to be uploaded for both teacher and student files, thereby increasing the accessibility and usability of the system. Lastly, there are plans to update the functionality and appearance of the web application, incorporating features such as scrolling, standardized formatting, and improved design elements to enhance the overall user experience.\nThe project was developed with the invaluable guidance and support of Dr. Mohamed Farag, a research associate at the Center for Sustainable Mobility at Virginia Tech. Dr. Farag\'s expertise in computer science and his commitment to educational innovation have been instrumental in steering the project towards success.\nIn conclusion, this project marks a significant advancement in the field of educational technology, particularly in the realm of academic grading. By leveraging the power of artificial intelligence and modern web technologies, it provides an efficient, reliable, and versatile tool for educators, streamlining the grading process and offering a scalable solution adaptable to various academic contexts. The future developments outlined promise to further enhance the capabilities of this already impressive tool, pointing towards a new era in academic assessment.',NULL,'N/A',16,'N/A',NULL,NULL,NULL,NULL,NULL),(30,'Automated Crisis Collection Builder - Final Project Report','In the contemporary digital landscape, access to timely and relevant information during crisis events is crucial for effective decision-making and response coordination. This project addresses the need for a specialized web application equipped with a sophisticated crawler system to streamline the process of collecting pertinent information related to a user-specified crisis event.\nThe inherent challenge lies in the vast and dynamic nature of online content, where identifying and extracting valuable data from a multitude of sources can be overwhelming. This project aims to empower users by allowing them to input a list of newline-delimited URLs associated with the crisis at hand. The embedded crawler software then systematically traverses these URLs, extracting additional outgoing links for further exploration. Afterwards, the contents of each outgoing URL is then run through a predict function, which evaluates the relevance of each URL based on a scoring system ranging from 0 to 1. This scoring mechanism serves as a critical filter, ensuring that the collected web pages are not only related to the specified crisis event but also possess a significant degree of pertinence. We allow the user to set these thresholds, which enhances the efficiency of information retrieval by prioritizing content most likely to be valuable to the user\'s needs.\nThroughout the crawling process, our system tracks a range of statistics, including individual website domains, the origin of each child URL, and the average score assigned to each domain. To provide users with a comprehensive and visually intuitive experience, our user interface leverages React and D3 to display these statistics effectively.\nMoreover, to enhance user engagement and customization, our platform allows users to create individual accounts. This feature not only provides a personalized experience but also grants users access to a historical record of every crawl they have executed. Users are further empowered with the ability to effortlessly export or delete any of their previous crawls based on their preferences.\nIn terms of deliverables, our project commits to providing fully developed code encompassing both frontend and backend components. Complementing this, we will furnish comprehensive user and developer manuals, facilitating seamless continuity for future students or developers who may build upon our work. Additionally, our final deliverables include a detailed report and a compelling presentation, serving the dual purpose of showcasing our team\'s progress across various project stages and providing insights into the functionalities and outcomes achieved.','[\"Fullstack Application\", \"Flask\", \"React\", \"Javascript\", \"Dockerfile\", \"Capstone\", \"Python\", \"Final Report\", \"Final Presentation\", \"CS4624\", \"Multimedia\", \"Hypertext\", \"Information Access\"]','N/A',16,'This submission contains the complete final project parameters for our CS4624 Capstone Effort. You will find our composite Final Report, Final Presentation, and Source Code in the accompanying files.',NULL,NULL,NULL,NULL,NULL),(31,'Crisis Events One-Class Text Classification','Analyzing web articles related to crisis events can help social scientists gauge public sentiment and form public policy around how to react to such disasters. However, data collection for such tasks is difficult. Manual dataset curation is time-consuming and costly, as a user needs to use some sort of search engine to iterate through multiple web pages, painstakingly analyzing each document thoroughly to determine the crisis events it may be related to. Automated processes, however, such as web crawlers, operate primarily via rule-based methods, which may not accurately classify individual documents as being related to the crisis event of interest. In our work, we seek to use machine learning techniques to determine whether individual documents are related to a specific crisis event using natural language processing techniques. To accomplish this, we treat the area of interest as a single class, and consider all other topics as not being of interest. We hypothesize that natural language processing techniques can be used to to classify a particular webpage as being relevant to a certain crisis. A potential motivation for this approach is to guide efficient web crawling using techniques from semantic analysis.',NULL,'N/A',16,'N/A',NULL,NULL,NULL,NULL,NULL),(32,'Practice 10k: Music App','In the world of music education, inspiring students to maintain consistent and effective practice routines has long been a challenge. Recognizing this dilemma, our clients embarked on a journey to leverage technology and reimagine the practice experience for budding musicians. The result of their efforts was a music application that aims to revolutionize the way musicians approach their practice routines by addressing both convenience and motivation.\nThe innovative concept offers users a number of features that enhance their practice sessions, monitor their progress, and make the entire experience more engaging. Among its many functionalities, the application allows users to plan practice sessions and initiate them with the aid of a built-in timer to track their practice duration. Moreover, the application presents users with visualization representations of their progress on a daily, weekly, monthly, and overall basis through diverse graphs, each highlighting distinct aspects of their practice habits. Additionally, users can delve into a journal-like feature in the application, allowing them to explore and reflect on their musical journey, drawing insights from past practice sessions.\nTo address the core functionalities mentioned above, our project relies on the integration of Firebase for user authentication and backend data storage, coupled with React Native to ensure cross-platform compatibility in the frontend. This framework facilitates effective communication between the backend and frontend, enabling the exchange of user-related data in order to meet the clients’ requirements within the application. This is notably exemplified by our organization of user information in the backend, utilizing specific collections for swift reading and writing of data as users engage with the application. That being said, as we reflect on the culmination of this semester-long project, it is evident that overcoming challenges and seizing opportunities has been instrumental in our gaining invaluable experience in both client collaboration and implementing diverse solutions. However, acknowledging the iterative nature of application development, we understand the ongoing need for refining the existing features and incorporating new ones in future development.','[\"Expo\", \"React Natice\", \"Firebase\", \"Music\", \"Practice\", \"Mobile Application\"]','Virginia Tech',16,'Expo',NULL,NULL,NULL,NULL,NULL),(33,'Traffic Simulator Input/Output GUI','This project aims to address weaknesses in the configuration of the INTEGRATION 2.40 microscopic traffic simulation software. The project is very powerful, capable of simulating hundreds of thousands of vehicles travelling across thousands of roads, while recording a wide variety of metrics. However, the simulation software is configured by a variety of plaintext input files. These files contain newline-delimited fields, with some fields being multipart and whitespace-delimited. These fields are documented in English, with the expected type of the field, as well as other constraints such as field length or numeric field range, expressed in a table format in the documentation. Additionally, some field constraints span multiple files. For example, links in the simulation, defined in the Link File reference start and end nodes that must be defined in a separate Node File. Since there is no validation program to ensure that these input files, which can be tens to hundreds of lines long, with multiple hundreds of fields, have correct and sane values in an interactive, easily runnable format, producing and validating these files is a tedious process.\nThe project solves the aforementioned problems by implementing a web-based interface to create, edit, manage, and validate the input files for the simulation tool. Users can upload a set of six input files, which together are defined as an input package, to the interface, which keeps track of the configured values. The fields in the input files can be edited through the interface through intuitive controls, such as text fields for text content, and drop-down menus for selections. Through the interface, users can perform automatic validation of the files. Any errors in constraint validation will then be surfaced to the user through the web interface, directing them to the appropriate field in the appropriate file for correction. Cross-file validation is also performed to ensure the input files are in a suitable form to be run by the simulation software. After editing and validating fields in the managed Package, users can opt to save the package to the server hosting the web interface, as well as download all input files as a zip package.\nAdditionally, the web interface allows users to template traffic demand values, which is a key component of the Demand File. Users are able to parameterize and create multiple Demand Files (as well as multiple Master Files, which reference all other input files) for combinations of vehicle classes provided by the user. For example, a user might want to create separate input files where Vehicle Class 1 has a traffic demand between 0 and 0.5 (to a maximum of 1), and Vehicle Class 2 has a traffic demand between 0 and 0.2, both in increments of 0.1. This results in 6 * 3 = 18 demand files in total for each combination of the Vehicle Class 1 demand and Vehicle Class 2 demand. The web interface saves time needed to create these files manually, which can be extreme in cases with many combinations of vehicle demand classes.\nOur project is intended to help users of INTEGRATION 2.40 in saving their time and effort when creating input files for the simulation tool. Apart from the project implementation details, we also ensured that project infrastructure reduces user effort and maintenance. The project can be deployed on multiple platforms and can also be packaged as an easy to deploy Docker image for maximum flexibility.','[\"simulation\", \"traffic\", \"interface\"]','N/A',16,'simulation',NULL,NULL,NULL,NULL,NULL),(34,'Crisis Event LLM','Navigating through the intricate landscape of understanding and classifying crisis events, the \"Crisis Events Language Model\" project embarked on a comprehensive exploration leveraging Natural Language Processing (NLP) and machine learning. With a primary focus on utilizing BERT, a powerful PreTrained Language Model, our objective was to create an adept classification system for textual data related to crisis events sourced from the web.\nOur methodology involved the adept use of the BeautifulSoup library in Python for web scraping, enabling the extraction of textual data from URLs associated with crisis events. This rich dataset served as the backbone for training and evaluating our models. Post-data acquisition, we fine-tuned BERT to align with our specific use case, adapting its output layer to meet our unique classification goals. This strategic modification enhanced BERT\'s capabilities in recognizing, interpreting, and categorizing crisis event data with precision.\nSimultaneously, on the front-end development front, we constructed an intuitive interface using HTML and CSS. This user-friendly interface not only facilitates the visualization of the model\'s outputs but also simplifies user interaction and data input. The result is a practical tool poised for deployment in real-time crisis management situations.\nAnticipating multiple impacts, our project positions itself to simplify the comprehension and categorization of crisis events. This functionality, tailored for decision-makers and crisis management teams, promises to be a valuable asset in the face of urgent situations. Moreover, for the participating students, the project provides a dynamic learning experience, bridging theoretical knowledge with practical applications in NLP, text classification, and transfer learning.\nThroughout the project\'s duration, team members assumed diverse roles, from web scraping and model implementation to front-end development and meticulous documentation. This collaborative effort blended skills in programming, software engineering, Python, and machine learning, ensuring a holistic approach to project development.\nIn conclusion, our project not only serves as a testament to the technical prowess and collaboration within our team but also makes substantive contributions to the realms of crisis management and NLP. It underscores the potential of integrating machine learning and language models in crisis management, offering valuable insights and avenues for future exploration and development in this critical area.',NULL,'N/A',16,'N/A',NULL,NULL,NULL,NULL,NULL),(35,'Behind Density Lines: An Interface to Manually Segment Scanning Electron Microscopy Images','SEM (Scanning Electron Microscopy) is a strong imaging technique used in many scientific domains, including materials science, biology, and nanotechnology. Researchers can use SEM to obtain high-resolution images of specimen surface morphology and topography, providing a precise glimpse of structures at the nanoscale. SEM photos reveal intricate surface details, allowing scientists to investigate the texture, shape, and size of particles, cells, or materials with incredible accuracy. Currently, manual segmentation of SEM images is an important stage in the analysis process for researchers. Manual segmentation entails painstakingly drawing and naming sections of interest within photographs, such as specific structures or particles. Researchers often trace object boundaries using sophisticated software tools built for picture processing and analysis.\nWe built a gameified multiplayer online application allowing individual contributors to manually segment a SEM picture in real time due to the time and effort required. One important goal was to involve the next generation of scientists and researchers with a demonstration at the Virginia Tech Science Festival in November 2023.\nWe designed a comparison score technique for a given segmentation to a reference segmentation for a specific SEM picture to provide participants with fast feedback. This enabled individuals and groups to measure their performance while also incorporating a gaming element.\nWe now have a comprehensive understanding of how to create a full-stack project thanks to this initiative. We discovered how to leverage Amazon Web Services, such as EC2, to scale the infrastructure of our website from the backend. Through the use of Javascript frameworks and packages such as NextJS,Socket.io, and ThreeJS, we have created an intuitive user interface for group manual segmentation.','[\"SEM\", \"Scanning Electron Microscopy\", \"SEM Segmenting\", \"Image Segmentation\", \"Web Application\", \"Web Development\", \"Virginia Tech Science Festival\", \"Painter Canvas App\"]','Virginia Tech',16,'BehindDensityLines.zip - Entire code repository for the project. \nBehindDensityLinesPresentation.pdf - Final project presentation in PDF.\nBehindDensityLinesPresentation.pptx - Final project presentation in PowerPoint.\nBehindDensityLinesReport.pdf - Final project report.',NULL,NULL,NULL,NULL,NULL),(36,'Chorobesity: Modern Insight Into An Enduring Epidemic','Health researchers are looking for all possible relationships between two health conditions, obesity and diabetes. To investigate the issue robustly, create detailed experimentation, and develop lasting solutions, the Chorobesity project presents a visual tool of the geographical relationship between obesity and diabetes for our clients to utilize in their studies. Making use of different levels of maps, as well as different color “keys”, the user can study different regions’ health condition statuses.\nThe Chorobesity project aims to be a visual and dynamic tool that researchers can use to further their understanding of the geographical correlation between obesity and diabetes. It provides relevant data and tools for the user to easily interpret and tweak this data for their best understanding. This interactive map, in providing a snapshot of the current health profile of the United States, seeks to be an indispensable tool for policymakers, health professionals, and the general public to understand how obesity and diabetes correlate as the clients see fit.','[\"health map\", \"obesity\", \"diabetes\", \"visualization tool\", \"interactive map\", \"mouse hover\", \"choropleth map\", \"geographic correlation\", \"health issues\"]','Virginia Tech',16,'health map',NULL,NULL,NULL,NULL,NULL),(37,'Topic Modeling Toolkit','The Topic Modeling Toolkit project began with an existing text mining toolkit and aimed to enhance its functionality by incorporating cutting-edge topic modeling techniques. Specifically, BERTopic, CTM, and LDA were used to extract pertinent topics from a corpus of text documents. The resulting web-based platform provides users with a search engine, a recommendation system, and a usable interface for browsing and exploring these topics.\nIn addition to these enhancements, our team also implemented a text-filtering framework and redesigned the user interface using Tailwind CSS. The final deliverables of the project include a fully functional website, user documentation, and an open-source toolkit that can be used to train machine learning models and support browsing and searching for various text datasets.\nWhile the current version of the toolkit includes BERTopic, CTM, and LDA, there is potential for future work to incorporate additional topic modeling methods. It is important to note that while the project originally focused on electronic theses and dissertations (ETDs), the resulting platform can be used to explore and comprehend complex subjects within any corpus of text documents.\nThe topic modeling toolkit is available as an open-source package that users can install and use on their own computers. It is available for use and can be used to support browsing and searching for various text datasets. The intended user group for the platform includes researchers, students, and other users interested in exploring and understanding complex topics within a given corpus of text documents.\nThe resulting topic modeling toolkit offers features that facilitate the exploration and comprehension of intricate topics within text document collections. This tool has the potential to aid researchers, students, and other users in their respective fields.','[\"Machine learning\", \"Topic modeling\", \"User interface design\", \"Text filtering\", \"Tailwind CSS\", \"BERTopic\", \"Search algorithms\", \"Recommendation algorithms\"]','Virginia Tech',17,'Machine learning',NULL,NULL,NULL,NULL,NULL),(38,'Classifying ETDs','Electronic Theses and Dissertations (ETDs) are academic documents that provide an in-depth insight into an account of the research work of a graduate student and are designed to be stored in machine archives and retrieved globally. These documents contain abundant information that may be utilized by various machine learning tasks such as classification, summarization, and question-answering. However, these documents often have incomplete, incorrect, or inconsistent metadata which makes it challenging to accurately categorize these documents without manual intervention since there is no one uniform format to develop the metadata. Therefore, through the Classifying ETDs capstone project, we aim to create a gold standard classification dataset, leverage machine learning and deep learning algorithms to automatically classify ETDs with missing metadata, and develop a website to allow a user to classify an ETD with missing metadata and view already classified ETDs. The expected impact of this project is to advance information availability from long documents and eventually aid in improving long document information accessibility through regular search engines.','[\"Gold Standard ETD Classification Dataset\", \"Deep Learning\", \"Text Classification Models\", \"Interactive User Interface\", \"Data Cleaning\"]','Virginia Tech',17,'Gold Standard ETD Classification Dataset',NULL,NULL,NULL,NULL,NULL),(39,'Chapter Summarization','A thesis is the amalgamation of research that serves as the final product of a graduate student’s knowledge about the information they learned throughout their graduate research. A dissertation is a graduate student’s opportunity to present their original research that they have worked on during a doctorate program to contribute new theories, practices, or knowledge to their field. Theses and dissertations represent the culmination of research of students and therefore can be extremely long. Electronic theses and dissertations (ETDs) are the digital versions of theses and dissertations so that the research and knowledge explored can be more accessible to the world.\nETDs typically contain an abstract describing the work done in the document. However, these abstracts are simply too general, which means they often don’t help readers. There is no happy medium between getting essentially no information from generic abstracts and reading through a dense paper. This is an issue on a global scale.\nWe created chapter summaries of ETDs which aim to help readers decompose and understand the documents faster. We make use of existing machine learning summarization models, specifically Python packages and language models, to help with the summarization. Part of this project is to create a dataset we can work with to create and test our summarization model on. This summarization dataset has been created by annotating the chapters from 100 ETDs (after chapter segmentation). We want to be as diverse as possible, while also being able to pick up on patterns, which is why our ETDs are from a plethora of fields.\nWe have implemented a data extraction pipeline that builds on work done by the Object Retrieval Code from Aman Ahuja et al. Based on this we have created a summarization framework that accepts the chapter text as input and generates chapter summaries that are integrated into the given base front-end website application.\nWe have completed 4 summarization scripts that utilize pre-trained models from Hugging Face which intake the data extracted from the chapter and output a summary of the input data. The four models we used were BART, BigBirdPegasus, T5, and Longformer Encoder Decoder (LED). We were able to use these scripts on all the chapters that we manually segmented to get summaries of all the chapters. We organized these summaries based on what model we used to obtain them in our GitLab repository. We used these summaries to populate a database which was intended to be used for the search functionality of our frontend application. There is more about the specifics of the backend and frontend in section 6.0 Implementation.\nWe gained a holistic understanding of working on a full-stack project. On the backend portion, we learned how to use existing libraries and resources like pandas, PDFPlumber, and WordNinja to extract and format data from an input source. We also learned how to use resources like Hugging Face to understand natural language processing models and the pros and cons of various types of models. By creating scripts to utilize such models for text summarization, we were able to learn the nuances of working with pre-trained models and understand how that can affect our product. For example, if a model was pre-trained on a massive text repository, then it had better chances of recognizing more uncommon words in ETDs. On the frontend portion, we gained experience using React and JavaScript to create a functioning website. We also learned the process of understanding, dissecting, and updating a codebase we inherited from another team. We learned how to create and populate a database in PostgreSQL (commonly referred to as Postgres).','[\"ETDs\", \"Chapter Summarization\"]','Virginia Tech',17,'ETDs',NULL,NULL,NULL,NULL,NULL),(40,'Summarization Evaluation','Electronic Theses and Dissertations (ETDs) are digital versions of academic papers of graduate students. ETDs are highly complicated and lengthy texts: these include multimedia elements and other forms of information. A digital library with summaries for each ETD would enable more people to explore all these texts to learn about various domains. However, most chapters of ETDs don’t have summaries so the solution was to create AI generated summaries for users across disciplines to read.\nBefore these summaries are accessible to the public, Bipasha will find researchers in various disciplines to evaluate the summaries. Incorporating human feedback into AI-generated summaries results in improved accuracy, relevance, originality, engagement and satisfaction. Quantitative measures for evaluating AI-generated content are great but qualitative feedback is important too. Subject matter experts can detect errors and inconsistencies in these summaries: this feedback provides guidance.\nOur team has developed a website that enables users to view and rank AI-generated summaries of texts against the ground truth (provided) chapter summaries. The users will not know beforehand which is which. The scholars (those with an education level of graduate school and beyond) should be able to accurately evaluate the summaries for the ETDs in their field. The purpose of this project is to allow human evaluation of these texts. The ranking feature serves as a form of feedback to perfect the AI generated summaries. The domain experts will use this website to determine the model that serves as a gold standard for summarization.\nFor now, the intended users for this application are subject matter experts at Virginia Tech, so that they can evaluate the summaries. The evaluation will provide an idea of which model performs best to serve as a gold standard for AI generated summaries. Eventually, the final model will be used to serve users outside of Virginia Tech who want to know more about the domain that the ETD is associated with. Providing these summaries allows those outside the domain to grasp the basic concept of the ETD and its associated chapters without having to read the entire paper.','[\"Summarization\", \"Evaluation\", \"Summary\", \"AI Generated\"]','N/A',17,'Summarization',NULL,NULL,NULL,NULL,NULL),(41,'Object Detection and Document Accessibility','Electronic Theses and Dissertations (ETDs) are the primary way that students and professors write down and report their degree research. They allow new minds to understand where that field of study was left off, and how to continue the work that has been left. However, since many of the ETDs uploaded onto the internet are presented via PDF, it\'s difficult for users to view these ETDs in an effective manner, especially when you consider potential students with disabilities such as visual impairments. The goal of this project was to extend upon the previous work that has been done to make a Flask-based web application so that we can transform these long documents into something much more readable, user-friendly, and accessible via HTML rather than PDF. Also, our goal was to apply an algorithm to the returned bounding boxes that come from the object detection model to make sure that separate paragraphs and references are placed into their own box for correct XML generation on the website. To make the application\'s UI usable, we have applied a few changes to improve the experience. We have created the option for users to download the paper via PDF or XML, have a side-bar on the left of the website that contains a dynamic table of contents to jump to whatever part of the paper you select, and have a side-bar view on the right of the website that contains the original PDF so that any errors in our application don\'t ruin the user\'s understanding. We plan for future contributors to add a dark mode and dyslexic-friendly font. Lots of accessibility features will be added via HTML/CSS/React through improving the UI, but what\'s also included is the option to use an on-screen reader. Our project focuses on using NVDA, a popular screen reader, to allow for users with potential visual impairments to be able to listen along to the ETD instead. This was studied thoroughly throughout the course of this project. Finally, for the algorithms side of the project, the focus has been to improve upon the returned bounding boxes from the object detection models to separate paragraph and reference bounding boxes to only include one paragraph or one reference per box. The object detection models do the best they can for the amount of training they\'ve received, but errors are still possible. This side of the project focused on fixing those errors from the model to make sure that the XML generation works well and the text is readable on our final application. The algorithms team was able to get a good post-processing algorithm to work for around 90% of the paragraphs in the ETDs that were tested, but were unable to get to the references part of the deliverable. This is left for future collaborators.','[\"ETD\", \"Electronic Theses and Dissertations\", \"Object Detection\", \"Deep Learning\", \"Accessibility\"]','Virginia Tech',17,'ETD',NULL,NULL,NULL,NULL,NULL),(42,'ETD Recommendation System','Our project involves expanding upon a previous recommendation system built by CS 5604 students. Previous CS 5604 teams have created a chapter summarization model to generate summaries for over 5000 Electronic Theses and Dissertations (ETDs). We used these summaries to fuel our recommendation system. Using chapter summaries improved our ability to predict resources that a user may be interested in because we narrowed our focus to individual chapters rather than the abstract of the whole paper. Authors will benefit from this recommendation system because their work will be more accessible. We provide a web page for users to explore how different clustering algorithms impact the search results, giving the user the ability to modify parameters such as the number of clusters and minimum cluster size. This web page will appeal to niche users interested in experimenting with recommendation systems, allowing them to fine-tune the recommendation results. We recommend for future work to continue exploring different clustering algorithms, as well as using our chapter recommendations to fuel a recommendation list based on each chapter. During this project, we learned about clustering algorithms, working as a team, and starting a project from the ground up. A previous CS5604 team built a stand-alone website that supports search, a recommendation system, and the ability to experiment with different search methods. During this semester, we expanded upon the existing website, using clustering algorithms to experiment with the recommendation system. Users may specify different parameters to understand how different clustering algorithms may change the recommendations.','[\"ETDs\", \"Recommendation System\", \"Machine Learning\", \"clustering\", \"KMeans\", \"DBScan\"]','Virginia Tech',17,'ETDs',NULL,NULL,NULL,NULL,NULL),(43,'Marine Blender','A model of a realistic marine environment is needed for training a rugged, onboard optical sensor designed by Cell Matrix Corporation, a VTCRC COgro member (i.e., a small company in Virginia Tech\'s Corporate Research Center), a project led by Dr. Peter Athanas, an ECE professor at Virginia Tech. This will be accomplished within Blender, a free and open 3D modeling and rendering tool. The chosen environment is the intercoastal waters of the Palm Beach Inlet in Florida, between the Port of Palm Beach and the Inlet, approaching the Inlet from the south side of Peanut Island. This active inlet and port area gives the scene of the Blender model.\nTo build an accurate representation of the specified area we will construct a terrain model for the Palm Beach Inlet water area from the Port of Palm Beach to the Inlet, including where the Intercoastal Waterway channel meets the Inlet channel, south of Peanut Island. This covers the surrounding islands and land masses, bridges, and large structures. There will also be roughly five types of boats to model (i.e., yachts, sailboats, mega-yachts, cargo ships, fishing boats, and other boats commonly found in the area), to represent different situations. Different looking classes of boats are needed to train the marine sensor to recognize them, so we choose different classes and create or find-and-customize a model for a boat from each class. The team will be provided with the trajectories of individual boats traveling this area from AIS ship tracking data published by the US Coast Guard. To simulate these realistic situations we have written a Blender script that allows boats to transit along these AIS tracks.\nThe renders we created from our blender project are representations of the Palm Beach Inlet water area, and will hopefully serve as a useful resource for AI model training.','[\"Blender\", \"Animation\", \"Marine Environment\", \"Model\", \"Palm Beach Inlet\"]','Virginia Tech',17,'Blender',NULL,NULL,NULL,NULL,NULL),(44,'SharkPulseApp','The team was required to redesign, update, and implement changes to an existing website called SharkPulse for a project focused on monitoring global wild shark populations. Currently, the project is led by Dr. Francesco Feretti, an Assistant Professor for the Department of Fish and Wildlife Conservation. The website was built using WordPress with frontend CSS and HTML, and the backend support was provided by PHP, Javascript, and RShiny to implement the Validation Monitor Page. Our team’s primary objectives were to convert the static framework of the website to a dynamic, responsive one. Additionally, we aimed to convert the Instagram monitor from a PHP script to a R Shiny app and merge the Instagram and Flickr monitors into a single page, which would allow toggling between the two pipelines. The Validation Monitor is a user interface that validates records collected from Flickr and Instagram (from the data_mining and Instagram tables). Since shark photos are primarily collected from social media platforms like Instagram and Flickr, the accuracy of the shark information may not be completely reliable. To address this issue, a form is provided with each picture of the shark for users to fill in, in order to validate the originality, type, pieces, and location of the shark. However, the Monitor\'s user interface is currently not responsive, dynamic, or easily navigable by users. By the end of the semester, we successfully combined two versatile pipelines on one page and redesigned both the Validation Form for Instagram and the Flickr Pipeline. Additionally, we added a feature for common name and scientific species autocompletion, supporting users in filling out the Validation form more easily. Furthermore, the map in the Instagram validation form now functions properly, showing the current location of the shark marker and allowing users to search for another location by name or coordinates, helping validate the shark\'s accurate location.','[\"shark\", \"validation\", \"map\"]','N/A',17,'shark',NULL,NULL,NULL,NULL,NULL),(45,'CS 3604 Case Study Library III','This submission describes the CS 3604: Professionalism in Computing Case Study Library, a Library coordinated by our client Dr. Dan Dunlap, that contains the recent case studies written by students in the class. The Case Study Library website provides a platform through which these case studies can be viewed. This was the third group to work on the Library, and the current Library allows for student case study upload, searching, and filtering by course topic. However, upload was through one admin account given to all students provided by the teacher.  This meant once a student uploaded, they could not go back to edit their submission as there was no way to link users to uploads.  Additionally, the interactivity of the website was limited. The first goal of this iteration was to implement login functionality in a manner so that students can log in using their Virginia Tech accounts. This enables us to link users with their uploads and thereby allows them to edit. In order to improve the interactivity of the site, metadata fields will be added for tags and liking. When uploading, students will be able to select various tags from a bank of options that pertain to the subject of their case study, which can later be used for sorting. When viewing case studies, website users will be able to like a submission, and the number of likes on each submission will be stored which can be later used for a recommended page. Our work will increase the opportunity for interaction with users of the website, allowing students to better search for case studies by topic, and to like the studies that others upload. Currently, all of the features that the group attempted to create are working and present, but upload is still not working due to the bucket pointing to the wrong place. The group worked together to build these features as requested by the client, and had to go through a few refactors of the goals in order to reach reasonable milestones over the course of the semester.','[\"Case Study\", \"CS 3604\", \"Dr. Dan Dunlap\", \"Library\", \"AWS\", \"DynamoDB\", \"Login\", \"Authentication\", \"Metadata and Tagging\", \"Website\", \"File Storage\", \"Computer Science\"]','Virginia Tech',17,'Case Study',NULL,NULL,NULL,NULL,NULL),(46,'Schizophrenia Simulator','Schizophrenia is a chronic mental disorder characterized by frequent psychosis and audiovisual hallucinations affecting 24 million people globally. Despite its prevalence, the varying degrees by which the symptoms present combined with misguided media portrayal make schizophrenia and related psychotic disorders one of the more poorly understood and stigmatized mental illness diagnoses. Because it is so misunderstood, it is hard for those with the illness to get help and be treated properly. The objective of this project was to develop a virtual reality simulation using Unity, aimed at replicating the symptoms and impacts of schizophrenia.\nThe end result of this project is a VR simulation that simulates the symptoms of schizophrenia. It can be used with the Oculus Quest and is available through Dr. James Ivory. This project can be worked on by future students if they would like. This includes testing, more attributes to the simulation which could be more interactions or hallucinations, and keeping it updated with the Quest and Unity future versions.','[\"VR\", \"Virtual Reality\", \"Schizophrenia\", \"Simulation\", \"Hallucination\", \"Mental Illness\"]','Virginia Tech',17,'VR',NULL,NULL,NULL,NULL,NULL),(47,'Covid-19 Fake News Detection','The Covid-19 virus is a respiratory illness that causes the isolation and retreat of people globally. People wanted updates and information in real time related to the virus such as what regions/areas are affected, to what degree are the regions affected (heavily infected, none infected, etc), how to prevent catching the virus, and cures for the virus. Social media became a popular platform for people to share information, news, and opinions about the virus. As much positive information that may be spread among social media, just as much, if not more, misinformation can be spread on social media platforms.\nMisinformation is harmful because it can directly affect the health of individuals who fall victim to the misinformation. For example, say a twitter user tweets medical advice about Covid-19, and people who see the tweet choose to follow the advice. Now consider the scenario where they were intentionally spreading false information, which is indeed the opposite of what you should do. The individuals who followed the twitter trolls medical advice may have their own health at risk, and anyone in their sphere of influence.\nOur aim is to understand the types of misinformation spread in social media, and help people identify misinformation spread on Twitter related to the subject of Covid-19. We’re going to do this by extracting relevant information such as the content of a tweet (the tweet itself) or the author of the tweet. Then, we will identify whether the tweets include true information or fabricated information. Once we do this, we are going to test and train an AI model to identify whether a tweet is spreading misinformation, or real information. After we train an AI model to identify the type of information, we will categorize the tweet into the category it was trying to spread information about. Our end goal is to integrate the preprocessing script and the AI model with a website that shows the analysis of the tweets. We want users to be able to insert a tweet into our website related to Covid-19, and the user should be returned with the relevant classification of the tweet. Also, users will be able to download a Web Archive file (WARC) of the archived tweet. Overall, we think the combination of these tasks will help aid users in identifying misinformation related to Covid-19.','[\"COVID 19\", \"Fake News\", \"Fake News Detection\", \"Python\", \"Machine Learning\", \"TWARC\", \"MySQL\", \"Data Processing\", \"Text Classifier\", \"Tweets\", \"Twitter\"]','Virginia Tech',17,'COVID 19',NULL,NULL,NULL,NULL,NULL),(48,'Tunisia Twitter Data','Following the 2011 Tunisian Revolution, Tunisia is widely recognized as an Arab-Spring success story. With campaigns for civil resistance against corruption and civil oppression, the Tunisian Revolution consisted of mass demonstrations that ultimately inspired presidential elections and other democratic reforms across the nation, and a wave of similar protests across the Arab world. In 2021, amid ongoing demonstrations against government dysfunction and corruption, Tunsian President Kais Saied suspended the parliament, replaced the Prime Minister, and began drafting constitutional amendments which reversed nearly a decade of democratic reforms. As freedoms of speech and expression, the right to organize, and many local media outlets have been oppressed, Tunisians have taken to platforms like Twitter to speak truthfully. Thus, CS4624 Team 21 was focused on identifying and analyzing Twitter data relating to democracy, political reforms, and public sentiment in Tunisia since 2020.\nTeam 21 primarily worked with clients Drs. Kavanaugh, Sheetz, Miller, and Farag to analyze Tunisian Twitter data collected by a larger research team in collaboration with Virginia Tech’s (VT) University Libraries. The team handled Twitter data previously collected at VT, as well as more recent data that extends the previous collection. After preprocessing all data to add sentiment scores and filter by English language, the team analyzed the cleaned collection of tweets for key terms and hashtags provided by their clients. The team determined counts for each keyword, extracted a list of URLs used in the tweets, and created visualizations of topic models to visualize monthly keyword and sentiment trends in the relevant timeline. Lastly, the team converted the cleaned tweet collection to consistent JSONL format determined via consultation with the client for eventual integration into the VT library repository. Ultimately, the team expects their project to revitalize research at VT related to Twitter data, inspire new publications about Tunisia based on Twitter data, and lead to a greater understanding of public sentiment about political reforms in Tunisia.','[\"Twitter\", \"Tunisia\", \"Arab Spring\", \"Jasmine Revolution\", \"sentiment analysis\", \"data visualization\"]','Virginia Tech',17,'Twitter',NULL,NULL,NULL,NULL,NULL),(49,'CTE Website','Our goal for this project was to work on recreating the CTE website in order to improve the maintenance capabilities for the administrator and to fix the flaws of the site. The CTE program at Virginia Tech stands for Computational Tissue Engineering, which is an interdisciplinary graduate education program focused on the convergence of multiple scientific disciplines, with the goal of driving advances in tissue engineering. The site is split into sections that cover the Research, News, and Events of the program as well as an archive of the Faculty, Students, and Alumni of the program. The site was created with a CMS called Basecamp over 10 years ago. Because so much time had passed, it became difficult to do maintenance on the underlying code. This was mainly due to the fact that Basecamp was developed with PHP, and the specific version of PHP that the CMS was originally developed on had become outdated. This was the reason our client, Dr. Murali, who is the Associate Program Director of the CTE program, tasked us with recreating the site, as it had become too difficult to maintain the site.\nThe new site was built with many of the deliverables that had been discussed with Dr. Murali. In the completed site, the admin has the ability to dictate permissions to other users, including the ability to grant editing privileges. Student profiles that were entered into the site can easily be changed to alumni profiles. Although pages for the site cannot be toggled on and off, they can be functionally disabled and enabled within the WordPress Dashboard. New entries of profiles, such as students, would automatically be displayed on the site. The site could recognize different categories of data including “people” entries. The admin has the ability to preview pages when making changes. Fields can be changed to be a list of data types within certain editors. The admin also has the ability to create forms on the site.','[\"Website\", \"CTE\", \"Computational Tissue Engineering\"]','Virginia Tech',17,'Website',NULL,NULL,NULL,NULL,NULL),(50,'VTHSPCwebsite','Virginia Tech hosts a yearly high school coding competition. This contest is held online and contains information of current and past events. The website is dated, which makes it unappealing and hard to navigate. Our client, Dr. Godmar Back, tasked us with the goal to create a new website, with the new website meeting a few requirements.\nAnyone could theoretically use the website by visitinghttps://vthspc.cs.vt.edu/, but the main users would be those interested in the Virginia Tech High School Programming Contest. Users would be able to see detailed information about the current and past contests. This information ranges from organization details like when and where the contest is held to rules, regulations, and FAQ.\nOur team\'s main goal for this project is to not just improve the look and feel of the website but to also attract the new generation of coders. We believe that with what we completed for this project it would help those interested in coding actually take the first step. This high school programming contest is a great way to get high school students involved in the first place, but with a better and modern design, even more people will be attracted.\nThe website has three distinct type of pages. First is the Contest Description page, where the current year\'s competition is described. Second is the Past Contest pages, where each previous contest has their own page, containing their separate information. Third is the General FAQ page, where the average user can get answers to commonly asked questions. All of these pages were created using Docusarus, an open source tool for building, deploying and maintaining websites, and each page was written in Markdown/MDX. When deploying the website, Docusaurus takes the Markdown/MDX files and builds them into static HTML files to be served.\nOnce the website was completed, we had fellow classmates and student participate in a Google Forms survey, where they were asked questions about the design and structure of the website, and given test cases to gauge the difficulty of finding specific information on the website. Overall, the response to the survey was positive.\nThe website is available athttps://vthspc.cs.vt.edu/.','[\"Website\", \"Programming Contest\", \"Website Development\"]','Virginia Tech',17,'Website',NULL,NULL,NULL,NULL,NULL),(51,'Gaming and Political Extremism','The gaming and political extremism project was a collaborative effort between our student group and the VT Gamer Lab in order to test the idea that military simulation gaming communities have experienced a rise in political extremism. In order to determine if a correlation existed between the two, YouTube video data was scraped from the website, and corresponding visuals were created from said data after it had been cleaned. The visuals were then examined in order to determine if there were spikes in mil-sim YouTube video interaction during the periods of time that surrounded major political events. Through said analysis, the conclusion was that the spikes in mil-sim YouTube interaction could not be entirely related to political events, as the periodic spiking of said interactions may have been due to resurgences in a game\'s general popularity. As this is the end of the project, Our exact methods and work completed can be found within our final report and presentation.','[\"Gaming\", \"Political Extremism\", \"Extremism\", \"Mil-Sim\"]','Virginia Tech',17,'Gaming',NULL,NULL,NULL,NULL,NULL),(52,'Ocean DB','For this project, our goal was to collect relevant ocean data as well as automate the collection of future ocean data in order to help our client with his research of studying the effects of climate change on the features of the ocean and the likely effects on human health and biodiversity. By looking at relevant ocean data such as surface temperature as well as surface salinity, our client can draw conclusions on the behavior of pathogens that could lead to disease. For this semester, our goal is to build a database consisting of surface temperature and surface salinity dating as far back as 1900 that our client can access. In addition, we have built a script that our client can run in order to input new data into the database when it is released.','[\"Salinity\", \"Temperature\", \"Script\", \"Data\", \"Raster\", \"GIS\"]','Virginia Tech',17,'Salinity',NULL,NULL,NULL,NULL,NULL),(53,'Women Climate Change','For decades women have been underrepresented in academia regardless of subject or profession. This project aims to shed light on women’s achievements specifically in the intersection of Climate Change and Disease by generating a replicable matching algorithm that can be applied to label large datasets with the sex of their authors. This data will then be turned into a variety of visualizations that will help more accurately depict women’s involvement in academia. The team utilized an open source MIT web scraping tool to scrape PubMed, an online directory of research papers to formulate the dataset for this project. The scraped data was left in CSV format, which we then piped into a Python file to conduct the processing. We have downloaded publicly available datasets labeled with the most common names in Canada, the USA, Mexico, Brazil, France, Finland, Australia, and India to create our labeled names repository. The Python routine we used holds the labeled names repository as its backend and looks for matches between the names in the input files, the author’s names and the names in the labeled directory. Following the application of this matching algorithm on our scraped dataset, the now labeled data was placed into Tableau to generate our visualizations. It was mentioned earlier that this project specifically aims to highlight women’s accomplishments in the field of Climate Change and Disease, but our overarching goal with this project is to design a replicable approach that can be easily applied to other fields such as “Agriculture” or “Occupational Therapy”. Through this project we aim to help women get the accreditation they deserve in a variety of fields, with the start being climate change and disease. This project will also provide researchers/data analysts with an easy to use tool in the future to quickly label named datasets\nmore accurately than current tools on the market.','[\"Climate Change\", \"Disease\", \"Women\", \"Labeling\", \"Python\", \"Blacksburg, VA\", \"PubMed\", \"Scraping\"]','Virginia Tech',17,'Climate Change',NULL,NULL,NULL,NULL,NULL),(54,'HCIInterfaceForStroke','The Robotics and Sensorimotor Control Laboratory (RoSenCo), led by Dr. Netta Gurari, in the Department of Biomedical Engineering and Mechanics, is dedicated to conducting neurological research with stroke survivors. Stroke is a leading cause of disability worldwide, and RoSenCo aims to contribute to the development of treatments by studying the effects of stroke on tactile perception. RoSenCo researchers have designed a series of experiments to advance their goals, and this project focuses on developing software to facilitate these experiments in a flexible and expandable manner. The software, known as RoSenCo Experiment Manager (RoSenCoExMan), has been implemented to control actuators and collect data from sensors at a rate of 1600Hz. It also provides real-time graphing of selected data streams, displays text-to-speech-powered audio-visual instructions for participants, and saves collected data in the required format for subsequent analysis. RoSenCoExMan is written in Python, utilizing various libraries for implementing features such as graphing, hardware access, and text-to-speech capabilities. The software employs multi-processing techniques to achieve the required performance. Opportunities for future work include extending the audio-visual participant feedback functionality to enable experiments utilizing more dynamic visuals, and the addition of a graphical user interface.','[\"Neuroscience\", \"Python\", \"Stroke\", \"Magnetic Resonance Imaging\", \"Text to speech\", \"Pneumatics\", \"Real-time plotting\"]','Virginia Tech',17,'Neuroscience',NULL,NULL,NULL,NULL,NULL),(55,'Shark Detection Classification','In recent years, collaborative work between previous CS4624 capstone groups and Dr. Francesco Ferretti\'s team contributed to the development of the SharkPulse project. With the goal of enhancing shark conservation and elevating public awareness through the collection and analysis of global, crowd-sourced shark sightings data, SharkPulse developed a data / machine-learning pipeline to detect and classify sharks from a given image. This report presents the improvement of the machine-learning pipeline previously established in \"Shark detection and classification with machine learning\" (Jenrette et al.). The improvements to the pipeline increased classification accuracy as well as species breadth. Mainly, the existing classification architectures are replaced with Transformers (ViTs). The updated shark identifier achieves an accuracy of 96%, the updated genus classifier, an accuracy of 72%, and the updated genus-specific species classifiers, an average accuracy of 74%. This updated classification system is able to classify 27 genera and 51 species. A framework for automating data-collection, model training, and maintenance is also introduced. Potential future work is discussed, including integrating the model into the SharkPulse platform.','[\"shark\", \"image classification\", \"database update\"]','Virginia Tech',17,'shark',NULL,NULL,NULL,NULL,NULL),(56,'Pd-L2Ork','Over the past several years Professor Ico Bukvic has been developing his own extension to the visual programming language of Pd, called Pd-L2Ork.\nThis software was designed to take the functionality of Pd, a computer music programming software system, and apply it to Virginia Tech\'s Linux Laptop Orchestra, or L2Ork.\nSince 2010, it has been used extensively in education, research, and production. Another product of this research is the K12 mode that Pd-L2Ork has more recently introduced with the help of a prior CS Capstone Team.\nK12 mode is targeted specifically at beginners, to introduce sound programming and design, as well as for elementary to high-school students.\nOur team\'s job was to expand on this existing software in two distinct ways.\nThe first way we expanded the software\'s functionality is through the implementation of object tooltips.\nSince Pd-L2Ork is a visual programming environment, the objects you create in it take up physical space and can be hovered over.\nEach object (or patch) has a series of inlets and outlets that can be used to pass data into an object and receive output from an object, respectively.\nEach of these inlets and outlets require specific types of data to be passed into them for different uses.\nYou might call the values passed into inlets parameters to a function/method.\nConsequently, you might call the values received through the outlets of an object the return value of a function/method.\nThe way that the programmer identifies what needs to be passed into each inlet and what is received through each outlet is through tooltips.\nHover over one of these inlets/outlets to view the object\'s tooltips, which describes the type of data the inlet needs and what the data should describe.\nOur team\'s task was to take the descriptions of each object\'s parameter(s) and return value(s) from the object\'s documentation and create tooltips for each object in every patch. This task was composed of two parts: tooltips for an object itself and tooltips for inlets and outlets of every object. In order to accomplish the first part, we take the description of the object out of the index and attach it as a title tag to the particular object. For the second part, we needed to parse in the information of the inlets and the outlets of the objects from the index and also attach it as a title tag to the particular inlet or outlet. The deliverable for this task was a merge-able code branch with code to enable this feature, which we gave our client access to.\nThe other task of our team was to integrate a button that moves a Pd-L2Ork patch onto a web browser.\nThis first involves compiling the original Pd-L2Ork code (written in C) to Web Assembly through a tool called Emscripten.\nThis took up the majority of our time because of the large code base and many errors found by the Emscripten compiler that needed to be corrected.\nOnce we edited the code base to make it Emscripten compatible and compiled it, we included the generated JavaScript file in our front-end code and linked the audio back-end to the front-end on the browser.\nThis was a difficult task, but our team gained valuable insight from similar examples in PdWebParty and Purr-Data.\nIn order to accomplish this task, we performed the necessary research to begin implementation, solved problems through file comparison and classic debugging strategies, and we asked Dr. Bukvic for help promptly when we needed it.\nThe deliverable for this task was a button that could gather up all necessary files to display a patch current in Pd-L2Ork onto a web browser.\nWe did not complete the work needed for this deliverable, but have provided documentation (in the sections for the Developer\'s Manual and on Future Work) regarding what has been done by the team, and what can be done by future teams to reach this deliverable.','[\"Pd-L2Ork\", \"Computer Music\", \"Tooltips\", \"Emscripten\"]','Virginia Tech',17,'Pd-L2Ork',NULL,NULL,NULL,NULL,NULL),(57,'Role Playing Game AI System','For this project our team was tasked to create an AI for an immersive role playing game that contains a modular ability system. The AI must be capable of interacting with the ability system effectively. The AI must be fair, and as such will be constrained by the same constraints as the players. It must have the same abilities, follow the same rules, and only operate on information that is also available to the players. Additionally, the AI must be capable of being an adversary or an ally to the player. We have implemented a basic routine-based approach to solve the problem. This approach uses a \"Black Box\" function to deduce a general course of action to take, such as an attack, defensive, or healing ability. Then, the routine decides on the best approach to achieve its goal and adds specific actions to an action queue. Additionally, we have added ability tags that the AI uses in order to understand what purpose each ability serves. With the use of such an approach, we hoped to create an AI that is both challenging and is immersive to play with or against. We performed manual testing by inserting our AI into various pre-defined scenarios. We have also documented our code with comments that explain the functionality of our code. Our deliverable content is mainly contained within the files \"BlackBox.cs\" and \"State.cs\" with small additions to other files. As the game is still in the process of development, much of its functionality is still incomplete. As such, we had to work around some missing functionality, such as incomplete character classes and abilities, by making some assumptions about the final vision of the client. Therefore, the Black Box and the routines will need to be modified as new functionality is introduced into the game in order to better make use of that new functionality.','[\"video game\", \"AI\", \"NPC\", \"game design\", \"role playing game\", \"decision optimization\", \"Unity\"]','Virginia Tech',17,'video game',NULL,NULL,NULL,NULL,NULL),(58,'Visualizing eTextbook Study Sessions Final Report','This project aims to implement a web interface to visualize interactions generated from students’ study sessions using OpenDSA content. Our goal is to help instructors conveniently understand how their students learn and interact with OpenDSA and identify areas of improvement in the system to aid the learning process. There is currently a lack of tools available for professors to use to visualize OpenDSA users\' learning process. OpenDSA is used by many classes at Virginia Tech as a supplement to student’s learning material. Having these visualization tools would help see if students are engaged in the material, and how they are utilizing the eTextbook.\nWe will be taking the interaction data from a MySQL database. Then, we will be utilizing a modified version of Samnyeong Heo’s python script to convert the raw interaction data to abstract data for visualizations [3]. Next, we will create visualizations using Python libraries such as Numpy, pandas, and more. Finally, we will create a web interface on PowerBI to show our visualizations.\nThe final deliverables of this project include a fully functional web interface and a visualization tool for the interaction data on OpenDSA. We must also create a final report and prepare our presentation to give in front of our class, CS_4624, Multimedia, Hypertext, and Information. We will submit the above to our client Mohammed Farghally and our professor Mohamed Farag. We will be iterating upon our report and presentation as we continue to design and develop our interface and visualizations. We will be including more testing information and our evaluations and analysis of our envisioned system.','[\"OpenDSA, Python Visualizations, Interactive Dashboard, eTextbook, Multimedia, Hypertext, Information Access\"]','N/A',17,'OpenDSA, Python Visualizations, Interactive Dashboard, eTextbook, Multimedia, Hypertext, Information Access',NULL,NULL,NULL,NULL,NULL),(59,'A Literary Review on the Current State of Drone Technology in Regard to Conservation','This paper will review the types and use of unmanned aerial vehicles (UAVs) in conservation. Drones are being used as the preferred method of monitoring terrestrial and aquatic wildlife in difficult areas, thanks to the low price and efficiency that these tools offer. The paper discusses the three main types of drones: fixed-wing, rotary-wing, and hybrid. Fixed-wing drones are best suited for general surveillance of large areas and long-distance flights, while rotary-wing drones are small, light, and maneuverable, making them ideal for tasks such as photography, filmography, inspection, and surveillance. Hybrid drones are more complex, combining fixed and rotary wings or rotors. The paper also explores the potential benefits of adding solar panels to drones to improve their energy efficiency.\nMultiple instances of the successful use of drones in the field were documented, including drones being used to identify objects in water, land, or air. Advanced machine learning algorithms were proven to be highly effective in identifying targets for military, conservation, and other purposes. The optimal placement of docking stations for aerial drones was discussed, and how they could be found using a new algorithm, back-and-forth-k-opt simulated annealing, or BFKSA was also discussed.\nOverall, drones provide a cost-effective and efficient way to monitor and protect wildlife, making them an important tool for conservationists.','[\"Drones\", \"Conservation\", \"Machine Learning\", \"UAV\", \"AUV\", \"Vision-based-tracking\"]','Virginia Tech',17,'Drones',NULL,NULL,NULL,NULL,NULL),(60,'TaxDataDashboard','The Internal Revenue Service (IRS) Form 990 is a form filed by nonprofit organizations. These forms collect information about the organization\'s finances, officers, contributors, and other information. These forms are required to be published by law. Starting in 2011, the IRS made the data available in an AWS bucket. As of 2020, the IRS has published the data on its website.\nIn Spring 2022, Dr. Zach approached Dr. Fox with the TaxData project. He began working with groups of students to compile this data into a database and to begin analyzing the data. In the Spring 2022 semester, a group of students from the CS 4624: Multimedia, Hypertext, and Information Access capstone. This capstone built a basic database and began to parse the data. In the Fall 2022 semester, one student took over the project and modified the database to be more effective.\nThis semester, we took over the project from where it was previously left off. Dr. Zach tasked us with creating visualizations that show correlations in the data. He wanted to be able to easily input a list of EINs and showcase a variety of graphs and charts for that list. These visualizations were then to be put on an interactive dashboard where all visualizations could be seen in one easy-to-access location.\nThe report outlines the additional work we completed this semester through: Objectives, Deliverables, Requirements, Design, Implementation, Testing, User Manual, Developer\'s Manual, and Lessons Learned.\nThe work completed this semester includes an interactive dashboard that will allow users to view the visualizations created and explore the data with different filters, as well as various visualizations completed based on a list given to us by Dr. Zach. In the future, we recommend the database be made more efficient by combining the tables so they are no longer by year, but only by form type. We also believe that it would be more efficient for both users and developers if the database and dashboard be published to a server that is easier to access.','[\"Form 990\", \"Form 990-EZ\", \"Schedule J\", \"Visualization\", \"Dashboard\", \"Tourism\"]','Virginia Tech',17,'Form 990',NULL,NULL,NULL,NULL,NULL),(61,'Interactive Text Classification & Evaluation','Text classification is a critical task in natural language processing that assigns predefined categories or labels to text documents. It has become even more important than ever with the rapid growth in the sheer number of text documents with the introduction of social media. It is highly practical to have a machine classify these documents rather than a human manually identifying the contents of a document. Starting in 2007, a project from the Google Summer of Code program released a free Python machine-learning library that featured many classification, regression, and clustering tools. This project will be based on this library to perform the necessary text classification from generating a model to outputting a prediction given text.\nThe goal of this project is to create an interactive text classifier with the web application, user, and developer manuals as deliverables. Our team will work closely with a client to ensure that our application is on track and fits their needs. The main objective is to develop a web application that allows the user to interact with a machine-learning text classification model by tracking its correctness based on the principle of supervised machine learning. The UI should display keywords that were used to classify the text and highlight them to the user. The interactive portion of the application comes from the fact that the user will be able to classify the text themselves, mark down whether the highlighted text is right or wrong and save the document for future reference.\nThis project is the first of its kind this Spring 2023 semester and no previous groups in other semesters have done a project like this. Our group will have to start from scratch, and use tools and technologies that are unfamiliar to us but have the willingness to learn them. Our approach in building this application is to use a similar stack to MERN but instead of Expressjs, we opt to use Flask as our back-end server to handle our scikit-learn machine learning script. We use Reactjs as the front-end framework, and Nodejs to run the application and use other features. Lastly, we use MongoDB as our database to store documents, classifications, and other important attributes. We hope that our project will provide valuable insight into the effectiveness and power of machine learning and allow those who wish to continue our project to be able to with ease through reading our user and developer manuals in this report.','[\"Text\", \"AI\", \"Artificial Intelligence\", \"ML\", \"Machine Learning\", \"Classification\"]','Virginia Tech',17,'Text',NULL,NULL,NULL,NULL,NULL),(62,'Traffic Simulation Management System','The Integration Traffic Simulator is software used by researchers, traffic planners, and traffic engineers from all over the world. The software can be helpful to simulate important factors such as safety or environmental risks after being given input data such as road networks, speed limits in the network, number and types of cars that usually travel on the network, etc[1]. However, the software is currently only able to run one simulation at a time with no way to store metadata and no easy way to re-run old simulations. This means that using the system, especially for someone who regularly uses the simulator and might have to run hundreds or thousands of simulations, could take an exceptionally long period of time and old simulations could get misplaced. To fix this problem, our team was tasked with creating a web application. The goal of the application was to provide one convenient system capable of easily running new simulations and storing hundreds of old simulations with all their information for later reference. We successfully implemented a fully usable system that allows users to run new simulations, see previous simulations, view the outputs of any simulation, re-run old simulations, download simulations as a ZIP file with everything needed to reproduce them, use custom versions of Integration, and more. We used React for our frontend, we used Node.js for the backend, and we connected the two using an API. MongoDB is used to store simulation metadata while the filesystem stores actual simulation files. In this report, we will fully discuss the requirements, design, and implementation for the application. In addition, we have included a user manual explaining how to use the application\'s features and a developer manual with all information required for installation and future development.',NULL,'Virginia Tech',17,'N/A',NULL,NULL,NULL,NULL,NULL),(63,'Crisis Events Webpages Archiving','Webpages disappear online rapidly. When something like a crisis event occurs,\nit is very important to retrieve and preserve all web pages related to that event\nbefore they disappear in an effort to record their digital history. Web archiving\nis a technology that enables storing webpages in a format called WARC (Web\nARChive). WARC records save all the pertinent information required to replay\nthe webpage as it was online, such as the HTML data, files, and ads.\nThe goal of this project is to implement a web archiving system that can\narchive a large number of web pages depending on user input. To solve this\ntask, we have implemented two main scripts using Python due to its known\nscripting capabilities, and a user interface to provide a recording and replaying\nfunctionality to our system.\nThe first script’s purpose is to go through user-given websites and archive\nthem in the Web ARChive (WARC) format, with one WARC file per website.\nIt is capable of accepting a URL and a collection name to direct the archived\nURL to, using various Python libraries and packages to do so like pywb and\nsubprocess, as well as waiting between URLs to not overload a server with\nrequests, avoiding being blocked by the target website(s). It operates by reading\nURLs from a given text file and utilizes multithreading for an overall faster\nperformance during archival.\nThe second script’s purpose is to replay WARC files, showing the archived\nwebpage(s) as it initially was before archival. It is capable of accepting a WARC\nfile (.warc or .warc.gz) inputted by the user to be displayed using the webbrowser\nlibrary on the user’s own web browser. Similar Python libraries are used in this\nscript in its implementation.\nThe user interface was created using Node.js and React and is based on\npywb’s WebUI, serving to provide the user with an easier way to use the afore-\nmentioned scripts. A Flask script then used to link the UI and scripts for\ntogether, allowing for greater usability, with the functions the scripts have to\noffer to be available for easier use. Using the UI, a user can search for an\narchived page using its URL and is presented with a pywb calendar to view the\nwebsite capture(s) in their local browser.\nThe final deliverables of this project include completed scripts, a user inter-\nface, a set of presentation slides, and a final report submitted to our professor\nand client, Mohamed Farag. The report and presentations show the progress\nour team made throughout the stages of our project’s development.','[\"webpages\", \"archive\", \"WARC\", \"URL\", \"retrieve\", \"web archive\"]','N/A',17,'webpages',NULL,NULL,NULL,NULL,NULL),(64,'Smart Parking Recommender Mobile Application','Organized parking is an important aspect of living amongst each other in a society. Most people identify parking as a cumbersome activity because there is typically a lot of stress involved when interpreting vague parking rules. However, parking is a necessary evil as it keeps matters fair and civilized. The necessity for parking generates a huge amount of revenue for the economy.\nThe parking industry consists of a multitude of different aspects such as parking lot management, parking garage construction and management, and valet parking services. The market size for parking measured in revenue is estimated to be around 8.2 billion. Parking is an essential aspect of transportation as regulations and policies are necessary to improve transportation efficiency. As more people move into an area, the more the parking industry is expected to grow.\nWith universities always striving to recruit new students, an enormous parking infrastructure is needed to maintain the peace and stability on campus. Unfortunately, this often comes with painfully rigid administration that often inconveniences the daily lives of students.\nParking becomes a headache to many college students due to the increasing number of students, but stagnant number of parking spaces. This event drives down the availability of parking spaces and takes away from students’ learning experience as they often have to plan hours ahead to make it to class.\nThe smart parking application aims to provide its users with a more convenient parking experience over its nonusers. The application only supports parking at James Madison University at the current moment, but the goal is to expand to all universities. The core premise on how this application is able to provide this functionality is by analyzing the trends in parking spot occupancy and time. James Madison University installed sensors in their parking spaces that are able to track if it is occupied at all times of the day. The parking status is updated live on the official university website. The smart parking application utilizes the API that tracks these updates and observes for patterns to provide the user the most optimal place to park.\nThe tech stack for the smart parking application will be the MERN stack with MongoDB Atlas for real-time data utilization. MongoDB is chosen because of its malleable document structure. The backend is an Express/Node.js server with Python for the machine learning program. The frontend is a React Native iOS/Android application that fetches data from the API, and uses several component libraries for UI design, including Lottie, RNUI, and React Native Map. The app allows any person to use it without requiring them to create an account.\nThe purpose of the application is to provide users an advantage in parking over non users. The reasoning behind this is if everyone used the application to get the best parking spaces, then there would be a paradoxical effect and then no one could get the best parking spaces. The main goal here is to give the users of the application a more convenient parking solution, but resolving the lack of parking spaces is a potential issue to tackle in the future.','[\"smart parking\", \"parking\", \"mobile application\", \"recommender\", \"James Madison University\"]','Virginia Tech',17,'smart parking',NULL,NULL,NULL,NULL,NULL),(65,'Object Detection','For this project, our team took 20,000 image samples from ETDs and annotated them using a Python package called PyLabel. PyLabel is an open-source Python library used to label PDFs. PyLabel can also take a trained dataset and use it for AI-aided annotations. We also created a pipeline in order to divide the dataset into equal pieces, where a user can select the number of samples they want to annotate. Then old sample data is cleared out and replaced with new sample data that contains classes with low accuracy. Finally, we saved the annotations as a YOLOv7 .txt file which is accumulated in order to retrain the model with 10,000 annotated images and finally with 20,000 annotated pages. With these annotated pages we conducted an experiment timing how long it takes to annotate the pages to see the improvement of the average time per page to annotate as the different models were trained. We concluded that the model trained with 10,000 pages was significantly faster than the original model.','[\"Machine Learning\", \"Labeling\", \"Object Detection\"]','Virginia Tech',14,'Machine Learning',NULL,NULL,NULL,NULL,NULL),(66,'CS3604 Case Study Library II','Examples of applied knowledge are vital to any university student looking to develop a deep and abiding understanding of their major. Such examples pour the foundation of changing the world through novel, thought provoking innovations and advancements by offering insight into how an idea can turn into a reality. This Case Study Library was developed with the purpose of providing students and others a litany of various cases in which specific Computer Science topics were relevant in industry.\nCase studies, in this context, are the multimedia presentations by students of Virginia Tech in CS3604: Professionalism in Computing that take place at the end of the semester. Students are instructed to pick an example from industry pertaining to the Internet, Artificial Intelligence, Intellectual Property, Commerce, or Privacy. Through thorough research and the learnings from the class itself, the students construct their presentations on their topic of choice.\nThis project is the second iteration of the Case Study Library. From the previous project group’s work, the library held more than 500 case studies by students of Professionalism in Computing. It was evident that there were some major aspects that could be improved upon. These included titling the case studies by file name, no classification of case studies by course topic, and no thumbnail images. Along with this, a significant percentage of files were unable to be displayed due to file format issues. The burden of uploading case studies was on the professor, who needed to run a Python script to batch upload site items. This iteration of the project had a solid understanding of what needed to be done, including the addition of a student upload page, stylistic corrections, search parameter specification, and thumbnail images for files. Search filtering, student authentication, collections by course topic, and functionality to upload more than one case study file were also added. Changes to the site were made via a frontend administrative page as well as making additions and modifications to the code base.\nWith the Case Study Library having been improved, it now stands as an effective tool that current students of Professionalism in Computing can reference while they work on their own case studies.','[\"CS3604\", \"Case Study\", \"VTDLP\", \"Computer Science\", \"Artificial Intelligence\", \"Privacy\", \"ICT\", \"Commerce\", \"Intellectual Property\", \"Computing\", \"Library\"]','Virginia Tech.',14,'CS3604',NULL,NULL,NULL,NULL,NULL),(67,'Safe Eating App 2','Food allergies affect about 26 million, or 10%, of the overall US population; even with varying levels of severity, allergies continue to reduce many citizens’ quality of life. Many allergies are prevalent since birth, but certain allergies, like Alpha-Gal Syndrome, can be acquired at any point in life. Alpha-Gal Syndrome in particular is spread through Lone-Star Tick bites.  This syndrome is a mammalian meat allergy involving an IgE antibody response to galactose-α-1,3-galactose. This sugar, known as alpha-gal, is found in all mammals except monkeys, apes, and humans. Alpha-Gal syndrome is not well-known; in fact, the majority of our group had never heard of this syndrome until we started working with our clients.\nFood labels are required, by law, to list all ingredients and allergens involved in the production of the food item.  While all products abide by this law, there are instances in which secondary ingredients are used to refine the main ingredients, and these secondary ingredients are not listed or mentioned. This lack of information can prove to be deadly to people with allergies, especially those affected by Alpha-Gal Syndrome. For example, sugar starts off with a brown tinge.  In order to remove the brown tinge, to achieve the look of white sugar, the sugar can be refined with crushed cattle bones, which can possibly trigger an allergic reaction for those affected by Alpha-Gal Syndrome.  Companies are not required to disclose this information, since \"Crushed Cattle Bones\" are not a main ingredient of white sugar, but they can be life-threatening.\nTo tackle this issue, we created an IOS application that gathers allergy information from the user. With this information stored, the user can scan the barcode of a product they are interested in purchasing, and the app will return a signal whether the food is safe, dangerous, or “better to be cautious” based on the user’s inputted allergies. Additionally, there is a feature for the user to scan products for dependents with allergies, or young/older users that cannot use the application.\nTo gather information about the ingredients used in different foods, we are using an open-source database that allows users to add products and their ingredients so the database is always up to date. The database comes in many different formats like JSON and CSV, so we can use different methods to parse the data to retrieve information as fast as possible.\nThe previous iteration of this project focused only on the food available at Virginia Tech Dining Halls, but to make it more scalable, our clients approved our idea of scanning grocery store food, since it is their most troublesome area.\nThe final deliverables include a fully functional IOS application (along with the application’s code, executable file, and a video demonstration), a final report, and a presentation to showcase our application, as well as the thought process to the class and our clients, Deborah Nichols and Candice Matthis.','[\"Alpha-Gal Syndrome\", \"allergies\", \"iOS Application\", \"barcode-scanning\", \"Two Alpha Gals\", \"groceries\"]','Virginia Tech',14,'Alpha-Gal Syndrome',NULL,NULL,NULL,NULL,NULL),(68,'AI-Assisted Annotation of Medical Images','In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple image segments. More precisely, it’s the process of assigning a label to every pixel in an image so that pixels with the same label share certain characteristics. Image segmentation is an important step in almost any medical image study. Segments are used in images from microscopes that show us different types of cells and these cells contain hundreds of organelles and macromolecular assemblies. Cell Segmentation is the task of splitting a microscopic image domain into lots of different segments, which represent individual instances of cells, however, this requires enormous time for domain experts to label manually and thus the need for AI-Assisted annotation of medical Images. Our project will aid the annotators in receiving images quickly and easily through our web application and performing the predictions on these images.','[\"Website\", \"Segmentation\", \"AWS\", \"AI\", \"React\", \"Cells\", \"Medical\", \"Microscopic\"]','Virginia Tech',14,'Website',NULL,NULL,NULL,NULL,NULL),(69,'Immersive Archaeology','For this project, our job is to create a website that allows users to upload 3D scanning data from archaeological sites and artifacts found in a particular dig site for virtual analysis of corroborating evidence from the results of fieldwork. In archeology, the post-excavation analysis phase is typically the most time-consuming aspect of the archaeology process. The proposed Immersive Archaeology System would primarily contribute to this post-excavation phase, by creating an immersive environment for archeologists to connect. In the immersive environment, archeologists can gather potentially relevant data about a site and its artifacts from a library, for archeologists to analyze and interpret. This enables more flexible archaeological practices since both field-based and lab-based archeologists can corroborate via the immersive environment. Additionally, educators can make use of this system to teach learners about dig sites and their artifacts remotely. For this semester, our goal is to build the foundation of the project by creating a static, one-user, web environment. This semester, we created a website that allows users to upload artifacts and dig sites. With the artifacts and dig sites uploaded, the user can then view them in a 2D and XR mode. Future work can include functions such as multi-user interaction and discussion.','[\"XR\", \"Archeology\", \"Immersive\"]','Virginia Tech',14,'XR',NULL,NULL,NULL,NULL,NULL),(70,'Literature Mining of Antibiotic Resistance','Antibiotic resistance is a very important part of human health that needs to be constantly kept track of in order to give proper medication to patients. However, the large number of papers that come out makes it very difficult for a human to keep up. Thus, this project aims to create an automated way to extract meaningful information out of antibiotic resistance papers. It aims to create an automated way to collect and parse the papers. Then, it tries to analyze the papers and extract meaningful information out of it. In order to do so the project was divided into 4 main parts. In the first part antibiotic resistance genes were gathered to be used as search queries. In the second part PubMed papers were gathered using the genes. In the third part DeepEventMine was used to extract information from the papers. In the last part the extracted information was analyzed. The results from the data gathering process were good and lots of relevant papers were gathered. DeepEventMine was run successfully but most of the output was not very useful. The analysis part was mostly focused on statistical analysis and gave some useful results. As for the final deliverable the code used for data collection works well and can be used on other projects that work with medical papers. Detailed information on how to work with DeepEventMine can be found in this report. Finally, more work can be done in the analysis section to produce more information.','[\"antibiotic resistance\", \"data mining\", \"event extraction\"]','Virginia Tech',14,'antibiotic resistance',NULL,NULL,NULL,NULL,NULL),(71,'ThermaSENSE App','ThermaSENSE is a company that utilizes AccuTemp, a patent-pending technology that enables non-invasive, real-time, internal temperature measurement of almost any object. This technology has applications in industries such as medical, agricultural, and industrial processes. These devices can be used in a wide variety of locations, not all of which will be easy to physically access. To make use of this technology, a pipeline that remotely captures, structures, and visualizes this data is necessary.\nThe proposed system is an app-to-cloud pipeline that takes the Bluetooth data from ThermaSENSE devices into an iOS app, which processes and visualizes the data while storing it in Firebase where it can be associated with user accounts. This pipeline will be broken down into 3 major parts, those being the circuit board (representing the ThermaSENSE device), the iOS app, and the cloud storage. The circuit board processes sensor data in a local offline buffer and then transmits the data to the iOS app once a connection is established. The app has the ability to sign-up/log-in users, connect to ThermaSENSE devices, process and visualize sensor data, and push/pull sensor data from the cloud account associated with the user. We store user accounts and their sensor data with Firebase, which allows the retrieval of the sensor data while also providing a structure about how that data will be stored.\nThis project utilizes previous work given by our contact person, Ali Roghani, which includes Arduino code to send Bluetooth data, as well as an Adafruit Feather nRF52840 Express board. Arduino is an open-source electronics platform that we use to program the software for the device. Adafruit is a company that manufactures programmable boards, and for this project, we will be using an Adafruit Feather nRF52840 Express board. This provides the foundation for sending Bluetooth data and how the readings will be formatted for parsing. On the board, we implemented features such as an offline buffer and flexible data formatting. We created a ThermaSENSE iOS app that will capture the sensor data from the device and visualize the live readings. The app allows users to create an account that will be synced with Firebase to store sensor data. Using the user accounts, we can pull historical data into the app and visualize longer periods.\nOur final deliverables are the Arduino code for an Adafruit Feather nRF52840 Sense board, the ThermaSENSE iOS app, an admin Firebase cloud account, documentation for users and developers, and a report. Over-the-air board updates were not supported by Adafruit at the time of this project, but research and possible implementations were documented for future reference. Since this project has documentation for the board code, app code, and cloud admin structure, our client can maintain and improve this project in the future.','[\"Cloud Storage\", \"iOS App\", \"Adafruit Board\", \"Arduino\", \"Xcode\", \"Firebase\"]','Virginia Tech',14,'Cloud Storage',NULL,NULL,NULL,NULL,NULL),(72,'VTCDD Showcase','The Virginia Tech Center of Drug Discovery (VTCDD) is an interdisciplinary organization consisting of around 50 members across 17 STEM-related departments. To showcase the organization’s members, they maintain a website to highlight their members’ accomplishments. Specifically, there is a subset of pages dedicated to listing the publications and patents created by VTCDD members relevant to the organization. These pages have not been updated since 2017 and were created by a site contributor that had minimal experience utilizing Virginia Tech’s content management system. Our goal as a team was to create a solution to better showcase the patent and publication accomplishments of the VTCDD.\nStarting the project, we had complete creative liberty when it came to presenting the VTCDD publications and patents. Initially, the Publications page and the Patents page were endlessly scrolling static pages, making the sites poorly navigable. The publications only listed the citation, which made it difficult to quickly parse information from, especially since there were a variety of different citation formats. The site coordinator had to manually add new publications and patents to the site, which was a tedious process. With some of the site’s areas of improvement in mind, our goal was to make the sites more dynamic, and standardize the information through the use of filterable, paginated tables.\nThe deliverables consist of a final report, a web-scraper for Publication data, and web pages for the VTCDD site created on Ensemble: one for the Publications page, one for the Patents page, and another for the home page. The report shows our progress throughout our project lifecycle, ranging from ideation to testing. There are manuals for users and developers to ensure the web pages are properly maintained. Lastly, we discuss lessons learned and ideas for future development.','[\"VTCDD\", \"Center of Drug Discovery\", \"Patents\", \"Publications\"]','Virginia Tech',14,'VTCDD',NULL,NULL,NULL,NULL,NULL),(73,'Pd-L2Ork','Pd-L2Ork is an interactive visual programming environment that is designed for live manipulation of multimedia and digital signals. Pd-L2Ork is built off of the PureData programming language, which was first developed in the 1990s. Pd-L2Ork features a variety of pre-built components, all as source files, that allow users to experiment with the creation and manipulation of sounds, among other things. The programming environment also allows users to put together their own components for unique manipulations and compositions. The developers of Pd-L2Ork are aiming to create an application that is easy to learn and easy to use. To that end, they have begun to transition the application for web browser support. The work done by our group this semester therefore focuses primarily on improving the usability of the program by adding in certain features, as well as completing the transition of the application to be fully supported in a web browser.\nOur group’s work dealt primarily with building upon pre-existing features in order to improve functionality of a real-time visual programming environment for A/V and graphics processing. Our first task was to implement a tooltips functionality for the objects in Pd-L2Ork. This would enable users to see helpful information displayed whenever hovering their mouse over objects, or over certain parts of an object. For this task, we parsed through an index of information within the Pd-L2Ork file directory and connected each object to their corresponding tooltip found from that index, allowing for the functionality of displaying tooltips. The deliverable for this task was a merge-able patch containing our code allowing for this functionality, of which we sent to our client. Next, we were tasked with implementing a K12 editing mode, different from the default mode and settings for creating and editing files in Pd-L2Ork. This mode deliberately restricts certain features that are normally available when a user wishes to create and edit files. It also provides users with a menu interface that is designed to make creating and editing easier. As the name suggests, this functionality is aimed at K-12th grade students. For this task, we created a K12 menu that contains K12 objects that the user can place. This menu is visible when the user is in K12-mode, but can also be shown in the normal mode via the “Put” menu. We also removed certain menus from the top menu in order to reduce the amount of features the user can use. The deliverable for this task was a patch that we sent to the client containing all our additions involving the K12-mode.','[\"Tooltips\", \"K12 Mode\", \"interactive visual programming environment\"]','Virginia Tech',14,'Tooltips',NULL,NULL,NULL,NULL,NULL),(74,'ThermaSENSE Website','ThermaSENSE Corporation has developed several non-invasive sensing solutions for problems faced in numerous industries. It was founded by Ali Roghani. As part of his corporation, he needs an updated website. The current website is outdated and based on a platform called Wix. This project aims to implement new websites that are visually appealing, easy to use, and have a professional feel. To solve this task, we implemented two websites.\nThe first website was created using a platform called WordPress. It will be used to display all the information needed about the ThermaSENSE corporation. This includes text, images, and videos. The second website was created using React and React libraries. It will handle user login and graph data from the Firebase cloud database that is shared and populated with data by the ThermaSENSE app team.','[\"ThermaSENSE\", \"WordPress\", \"React\"]','Virginia Tech',14,'ThermaSENSE',NULL,NULL,NULL,NULL,NULL),(75,'Memory Modules','For many people, with age comes a variety of problems that can make living difficult. While the physical problems are prevalent and often easy to recognize, we must not forget to address the mental side of the difficulties that often come as part of the aging process. Providing comprehensive care for aging adults and others with cognitive issues is a noble cause that requires the cooperation of many different parties, and being able to express these goals to the world will improve the outreach of the program and help even more people.\nThe purpose of the Memory Modules project is to produce short videos that can be used to advertise to the public the mission of and services provided by the Engagement Center for Creative Aging (ECCA), an organization dedicated to the aforementioned cause. Over the course of this semester, our team of students has met periodically in order to film 360° video tours of the center as well as conduct interviews with many of the ECCA team members that make the mission a reality. The videos were edited into two productions that will be made available through the ECCA website and several social media platforms.\nPutting this project into motion has required knowledge of the inner workings of ECCA and getting to know the people behind the project. We performed extensive research both online and in person to tune in to the values that the organization stands for. Multiple meetings and interviews with their staff members were held to help us prepare to create the videos for this project. In addition, we were able to obtain and work with a variety of equipment to carry out our project. The use of a 360° camera and 4K camcorder to capture a tour of the center and the outside patio as well as an interview video with staff from the center helped to make the project a unique experience.','[\"MemoryModules\", \"ECCA\", \"ADS\", \"Adult Day Services\", \"Engagement Center for Creative Aging\", \"Immersive Video\", \"Virtual Tour\", \"Interview\"]','Virginia Tech',14,'MemoryModules',NULL,NULL,NULL,NULL,NULL),(76,'TaxData2','The Internal Revenue Service (IRS) Form 990 is filed by nonprofit organizations, such as tourism offices, recording financial information, contributors, officers and their positions, grants, and various other information about the organization. Starting in 2011, many of these forms were electronically filed into Extensible Markup Language (XML) format and were made public on the Amazon Web Services (AWS) and IRS websites. Although this information is available to the public, XML files are difficult to read. There can be over 1,000 lines of tax jargon in the XML files. Dr. Zach realized this and approached Dr. Fox with the TaxData project. The goal of this project is to convert tax data in flat file and XML file format into a readable format and run some preliminary analyses. TaxData was first introduced into the CS 4624: Multimedia, Hypertext, and Information Access capstone class during the Spring 2022 semester. A group of students worked on this project from January 2022 to May 2022. I have picked up this project for the Fall 2022 semester to make some improvements. The report outlines my work on the project through: Objectives, Deliverables, Requirements, Design, Implementation, Testing, User Manual, Developer\'s Manual, and Lessons Learned.','[\"Form 990\", \"Form 990-EZ\", \"Form 990-PF\", \"Schedule J\", \"XML\"]','Virginia Tech',14,'Form 990',NULL,NULL,NULL,NULL,NULL),(77,'KidDataViz','Data helps us to understand the world around us. Not only is interpreting data important, but understanding how to communicate data is an essential skill in the modern world. Teaching children how to make and understand data visualizations lays a solid foundation for their critical thinking and understanding of large-scale problems. This project aims to teach elementary school students how to visualize data engagingly and effectively.\nBuilding on this goal, our project was to develop an accessible website with the ability to host seven or more different game implementations designed around data visualization concepts. These games target three groups of school levels: 1st- and 2nd-grade, 3rd- and 4th-grade, and 5th-grade and up. The goal of the games is to break down complex data visualization concepts for various levels of understanding. Consequently, each game is designed to be fun and replayable, so children engage with the website for longer periods and learn more.\nThe website has been fully implemented and is accessible to the public. This implementation allows new developers to add games easily, assuming they are familiar with web development. Additionally, we have implemented seven games in either JavaScript or Unity, each of which is playable from the website. We have conducted testing for the application, via a digital feedback form provided to testers. The feedback given on these forms is used to improve the website and the games that are hosted on it.\nThe website features a mobile dropdown menu, an introductory page, a page for feedback, and seven games that teach core concepts of data visualization. The website can be used in classroom settings as an easy way for teachers to introduce data visualization to students.','[\"data visualization\", \"data analytics\", \"children\", \"student\", \"elementary school\", \"react\", \"unity\"]','Virginia Tech',14,'data visualization',NULL,NULL,NULL,NULL,NULL),(78,'AI Aided Annotation','Human annotation of long documents is a very important task in training and evaluation in NLP. The process generally starts with the human annotators reading over the document in its entirety. Once the annotator feels they have a sufficient grasp on the document, they can begin to annotate it. Specifically, annotators will look for questions that can be answered, and then write down the question and answer. In our client’s case, the chosen long documents are electronic theses and dissertations (ETDs) which are often 100-150 pages minimum, thereby making it a time consuming and expensive process to annotate. The ETDs are annotated on a chapter by chapter basis as content can vary significantly in each chapter. The annotations generated are then used to help evaluate downstream tasks such as summarization, topic modeling, and question answering.\nThe system aids the annotators in the creation of a Knowledge Base that is rich with topics/keywords and question-answer pairs for each chapter in ETDs. The core of the system revolves around an algorithm known as the Maximal Marginal Relevance. By utilizing the MMR algorithm with a changeable lambda value,  keywords, and a couple of other elements, we can identify sentences based on their similarity or diversity relative to a collection of sentences.  This algorithm would greatly enhance the annotation process in ETDs by automating the process of identifying the most relevant sentences. Thus, annotators do not have to sift through the ETDs one sentence at a time, instead making a comprehensive summary as fast as the MMR algorithm can work. As a result, annotators can save many hours per ETD, resulting in more human generated annotations in a shorter amount of time.\nThe final deliverables are the project, a final slideshow presenting our work throughout the semester, a final report, and a video demonstrating exactly how to use our platform. All of this is available here on VTechWorks in this report. Additionally, the project is being built using GitHub, making it free and available to the public to fork and modify in any way they see fit.','[\"Maximal Marginal Relevance\", \"Annotation\", \"Website\", \"AI Aided Annotation\", \"Chapter Annotation\"]','Virginia Tech',15,'Maximal Marginal Relevance',NULL,NULL,NULL,NULL,NULL),(79,'WebsiteSocialNetworksGreatBritain','WebsiteSocialNetworksBritain is a project designed to create a HTML website from an input of forty thousand XML records. These XML records describe historical British and American figures and their connections to each other through family, associates, and organizations. The client has built these records and wishes them to be viewable on a website. Per specifications, the website must be built using no third-party services. The final Java program consists of two parts: reading the XML records into a data structure and providing the ability to search the according to appropriate criteria. The program uses a binary search tree to store the forty thousand records and creates a pop-up menu to select the records the user wishes to view. The pop-up menu allows the user to search through the records and find previously unseen connections between the historical figures.','[\"Georgian\", \"Great Britain\", \"Social Network\", \"XML\", \"Java\", \"Historical Figures\"]','Virginia Tech',15,'Georgian',NULL,NULL,NULL,NULL,NULL),(80,'Anti-Poaching Drone Control','Our project assists the SeaQL Lab of Virginia Tech\'s Department of Fisheries and Wildlife Conservation. Working with the Marine Management Organisation of the UK, the Lab\'s project entails developing an autonomous drone swarm that can fly predetermined routes around the Chagos Archipelago and send alerts about potential poaching boats, based on machine learning image analysis in the drones\' attached computing modules. The main goal of this project is to save the sharks and the ecosystem of those waters while decreasing the time, money, and effort for the local Coast Guard to perform regular monitoring. Instead, the drones will send detection alerts to a remote server being monitored by a ranger if it spots a potential poaching boat. Our report details our contributions to the overall project.\nOur team took responsibility for several smaller tasks integral to the overall project. First, we familiarized ourselves with the Robotic Operating System (ROS) to connect, calibrate, test, and record video using the cameras provided. ROS will control much of the drones\' added functionality such as running the poaching boat detection algorithm, sending flight commands to the drones, and streaming video over a cellular connection. Next, we aided the larger project team in repairing one off-the-shelf drone for potential flight testing. After unsuccessful troubleshooting, we moved to help finish construction of the primary hexacopter. Finally, we wrote a script to start the 4G cellular connection automatically when a drone is powered on.\nThe AntiPoachingDroneControlReport details this work amidst the larger project goals of the SeaQL Lab. The  AntiPoachingDroneControlPresentation gives a brief summary of our project work and the lessons learned. This was presented to our CS4624: Multimedia, Hypertext, and Information Access class to summarize our project work and experiences.','[\"poaching\", \"drone\", \"autonomous\", \"shark\", \"Chagos\", \"conservation\", \"image analysis\", \"AI\", \"Jetson Nano\", \"ROS\", \"Robotic Operating System\", \"hexacopter\"]','Virginia Tech',15,'poaching',NULL,NULL,NULL,NULL,NULL),(81,'Safe Eats App','Food allergies and allergic reactions are a problem that many people face (roughly 50 million Americans experience yearly allergies) in their daily lives. Looking out for products that could aggravate these allergies can be very difficult for college students who are living on their own for the first time. Knowing what foods they can and cannot eat is essential. For example, anaphylactic shock can take up to a week to recover from, in severe cases. With the busy schedules and newfound independence associated with being a college student, it is important that access to information concerning allergens is easily available.\nTo solve this problem, we implemented an iOS application where students can easily view allergen information for on-campus dining halls at Virginia Tech. The main goal was to make the system faster and easier to use than the current Virginia Tech Dining Services website, in order to provide students with an easy way to check allergen information on the go. As it stands, Virginia Tech’s current implementation of distributing nutritional information about the food they serve is quite outdated, relying on a Windows IIS/8.5 server, with search latencies reaching upwards of 8 seconds and over 6 seconds for lookup latency. So, we worked to streamline the presentation of Virginia Tech Dining Halls nutritional and allergen data for students and faculty with allergen risks at Virginia Tech.\nThe final deliverables of this project include a partially functioning iOS application, a final report, and a set of presentation slides submitted to our sponsor, Dr. Edward Fox, our clients, Deborah Nichols and Candice Matthis, as well as Ashley Foster from Virginia Tech Dining Services. The report and presentation have progressed through our stages of development, including the project initiation -- in which we received criteria from the client and broke down deliverables, the planning stage -- in which a list of tasks was compiled, the execution stage -- which made up most of the development process, the testing phase -- in which we deployed the application to a small sample group, and the closing process -- where the product was presented to the clients and sponsor.','[\"Allergens\", \"Dining Halls\", \"Alpha-Gal\", \"Food Allergies\", \"iOS Application\"]','Virginia Tech',15,'Allergens',NULL,NULL,NULL,NULL,NULL),(82,'Parquet Containers','Archives preserve content and support various types of analysis, study, and use.\nArchiving the Web and social media (e.g., Twitter) content involves use of representations like Web ARChive (WARC) and JavaScript Object Notation (JSON).\nParquet is a newer representation that yields better performance with large collections for many data processing tasks.\nIn support of archive-related research at Virginia Tech in University Libraries and the Digital Library Research Laboratory (DLRL), this project involved the development of software (tested in Linux and Windows, as well as on local and container cluster environments), and its packaging in containers, for converting WARC to Parquet as well as JSON to Parquet.\nImages of the resulting containers are available to support the research of others.','[\"Docker\", \"Containers\", \"Apache Spark\", \"Parquet\", \"WARC\", \"JSON\", \"Twitter\", \"PySpark\", \"Scala\", \"Kubernetes\", \"Cloud\"]','Virginia Tech',15,'Docker',NULL,NULL,NULL,NULL,NULL),(83,'COVID-19FakeNews','COVID is a virus that rampages through every country, from rural to urban areas. Since the beginning of the virus, facts and science have been politicized to align with party agendas which have unfortunately resulted in constituents being misinformed about the dangerous virus. From early April 2020 to early May 2020, Dr. Mohamed Farag collected a large set of tweets from users on Twitter. In these tweets, Twitter users expressed their thoughts, opinions, and facts on the virus. We aimed to filter these tweets, sort them into classes, and utilize machine learning to determine if these tweets, and future tweets that are to come, are a reliable source of accurate information or not.\nOur goal in this project was to find rumors and false information that is spread about COVID as well as the perpetrators that spread this information. As more people around the world gain access to the internet, more people will continue spreading information and this results in an information “overload” where facts and myth are intertwined, and the public is unaware of the real truth. The COVID19FakeNews team focused on contributing to providing clarity to the public about which tweets spread dangerous lies.\nWe received a one terabyte file, filled with tweets, that Dr. Farag had collected. We converted these tweets into a unified format and stored them into a readable JSON format. We did this by making a Python script that utilizes different libraries associated with Python. We extracted the tweet IDs from the stored tweets collected, and, using the Twarc2 library, we were able to hydrate still existing – i.e., not deleted – tweets using the tweet ID that we extracted from the collection. This was crucial for finding currently visible tweets, so we can sort into future categories (buckets).','[\"Coronavirus\", \"Machine Learning\", \"SVM\", \"Twitter\", \"Angular\"]','Virginia Tech',15,'Coronavirus',NULL,NULL,NULL,NULL,NULL),(84,'Figure Extraction Website','This project aimed to extract figures from theses and dissertations, index them, and support searching of those figures. Figure Extraction Website intends to fix the problem of users having to go through each PDF file and find figures that match their interests. Instead, Figure Extraction Website allows curators or users to upload PDF files from their computer, and then support searches by the appropriate keyword inputs. Figures can be searched by the caption text or the words within the figures. Two open source tools, PDFFigures2 and PDFPlumber, are used to extract figures from the PDF files. Then, ElasticSearch is used to index the figures, captions, and document metadata.\nThe website is built based on the ETDUI website, which was given to our group by our clients. ETDUI allows entry of keywords and output of PDF files that have the keyword in the title or in the summary portion of PDF files. To focus on our aims, we removed some features of ETDUI, including login, register, advanced search, and voice search. Then, our group added some features, including a file select button and an upload button, so that users can easily upload PDF files.\nCurrently, the website is running on localhost, which can be cloned from the GitHub repository (https://github.com/JRReynosa/CS4624_Figure_Extraction_Website). The PDF files that are uploaded are stored locally, with path information given in the website.\nTesting proceeded with uploading a few PDF files. Searching was tested with keyword queries. There still exist problems, such as to find words or mathematical equations within the figures, as opposed to those within the captions.','[\"ETD\", \"electronic theses and dissertations\", \"thesis\", \"dissertation\", \"figure\", \"extraction\", \"Elasticsearch\", \"PDFFigures2\", \"PDFPlumber\", \"PDF\", \"Metadata\", \"Figure Extraction\", \"Python\", \"ndjson\", \"json\", \"Haystack\", \"sbt\", \"scala\", \"linux\", \"kibana\"]','Virginia Tech',15,'ETD',NULL,NULL,NULL,NULL,NULL),(85,'Risk Prediction Sentiment Analysis 3','Workplace safety is a growing issue in today\'s society and there have been many concerns in regards to the safety of workplace environments. This project looks to use worker narratives from a workplace to analyze general sentiment to predict risk. This can be done from a web application which allows users to upload worker narratives with CSV files and run the narratives on an NLP model to classify them with sentiment scores.\nThe main problem with building such a system is creating an effective sentiment analysis model geared towards worker narratives.   General sentiment analysis models are built to analyze the sentiment of everyday conversations rather than the specific use case of worker narratives. The prior group used the NLP model from AWS Comprehend. We surpassed the accuracy of that model leveraging the services of Google Cloud Platform, which introduced some migration complications from the AWS to Google Cloud infrastructure. The model was trained using 1544 manually labeled worker narratives to allow it to gauge sentiment specific to work environments. We also improved the user experience and security of the application, introducing instructions and login pages.','[\"Sentiment Analysis\", \"Machine Learning\", \"Full Stack Application\", \"AWS\"]','Virginia Tech',15,'Sentiment Analysis',NULL,NULL,NULL,NULL,NULL),(86,'VideoUDL','The profile of learners is ever-changing. Each student approaches, understands, and solves concepts differently. By anticipating and planning for learner variability, faculty, teachers, and students embrace their greatest strength: diversity. The Universal Design for Learning (UDL) is a framework that improves and enhances teaching and learning. The vision is to proactively design educational environments that are inclusive, accessible, and flexible for all learners. In order to optimize the design and delivery of course instruction, UDL focuses on providing multiple, flexible methods of representation, expression, and engagement.\nWhile the UDL framework emphasizes accessibility for a diverse learner population, this flexibility with respect to how course material is displayed or delivered can also be found in Accessible Educational Materials (AEM). AEM highlights print or technology-based educational content that is designed to be usable and understandable across learner variability, regardless of format.\nThe purpose of this video project is to bring awareness to the benefits of implementing the UDL framework and ensuring materials are accessible at Virginia Tech. Our team filmed and created an eight-minute video that includes graphics and various testimonials from Carolyn Shivers, Alicia Johnson, Rachel Mirsen, and Nevada Kershner. The video will also include closed captioning to increase accessibility. To reflect upon how course instruction and delivery address learner variability and preferences, the video introduces the concept of UDL and AEM, presents testimonials, highlights statistics, and presents a call to action for faculty to enroll in an online professional development course on UDL in higher education. Faculty can find this UDL course on the Technology-Enhanced Learning and Online Strategies (TLOS) Professional Development website to learn more about accessibility best practices.\nThroughout the semester, the team researched the application of the UDL framework and AEM in higher education, designed a storyboard for initial video concepts, created informational graphics, and developed a script that included narration, transitions, and interview questions. Furthermore, the team contacted multiple assistant professors and teaching assistants in order to hear their perspectives. The team filmed and edited interview footage, adding in proper transitions and detailed illustrations to deliver a promotional and information video about accessibility best practices at Virginia Tech. This video targets faculty members, encouraging them to incorporate UDL strategies into their course design and delivery. The Accessible Technologies Team at Virginia Tech can utilize this video to convey to faculty the value of Universal Design for Learning to support learner variability.','[\"Universal Design for Learning\", \"UDL\", \"Accessible Education Materials\", \"AEM\", \"Learning Disability\", \"Learning Disabilities\"]','Virginia Tech',15,'Universal Design for Learning',NULL,NULL,NULL,NULL,NULL),(87,'Twitter Collections','TwitterCollections is a continuation of work from a previous semester team called Library6Btweets. The prior team, which worked during Fall 2021, was composed of Yash Bhargava, Daniel Burdisso, Pranav Dhakal, Anna Herms, and Kenneth Powell. The current team that took this over, and worked on this during Spring 2022, is composed of Matt Gonley, Ryan Nicholas, Nicole Fitz, Griffin Knock, and Derek Bruce.\nBillions of tweets have been collected by the Digital Library Research Laboratory (DLRL). The tweets were collected in three formats: DMI-TCAT, YTK, and SFM. The tweets collected should be converted into a standard data format to allow for ease of access and data research.\nThe goal is to convert the collected tweets into a unified JSON format. A secondary goal is to create a machine learning model to categorize uncategorized tweets. The standardized format is in two styles: an individual level, and a collection level. Conversion varies for these levels, requiring, respectively, conversion of each tweet and its attributes to a JSON object, and conversion of a whole collection of tweets to a separate JSON object.\nOur work involved familiarizing ourselves with the previous semester’s work and its schema. The three formats for the tweets were as follows: Social Feed Manager (SFM), yourTwapperKeeper (YTK), and Digital Methods Initiative Twitter Capture and Analysis Toolset (DMI-TCAT). The previous team designed this schema with these tweet types in mind as well as the Twitter version 2 schema. The previous team also created a collection level schema that listed all of the tweet IDs in a given collection, to allow for determining which tweets belong in which collection. They designed this in accordance with the events archive website.\nWe were given the previous team\'s conversion scripts for each of the tweet formats as well. Each format needed a different script, as what attributes and what metadata from the tweets was collected differed. The format they were collected in also differed. DMI had the data split into six tables in SQL for any given topic, YTK had the data in separate tables for a topic, and SFM was in the format of JSON.\nThe original scripts were written in Python. For simplicity, we continued using Python as well. Our focus was on optimizing the scripts, as some of them were unusably slow. The scripts also needed to be modified to accommodate scale, where all the data could not be loaded into memory. We were provided six scripts, two for each tweet format: one script for the individual schema and one for the collection level schema.\nIn addition to the optimizations and modifications, a machine learning model was created to accurately classify the events for unlabeled tweet collections. The model can classify the tweets when fed the data from any of the formats. We experimented with a Naive Bayes model and BERT-based Neural Network model, and found the latter superior.\nThe new scripts, optimized versions of prior scripts, best machine learning model, and converted Twitter collection JSON files are our deliverables for this semester. We hope that a standardized set of data can allow for fast and effective research for those who want to incorporate tweets into their study.','[\"Twitter\", \"SFM\", \"DMI-TCAT\", \"YTK\", \"JSON\", \"Python\"]','Virginia Tech',15,'Twitter',NULL,NULL,NULL,NULL,NULL),(88,'WebsiteCyberRangeUS2','Online cyber threats have been an increasing concern since the dawn of the Internet. To combat this problem, USCyberRange, a cybersecurity education team, provides online courses and exercises to teach students about cybersecurity issues and solutions. Our team partnered with FourDesign, a graphic design team, to make the USCyberRange website have responsive design on devices with different screen sizes. Our team was provided with the initial website built by last semester’s group, as well as new Figma designs crafted by FourDesign. Throughout the course of the semester, our team used the WordPress interface and the Elementor plugin to implement many new Figma designs into the website and make them have responsive design for desktop, mobile, and tablet devices. Our team then received client feedback on our implementation, and made adjustments accordingly. The finalized product is a WordPress website that could be further developed to make all front-end pages responsive and to include their corresponding back-end functionality.\nOur team has delivered the WordPress website, this report, and the final slide presentation.','[\"WordPress\", \"Website\", \"Computer Science\", \"Capstone\", \"PHP\", \"Software Engineering\"]','N/A',15,'WordPress',NULL,NULL,NULL,NULL,NULL),(89,'TextMining','Electronic theses and dissertations (ETDs) contain valuable knowledge that can be useful in a wide range of research areas. Accordingly, we are building electronic infrastructure leveraging advanced work on digital libraries, for discovering and accessing the knowledge buried in ETDs. We focus on our work to incorporate topic modeling into digital libraries for ETDs. We present ETD-Topics, a framework that extracts topics from a large text corpus in an unsupervised way. The representations learnt from topic models can be useful for downstream tasks such as searching and/or browsing documents by topic, document recommendation, topic recommendation, and describing temporal topic trends (e.g., from the perspective of disciplines or universities).\nThe characteristics of different models make the classification distinguished. We provide four modes (LDA, NeuralLDA, ProdLDA, and CTM) to serve user groups with different browsing and searching requirements. Our job was to import the preprocessed database and the trained models (four models with different topic numbers), and to accurately display key information (such as topics, document title, abstract, etc.) on web pages. We chose Python as the main language to implement the back-end, while using Flask as a bridge connecting the back-end and front-end. On the basis of using HTML for displaying data, we were able to use JavaScript and CSS to make the whole set of web pages look more fluent and comfortable by optimizing the UI, to include graphic bars, buttons (like “Submit”, “Show more”, etc.), and tables.','[\"TextMining\", \"ETDs\", \"documents\", \"LDA\", \"NeuralLDA\", \"ProdLDA\", \"CTM\", \"Topic modeling\"]','Virginia Tech',15,'TextMining',NULL,NULL,NULL,NULL,NULL),(90,'PhonEtech: Recording Audio and Displaying Accompanying Text','The rise of the Covid-19 disease has brought challenges and opportunities that we have not been faced with before. With the new use of masks in our daily lives, reading lips to communicate with others when the surrounding environment may have increased noise levels, is a simple task that we no longer are able to fully utilize. The elimination of masks, however, does not eliminate the overall issue that trying to communicate in a noisy environment creates. To solve this issue, we have created and implemented an Android based phone application, PhonEtech, intended to facilitate and streamline communication between a variety of users, more specifically, when verbal communication is not ideal. Over the duration of a semester-long time frame, our team has designed, developed and tested this application idea, leading to a usable and functionable application. The finalized result is a phone app that can be developed further to create a more customizable experience with newer features. PhonEtech is not a replacement for verbal communication but instead an aid to a variety of users faced with various communication issues and roadblocks.','[\"Mobile Application\", \"Android\", \"Speech Assistance\"]','Virginia Tech',15,'Mobile Application',NULL,NULL,NULL,NULL,NULL),(91,'Object Detection','Electronic theses and dissertations (ETDs) contain valuable knowledge that can be useful in a wide range of research areas. To effectively utilize the knowledge contained in ETDs, the data first needs to be parsed and stored in an XML document. However, since most of the ETDs available on the web are presented in PDF, parsing them is a challenge to make their data useful for any downstream task, including question-answering, figure search, table search, and summarizing. For information search and extraction, contextual information is needed to perform these tasks. However, such semantic information is hidden in PDF documents. In contrast, XML can explicitly share semantic information. The structure within XML documents can enforce semantic continuity within the tag elements. Accordingly, knowledge graphs can be more easily built from XML, rather than PDF, representations. The goal of this project was to extract different elements of scholarly documents such as metadata (title, authors, year), chapter headings and subheadings, equations, figures (and captions), tables (and captions), and paragraphs, and then package them into an XML document. Subsequently, a pipeline responsible for the conversion and a dataset to support the object detection step was developed. Over the semester, 200 ETDs, both born-digital and scanned, were annotated using a online tool called RoboFlow. A model based on Facebook’s open-sourced object detection model, Detectron2, was trained with the created dataset. Besides that, a pipeline that utilizes the model has been built that converts an ETD in PDF into an XML document, which can then be used for future downstream tasks and HTML for visualization. A dataset consisting of 200 annotated ETDs and a working pipeline were delivered to the client. From the project, the Object Detection Team learned numbers of libraries related to the task, built a sense of the importance of version control, and understood how to split a large task into smaller and more approachable pieces.','[\"Object Bounding Box Detection\", \"OCR\", \"Computer Vision\", \"R-CNN Model\", \"Content Classification\", \"RoboFlow\", \"XML\", \"HTML\", \"Python\"]','Virginia Tech',15,'Object Bounding Box Detection',NULL,NULL,NULL,NULL,NULL),(92,'SharkValidatorGame','The team was provided with a platform to update and implement new design changes to the original website for a project led by Dr. Francesco Ferretti, an Assistant Professor in the Department of Fish and Wildlife Conservation. His research interests include studying the impact of humanity on the Earth’s oceans and conservation efforts. The website is built using WordPress with frontend CSS and HTML. RShiny provides the backend support to implement the Validation Monitor. We were tasked with converting the static framework of the website to a more dynamic and responsive framework and with improving on the current gamification scheme through a refined point rewards system and incentives.\nThe team was able to refine the current point reward system by integrating functionalities that will motivate users to validate sharks and support SharkPulse’s research for the ecology and taxonomy of shark populations. These functionalities include awarding users who are able to recognize rare shark species, and if the shark species are labeled as “endangered” or “vulnerable” or “critically endangered” according to the IUCN red lists of ecosystems and threatened species. Most of the completed changes were on the backend for the validation monitor and on the frontend for the identification guide. The team has also improved the identification guide to be a more suitable web page to educate users about sharks. It was updated by adding more questions and adding an option to let the users select “I can’t see” if they are unable to see shark characteristics from an image. Overall, the backend changes for assigning points based on if the user recognizes rare or threatened species is deployed. However, the project is still not complete, as the website still needs to be updated from being a static website to being a dynamic one. The rare species functionality could also be updated to improve the program’s performance.','[\"sharks\", \"rare sharks\", \"identification guide\", \"SharkPulse\", \"validation\", \"conservations status\"]','Virginia Tech',15,'sharks',NULL,NULL,NULL,NULL,NULL),(93,'TaxData','The Internal Revenue Service (IRS) provides a plethora of data related to tax-exempt organizations through the publication of IRS Form 990 tax filings in Extensible Markup Language (XML) format, hosted between their website and Amazon Web Services (AWS). These data sources possess filing data beginning in tax year 2012, and ending in the most recently filed and uploaded tax year of 2020. This defines the project’s study window as 2012-2020. The primary goal of this project is to create a database of Form 990 filings to support research related to tourism offices and various other tax-exempt organizations. The primary challenge of this project is to process filings from all years within the study window and upload them to the database in a unified manner. The development of this database utilizes tools such as Jupyter Notebooks, SQLite, and various Python libraries for scraping, preprocessing, and analysis. Due to the number of different return types and the massive amount of data contained in the forms, understanding the forms in their standard format is incredibly challenging. Additionally, most documentation about 990 forms is oriented to accountants or tax experts who are well versed in financial jargon. This issue extends to the XML data files themselves, as many of the XML tags are heavily abbreviated, and cross referencing each of them with its corresponding location on Form 990 is a tedious and near impossible task. The solution to these problems lies in archiving the data but also having it accessible for use.\nThis project can be divided into four phases: scraping, preprocessing, uploading, and analysis. The project begins with scraping 990 filings from the two sources highlighted above. The next phase, preprocessing, involves creating a common schema and converting the XML files into Comma Separated Values (CSV) and JavaScript Object Notation (JSON) formats. This is the most difficult and lengthy phase of the project as it involves understanding the 990 filings to the greatest possible extent through both automated and manual processes. Next is the uploading phase, where the database is built and populated with the preprocessed data. Finally, queries can be made to the database for the analysis phase to extract interesting financial trends. This final phase allows the team to maximize its familiarity with the database and supports the development of extensive documentation and the users’ guide that are included in the Users’ Manual section. The result of this project comes in two forms: the aforementioned database, and a set of CSV data pertaining to the 990 filings of all tourism offices present in the XML data. The database is structured in order to maximize the breadth and depth of analysis that is made available to the project’s client and other stakeholders. These other stakeholders include fellow researchers of tourism offices, and any other business researchers who may be concerned with the financial data of non-profits and tax-exempt organizations. The database contains tables that allow users to access specific data across an organization, or multiple organizations’ Form 990 filings. These tables are complemented by overview data tables, allowing for users to locate specific organizations based on the type of business they carry out (such as tourism offices), rather than limiting users to querying based on Form 990 filing data. Finally, per the client\'s request, all tourism office data is separately outputted into a set of CSV files.','[\"990\", \"990-EZ\", \"Tourism\", \"Database\"]','Virginia Tech',15,'990',NULL,NULL,NULL,NULL,NULL),(94,'ATinstagram','For this project, we wanted to discover if and how hikers use the social media platform, Instagram, to talk about Leave No Trace (LNT) principles on the Appalachian Trail. Leave No Trace principles refer to a set of guidelines that hikers should follow in order to promote conservation on trails.\nThe workflow to complete the project included: collecting relevant Instagram posts, performing sentiment analysis on these posts, and finally creating a series of graphs that show the different connections between posts. We started by utilizing Python, JSON objects, and Selenium to gather all of the Instagram posts with specific hashtags, such as “#AppalachianTrail” , “LeaveNoTrace”, and “LNT”. Selenium is used for the API calls, which retrieve the many Instagram posts. Information about each post, such as its geographic location, caption, and hashtag are extracted using JSON objects.\nThe final two parts of the project include performing sentiment analysis on the collected posts and then visualizing the data in a variety of ways. For the sentiment analysis, we analyzed each caption of every post, and assigned it a score ranging from negative one to positive one. Negative one would represent a highly negative sentiment and positive one represents a highly positive sentiment. From there, we utilized the K-Means Clustering algorithm to gather posts with similar hashtags. For the visualizations, we displayed what tags occur in the same post, connections between different hashtags, and the geolocations of the different posts. The deliverables of our project include the source code that is used to scrape the Instagram posts, perform sentiment analysis, and visualize the data, along with several folders showing the results of our data collection. These results include the scraped Instagram posts, the sentiment analysis results, and the visualizations we created. These deliverables could help our client and those interested with research relating to Instagram, Leave No Trace principles, and the Appalachian Trail.','[\"Instagram\", \"Scraping\", \"Sentiment Analysis\", \"Visualizations\", \"Hiking\", \"LNT\"]','Virginia Tech',15,'Instagram',NULL,NULL,NULL,NULL,NULL),(95,'FreeSpeechApp4VT','Over the course of his career, Mr. Matthew Newton, the coordinator of Assistive and Education Technology at Virginia Tech, has been working with applications and assistive technology in order to aid those who require non-verbal communication, to receive the means to do so. The FreeSpeech4VT project was launched under the direction of Mr. Newton to provide a free-of-cost tablet application that would allow users to easily communicate using type-to-speech functionality. Our goal as a team was to create an application that could provide basic and advanced implementations of features that paid-applications offer, while also promoting user customization of the application itself.\nThe project consists of one mobile application that pulls from the device’s local data and is able to store data into the device’s memory based on user input of words/tiles. Although cross-platform applications could provide more flexibility and accessibility for users depending on what operating system their devices run on, our team found it best to implement a thorough iPadOS application due to the high frequency of iPad usage for communicative purposes.\nThis application is designed for anyone who may struggle to communicate verbally, both temporarily or more long-term. There were some design choices to be more friendly towards those who may have difficulty with motor function as well. The range of people that can use this application is immense; it can be anywhere from someone who is unable to speak at all and may have some motor function issues to someone who has laryngitis and is able to type out sentences just not speak or speak loudly for some time.\nWe began by meeting with Mr. Newton to discuss his vision. Because ergonomics with the target user-group in mind was so important, we spent a lot of time on iterative design and wireframing, getting feedback and making improvements, and finally getting approval. We then began implementing the design as well as basic Text-To-Speech (TTS) functionality. After that, we implemented more customizability-centric functionality to allow both ease-of-setup for caretakers and ease-of-use for users. We obtained feedback via bi-weekly client meetings.\nWe have delivered an iPadOS application, this report, a final slide presentation, and a video walkthrough of the application in use.','[\"Assistive technology\", \"iPadOS\", \"Text-to-Speech\", \"Customizability\", \"Ergonomics\"]','Virginia Tech',15,'Assistive technology',NULL,NULL,NULL,NULL,NULL),(96,'CS Advising Bot','The CSadvisingBot project has a clear goal: to create a virtual assistant to answer frequently asked questions of computer science students at Virginia Tech. Advisors get asked the same questions over and over from students each semester. Most of these questions are not very complex or personalized and can be answered in a few sentences or links to more resources. These questions become a burden on advisors who have to spend their time responding to numerous emails with just a simple response. This project aims to relieve this burden from advisors by providing a service to quickly answer frequently asked questions in advising, computer science, and other frequently asked undergraduate questions. Our Client, Sally Hamouda, is an Assistant Professor in the Department of Computer Science at Virginia Tech. She proposed we create an online chatbot to help current computer science students, that can answer frequently asked questions related to advising, force add requests, funding, and more. Instead of building the chatbot from scratch, we were told to integrate a chatbot framework like LUIS or IBM Watson as a foundation of the chatbot. We decided to go with IBM Watson which uses artificial intelligence to connect user intents with dialog responses to answer a user\'s question as it is simple to integrate into any website, and as we hope the chatbot will be integrated into the Virginia Tech Computer Science advising website. We designed the chatbot to be primarily for current students, but it will also answer questions for prospective students, transfer students, graduate students, and parents. This target audience led to the creation of over 50 topics in which our chatbot can respond accurately with useful information.','[\"Chatbot\", \"Advising\", \"Virtural Assistant\", \"IBM Watson\", \"Virginia Tech Advising\", \"Artificial Intelligence\", \"IBM\", \"AI Chatbot\", \"Virginia Tech Computer Science\", \"Virginia Tech\"]','Virginia Tech',15,'Chatbot',NULL,NULL,NULL,NULL,NULL),(97,'CS3604 Case Study Library','For this project, our job was to create a website to house the previous years’ case study presentations. To do this we needed to support all of the file types that could be submitted such as PowerPoint, a PDF document, an MP4 video, and a Google Slides link. These files also needed to be categorized to allow for searching based on a variety of categories. These categories include the date, topic, keywords, file format, etc. These files would form a repository of case studies for future CS3604 students to look at as they create their case study presentations. Students can build off these cases or create their own. Students then would submit their case studies directly to the website. To accomplish this we used AWS to deploy the website directly onto the Virginia Tech Digital Library Platform. The custom domain for the website can be found atcasestudies.cs.vt.edu. The CS department has funded an AWS account for the website and 638 presentation files have been uploaded to the website. As a team, we tested the website to ensure that a specific presentation could be searched for and that the file was able to be accessed by the user.','[\"AWS\", \"VTDLP\", \"Library\", \"CS3604\", \"Website\", \"Computer Science\", \"Search\", \"Capstone\", \"Report\", \"Presentation\"]','Virginia Tech',15,'AWS',NULL,NULL,NULL,NULL,NULL),(98,'CellCycleViz','The CellCycleViz Project teamed up with our client, Dr. Cao, to create an educational website aimed at teaching users about the cell cycle. The website includes content for a wide range of users, including young students, the general public and experienced users interested in research data. Content on the website includes introductory information for users first learning about the cell cycle, detailed cell cycle models for those interested in more detailed information and mathematical models created with research data from Professor John Tyson\'s lab. Professor John Tyson is a Distinguished Professor in the Biology department. His lab focuses on studying Caulobacter cells, a type of bacteria widely distributed in freshwater lakes and streams. The website was developed using one HTML file for each webpage and JavaScript files to create interactive cell cycle visualizations.','[\"Cell Cycle\", \"Visualization\", \"Caulobacter Cell\", \"Models\", \"Biology\"]','Virginia Tech',15,'Cell Cycle',NULL,NULL,NULL,NULL,NULL),(99,'Library Tweets Conversion','The Digital Library Research Laboratory (DLRL) has collected billions of tweets over the course of years. These tweets were gathered using three different data collection tools, and have been organized into collections based on keywords. The different collection tools used were: Social Feed Manager (SFM), yourTwapperKeeper (YTK), and Digital Methods Initiative Twitter Capture and Analysis Toolset (DMI-TCAT). Because each of these tools store the tweets differently, the DLRL aims to consolidate these tweets so the Library can provide a service that allows the campus to easily access and use this data.\nOur job was to come up with a unified JSON format that all of these tweets could be represented by and to provide a way to convert them to this new format. Additionally, we had to provide suitable collection-level information for each distinct data collection that showed the connections between tweets and the collections they belonged to. To accomplish this, we have six conversion scripts. Three of these are for converting the individual tweets, and three of them are for compiling the collection-level metadata and preserving the relationship between tweets and collections. When run with the Twitter data, they provide a unified way to digest all of the collected data regardless of which method it was obtained by.','[\"YTK\", \"yourtwapperkeeper\", \"Twitter\", \"Data conversion\", \"Collection-level\", \"Python\", \"tweet\", \"Digital Methods Initiative Twitter Capture and Analysis Toolset\", \"DMI-TCAT\", \"Social Feed Manager\", \"SFM\", \"MySQL\", \"JSON\", \"Library\", \"Library tweets data\"]','Virginia Tech',12,'YTK',NULL,NULL,NULL,NULL,NULL),(100,'Wildlife Tracker App','In the previous semester, Dr. Luis E. Escobar from the Department of Fish and Wildlife Conservation initiated a project called Wildlife Diseases Tracker App to help with his research studies. The original project aimed to provide a free-of-cost mobile application with a website to accomplish early detection of wildlife diseases. During this current semester, our goal was to update and finalize a new version based on the previous project and work on the continuation of the smartphone application and the website to track wildlife diseases in real-time.\nThe project has two divisions: a mobile application and a website. The goal for the mobile application is to update and expand it with the capacity to take photos and get the geolocation and date of the photo.  It is able to record metadata and upload data to the server. The website is updated with the functionality to summarize the data and images collected for analysis, mapping. We wished for the implementation of a cross-platform mobile application; however, due to technical difficulties and time constraints, we could only update and finalize the existing IOS version of the smartphone application.\nFor our IOS application, we conducted tests and fixed existing bugs based on the project done by the previous group. We also improved the existing Upload functionalities by adding more input fields. Some new functions were also implemented including Map functionality and login credentials. A Face ID/Touch ID for users was also created for easy login. As for the website, an updated version of the home page was constructed with basic information about our project, and instructions for downloading and using the application. We also updated the map and the spreadsheet, so it displays and provide downloadable data to users. To test, we released a beta version and sent out questionnaires and surveys for students and collaborators to examine functionalities, aesthetics, and accessibility.\nThe final deliverables for this semester project include the updated versions of the website and the iOS mobile application, a final report that explains all aspects and details of our project, and a final presentation slide deck which we used to display what we have accomplished throughout the semester in the class, our client, Dr. Escobar, and Professor Fox. We would like to see our effort made in this project helpful to both students participating in related studies and to our client, Dr. Escobar.','[\"Mange\", \"Wildlife\", \"Disease\", \"Epidemic\", \"Infectious\", \"IOS\", \"HTML\", \"JavaScript\", \"Firebase\", \"Map\", \"Tracker\", \"Early Detection\", \"Smartphone Application\", \"SwiftUI\"]','Virginia Tech',12,'Mange',NULL,NULL,NULL,NULL,NULL),(101,'Tourism Websites','The project is about analyzing and visualizing metadata of tourism websites of three states (Virginia, Colorado, and California) from 1998 to 2018.Each state in the United States has its own state website that is used as a resource to attract new tourists to this location. Each of these sites usually includes great attractions in this state, travel tips and facts about this place, blog posts, and reviews from other people who have been there. Suggestions regarding what might attract potential customers could emerge from examining past tourism websites and looking for any patterns amongst them that would determine what worked and what didn’t. These patterns can then be used to determine what was successful and use that information to make better-informed decisions on the future of state tourism. We will use the historical analysis of past government tourism websites to further support research on content and traffic trends on these websites. The various iterations of each state\'s tourism website are saved as snapshots in the Internet Archive. Our team was given the Parquet files having the snapshots of data containing the information recording tourism for California, Colorado, and Virginia dating back to 1998. We used a combination of Python’s Pandas library and Beautiful Soup to examine and extract relevant pieces of data from the given Parquet files. This data was scraped to extract the meta tags used for the website as of that date. With this data, we plotted the presence of all the variations on a state\'s tourism website in chronological order. This made it possible for us to analyze the addition and removal of keywords and to see other changes that were made like using phrases, capitalizations, keywords in languages other than English, and updating of keywords based on internet trends. This led us to conclude that meta tags play a very important role in a website\'s search engine ranking and a lot of analysis needs to be done keeping in mind the primary user base of the website.','[\"tourism\", \"tourism websites\", \"Virginia tourism\", \"Colorado tourism\", \"California tourism\", \"keyword analysis\", \"meta tags\", \"plotly.dash\"]','Virginia Tech',12,'tourism',NULL,NULL,NULL,NULL,NULL),(102,'Website Wildlife Diseases Redesign','Professor Escobar requested a redesign of the current Wildlife Disease Research Website to meet modern day web design standards. The report contains a description of the requirements for designing the new website including both aesthetic and functional requests. These requests were realized using a coding paradigm known as the MERN Stack. A thorough description of how this paradigm was used to redesign the website is included in the report. Also, the report contains descriptions of the testing process for the website and the deployment strategy. Included in the report is a User\'s Manual and Developer\'s Manual. The User\'s Manual describes the user flow through the website providing descriptions of the various front-end functionalities on each page. The Developer\'s Manual provides a technical description of how the front-end and back-end were coded. Included is a thorough description of how to edit the code to edit website functionality. The presentation contains a high level description of the contents of the report.\nThere are two versions of the report with them containing the same content but in different formats. This is the same with the presentation files.','[\"MERN\", \"Quality Analysis Testing\", \"Website\", \"Wildlife Diseases\"]','Virginia Tech',12,'MERN',NULL,NULL,NULL,NULL,NULL),(103,'Misinformation Stocks','The growing levels of fake news in our media contributes to misinformation campaigns, the impact of which can spread to investment decisions. To analyze the extent of misinformation on investors, we collected financial articles surrounding specific stocks. We compiled these into a dataset containing the titles, dates, sentiment analysis and misinformation rating. We leveraged a machine learning framework to automatically determine the sentiment of a given article. A value is manually assigned to each article in reference to its level of misinformation. This information is displayed in a digital library for users to access. From our small case study analysis, our preliminary findings show that the misleading content of an article ultimately has little impact on stock value. Instead, the sentiment of the public towards the news, regardless of its validity, is the driving force behind price fluctuation. We created a zip file that includes all of the code that we created for each part of the project, so that others can access and add to what we made. We demonstrated the different aspects of our project in a presentation, explaining briefly how each of the sections work, with pictures to aid understanding.','[\"stocks\", \"fake news\", \"misinformation\"]','Virginia Tech',12,'stocks',NULL,NULL,NULL,NULL,NULL),(104,'US CyberRange Website','The US CyberRange client enlisted FourDesign to redesign their current website. The CS team worked with the FourDesign team to implement this website. Along with the custom WordPress theme that was developed, a presentation and report were generated. The presentation gives a brief description of the deliverables and the process, whereas the report gives an in-depth description of the entire project. The report provides enough information of the US CyberRange to use our custom theme. Additionally the report provides information for developers planning on continuing this project.','[\"Custom Theme\", \"Website\", \"WordPress\", \"US CyberRange\", \"FourDesign\"]','Virginia Tech',12,'Custom Theme',NULL,NULL,NULL,NULL,NULL),(105,'SharkPulse Validator Game','SharkPulse is an initiative to involve citizen scientists in monitoring global shark populations. It is inspired by Stanford’s Shark Baseline Project, and it aims to collect image-based sightings of sharks from around the world to support research on ecology and conservation and increase public awareness of their conservation status. This project is currently headed by Dr. Francesco Ferretti. He is an Assistant Professor for the Department of Fish and Wildlife Conservation.\nThe team was provided with a platform to update and implement new design changes to the original website. The original website was built using WordPress for frontend CSS and HTML, with RShiny providing the backend to implement the Validation Monitor. We were tasked with converting the static framework of the website to a more dynamic and responsive framework,\nimproving on the current gamification scheme with a better rewards system and incentives, sourcing data from streamlined pipes, and developing a convenient user authentication system across multiple social media platforms to allow users to log in and store their points. This report contains information on the original project given to the team, and the changes the team implemented as per Dr. Ferretti\'s request. The report also highlights work that can be attempted by future teams that build on the current project as worked on and delivered by the team.','[\"shark\", \"sharkpulse\", \"validation\", \"conservation\", \"gamification\"]','Virginia Tech',12,'shark',NULL,NULL,NULL,NULL,NULL),(106,'Disease Spread Simulator II','The purpose of our project is to display a model of the spread of an infectious disease throughout the Commonwealth of Virginia. The user interface should encompass the ability to display what-if scenarios, adjust relevant parameters, and visualize resulting output. This report will further explain the background of the project, our implementation, and what our group has learned throughout this experience.\nWe created a product to suit the needs of visualizing the given data and formula. It is provided in the form of a webpage or dashboard in order to display a graphical model about Chronic Wasting Disease (CWD). The results and predictions drawn from this information are shown on the dashboard in order to track and see the spread of CWD. The user or client is able to manipulate different sliders to see the kinds of data required. Using a model, the data will be manipulated using statistics on the data to output the correct information that is needed.\nThe dashboard is hosted on an application called R-Shiny. The client, Professor Luis Escobar, requested we use R-Shiny as it was the most familiar for both parties. We created the dashboard using the model provided to us and data that was taken over the course of some period of time. The R-Shiny Dashboard is a way of creating and presenting the change in data through the sliders. With the statistical model we can see a lot of change.','[\"R-Shiny\", \"dashboard\", \"statistics\", \"Chronic Waste Disease\", \"sliders\", \"Virginia\", \"deer\", \"visualization\"]','Virginia Tech',12,'R-Shiny',NULL,NULL,NULL,NULL,NULL),(107,'Campus Jobs','The Campus Jobs project had one clear goal: to connect graduate students to work opportunities. Many graduate students enrolled in Computer Science struggle to find work and are forced to knock on the doors of professors in hopes of finding research opportunities. Upon successful completion of our project, we hoped to allow these students easy access to jobs in Blacksburg. Our client, Trey Mayo, is the Director of Graduate Programs at Virginia Tech. He proposed that we create a website similar to LinkedIn, but with an important facet being it specializes in opportunities for graduate students in Blacksburg. The structure and design of this website was left open for us to explore. We elected to host a website on a designated Virginia Tech Virtual Machine on thecs.vt.edudomain. Additionally, we hosted a database in MySQL on the same machine in order to store student and job information. We designed the website to allow for student and faculty login, as well as storing general data about the student and information about the student’s candidacy for the job. We implemented a matching algorithm to allow for students and professors to find the most appropriate matches.','[\"Jobs\", \"Graduate\", \"Graduate School\", \"Graduate Student\", \"Funding\", \"Employment\"]','Virginia Tech',12,'Jobs',NULL,NULL,NULL,NULL,NULL),(108,'Risk Prediction Sentiment Analysis','Risk Sentiment Analysis proposes a solution for companies to evaluate user submitted reports to determine workplace safety. The web application develops a pipeline for managers to submit employee CSVs to be processed through a custom sentiment analysis model in AWS. Alongside that, it has graphical presentation capabilities to easily interpret queried data. Finally, the web application supports a feedback loop of re-training the model to improve its accuracy with a page dedicated for human review.','[\"Machine learning\", \"AWS\", \"Hypertext\", \"Sentiment Analysis\", \"Natural Language Processing\", \"NLP\", \"JavaScript\", \"React.js\", \"AWS Comprehend\", \"AWS S3\", \"AWS EC2\", \"AWS DynamoDB\"]','Virginia Tech',12,'Machine learning',NULL,NULL,NULL,NULL,NULL),(109,'Projected Augmented-Reality','Augmented Reality is one of the core pillars of the upcoming Industry 4.0 (the next industrial revolution) and is expected to have an enormous impact in the future. This is supported by Meta’s (formerly Facebook) announced plans to unveil their own virtual reality world. Typically, extended reality experiences, like that of Meta and others we have seen, require users to make use of personal headsets. Our project, on the other hand, developed technology for a communal augmented reality experience without the need for personal devices. Our team developed a calibration system to bring this glasses-free augmented reality experience to life. The approach we took involved a projector, a powerful Windows Desktop, a Microsoft Azure Kinect Camera, and a Qualisys Motion Tracking system. Combining these hardware components, we were able to track objects entering the projector’s frustum and accurately display 3D images on moving physical objects.\nOur final product entailed a C++ script that utilized OpenCV’s ArUco Marker detection module to estimate positions and scales of markers, a Qualisys Motion Tracking system to track rigid bodies moving around the room, a Unity program to tie all the hardware components together, and necessary documentation for further development of our project.\nWe found that the best way to approach this problem was to first project a scatter board of 25 ArUco markers on the physical calibration board. With the help of OpenCV and the Azure Kinect Camera, we then panned and scaled one projected ArUco marker to the center of the physical calibration board. Once OpenCV determined the ArUco marker was centered, the calibration was completed. Any graphic could then be accurately projected onto the moving board, giving the user an elegant glasses-free augmented reality experience.','[\"Augmented Reality\", \"Calibration System\", \"Projected Augmented Reality\", \"Automated Calibration System\", \"Unity\", \"Qualisys Motion Tracking\", \"ArUco Markers\", \"Computer Vision\", \"OpenCV\", \"C++\", \"C#\", \"Visual Studio\", \"Azure Kinect\", \"Unity Plugin\", \"Extended Reality\", \"Glasses-Free Augmented Reality (AR)\", \"3D Projection\", \"Projection Mapping\", \"Virtual Reality\", \"Azure Camera\"]','Virginia Tech.',12,'Augmented Reality',NULL,NULL,NULL,NULL,NULL),(110,'Blockchain Etextbook','The goal of the Blockchain Etextbook group was to develop new content related to Ethereum under the Blockchain section of the OpenDSA textbook. OpenDSA is designed to inform students and researchers within the field of computer science about key topics within the field. This team specifically covered the content related to Blockchain, a new area of study in computer science. The expected audience for this topic is students and researchers either currently working in, or who are studying the topics within, Blockchain. This textbook aims to provide a single place for referencing material related to the topic.\nUnder the supervision of Dr. Cliff Shaffer at Virginia Tech, the team developed Blockchain content for the textbook. This included creating interactive exercises for the users to learn with and writing prose composed from researching resources about Blockchain and Ethereum. The original description of the project covered topics broadly within Blockchain but Dr. Shaffer narrowed his interest with the team down to Ethereum and topics related to that.\nThe team wrote textbook content related to the concepts of Ethereum including proof of stake, hard forks, crypto hacking, Ethereum Virtual Machine (EVM), and Gas. Our deliverables were reStructured Text files and HTML exercises related to these topics. In addition, the report gives users a tutorial on how to use the chapters within the textbook as well as giving future developers details on how to modify and improve chapters within the books. The team learned some of the issues with writing a textbook on new material since there is often limited or conflicting information regarding the topics.','[\"Blockchain\", \"OpenDSA\", \"Ethereum\", \"Consensus Algorithms\", \"HTML\", \"Javascript\", \"Ethereum Virtual Machines\", \"Hard Forks\", \"Textbook\", \"Crypto Hacking\"]','Virginia Tech',12,'Blockchain',NULL,NULL,NULL,NULL,NULL),(111,'Reddit Shaming Karen','Online shaming behavior has become much more common due to the widespread adoption of online social media platforms such as Instagram and also large online forums such as Reddit. As a result, there have been various words which have been adapted to mean and represent something completely different than they originally did. This project focuses on one specific term which has shown an increase in popularity over the last decade.\nKaren is a slang term for an angry, and often racist middle-aged white woman who polices other people’s behaviors. This term has become associated with various physical features and personality traits such as having a blonde bob haircut, rudely asking to speak to managers, and attempting to police other people\'s behaviors. Unfortunately, there have also been instances of racially motivated actions becoming associated with the word as well.\nThe main goal of this project was to gain an initial understanding of online shaming behavior and also discover potential trends in the usages of the given keyword. As mentioned earlier, Karen will be the associated keyword, although the project will support Dr. Florian Zach with measuring data trends with any keywords he may require. Reddit will act as our sole source of information. Reddit is a social media platform or forum, where unlike other platforms such as Facebook, users gather in given communities (known as subreddits), that discuss whatever the topic of that community is.\nThe motivation for this project stems from a discovery by our client Dr. Zach. He discovered that during 2020, the first year of the Covid-19 pandemic, there was an increase in the number of “Karen” occurrences. Our client is interested in why this increase occurred and what the ramifications might be. Useful inferences can be made from the analysis of this phenomenon such as how “Karen” occurrences can be prevented, the main causes of “Karen” events, and more.\nOur first objective was to first obtain current Reddit data using public APIs. The obtained data was then preprocessed accordingly and stored locally. Our next task was to analyze the data using qualitative analysis in the form of Natural Language Processing (NLP). Some techniques used included noise reduction and removal, stop word removal, and lemmatization. Our resulting steps then provided us with access to a plethora of preprocessed data which in turn allowed various data trends within the specified 36 months to be applied to various graphs and charts for later visual analysis.\nThe completion of this project provided Dr. Zach with access to a plethora of current Reddit data trends related to the keyword “Karen” and will allow both him and future teams to perform numerous types of analyses to study the usage of “Karen”. Additionally, future expansion of this project is promising as our team has allowed for easy adaptability for future students and researchers.',NULL,'Virginia Tech',12,'N/A',NULL,NULL,NULL,NULL,NULL),(112,'Risk Prediction Sentiment Analysis','The risk sentiment analysis tool for workers and workplace is an effort to analyze and determine the safety culture and risk levels of a workplace. Our program will take the narrative reports of the safety and risk conditions from the employees and pass it on to our sentiment analysis software and will return the sentiment values (positive/negative/neutral/mixed) to the users. These values can be referenced by the workplace owners or other employees to have an estimate of the safety conditions at a particular place. Our goal is to accumulate all the information provided by the concerned/satisfied employees, undertake proper sentiment analysis on it and have a reliable output for examination.','[\"risk\", \"risk sentiment\", \"sentiment analysis\", \"workplace risk\"]','Virginia Tech',13,'risk',NULL,NULL,NULL,NULL,NULL),(113,'Library Tweet Support','This project aims to create an easily browsable interface with a collection of NDJSON formatted tweets from Twitter.\nIn the Fall of 2020, 4 teams in CS5604 built a system to manage 3 of the University’s archive collections, of ETDs, Webpages, and Tweets. This system included a webpage front-end to serve these collections to users, as well as a feature for researchers and curators to manage data using a KnowledgeGraph and Apache Airflow.\nThe one front-end team developing this website had a very large task and as such, they were unable to fully flesh out all its features. Specifically, the tweets portion of this website was lacking advanced searching functionality, as well as a clear interactive user interface. My project focused on extending the tweets functionality of this website and managed to accomplish this.\nMy project features a GUI where users can search through the tweets collection and will have results displayed to them one at a time. In my implementation, I used React, CSS, and ElasticSearch. However, the website it is contained in also uses Docker, Flask, Kubernetes, and Python 3.6. The search fields are text, location, and a range search between two dates. When a query is conducted, results will be displayed to the user 5 at a time. Each tweet result contains both the information contained within the tweet result (i.e., username, display name, tweet text, date, favorites, replies, and retweets) as well as data on the user who published the tweet (i.e., total favorites, total posts, total followers, and a link to the source tweet). Also, if a tweet contains a hashtag, each of these are linked to a search on Twitter for that hashtag.\nThis project can be used to browse an archive of tweets. It will be useful in querying tweets for research, such as searching for all tweets made about a subject that were posted from a certain location at a certain time.','[\"Twitter\", \"Tweets\", \"Library\", \"ElasticSearch\", \"React\", \"Indexing\"]','Virginia Tech',13,'Twitter',NULL,NULL,NULL,NULL,NULL),(114,'Musart Web Application','The MusArt web application was created in the hopes of combining available audio and visual technology to be used for good. The origins of this project were inspired by Professor Ico Bukvic, my client, who conducts a significant amount of work at the intersection of music and technology. Many people, who will be recognized throughout this paper, helped in the ideation phase of this project, providing insights from their respective experience. As a result, this project rests on a very broad foundation, pulling together many different ideas into one cohesive final product.\nThe goal of MusArt is to provide users with an application that allows them to express themselves, regardless of self-perceived creative abilities. A parallel goal is to provide an activity to actively relax by using engaging audio and visual stimulation. This benefits the user due to the powerful effects that music has on our biological functions, combined with the benefits of tuning into our senses of vision and hearing. This idea of tuning in is common in meditation practices.\nThis product works on the idea of limiting the amount of choices available to a user, giving them enough freedom to feel that what they are creating is theirs without giving too much freedom that would lead the user to feeling overwhelmed. Regardless of experience, anybody should be able to use this tool as long as they have access to a computer.\nThe MusArt interface consists of a control center, a workspace, and a visual display.\n	The control center allows the user to choose from a set of given music/visual templates, which cover a wide range of styles, then play, pause, and reset this music-visual piece.\n	The workspace is where the user is able to manipulate various aspects of the music/visual piece, using different input devices.\n	The visual display is where the responsive visual appears.\nIncluded in this submission are two versions of the final report (PDF and editable Word document), and two versions of the final presentation. These together cover my progress on this project up to 04/29/2021, along with my current goals for the near future.','[\"Music\", \"Visualization\", \"Art\", \"Drawing\", \"Generative\", \"Creative\", \"Color\", \"FFT\", \"Howler.js\", \"p5.js\"]','Virginia Tech',13,'Music',NULL,NULL,NULL,NULL,NULL),(115,'Disease Spread Simulator','The goal of our project is to provide a prediction tool to allow researchers to model the spread of Chronic Wasting Disease in deer populations in Virginia. We will provide predictions of prevalence of Chronic Wasting Disease on a county by county basis, and simulations of the changing prevalence in counties over time. This tool will be provided in the form of a website, which will allow user input for different parameters affecting the prevalence of Chronic Wasting Disease and displaying results from the predictions our application will make. We will allow the user to provide parameters for the importance of both deer population density and the vegetation level that affects the prevalence of Chronic Wasting Disease. The website will have many options to display the prevalence values we calculate including a statewide heatmap of prevalence by county and a county specific view.','[\"chronic wasting disease\", \"Virginia\", \"prevalence\", \"county\", \"seroprevalence\", \"statistics\", \"Bernoulli\", \"disease\", \"deer\", \"prion\", \"website\", \"R-Shiny\"]','Virginia Tech',13,'chronic wasting disease',NULL,NULL,NULL,NULL,NULL),(116,'Integrated Framework for Dairy Feeding','Herds are typically composed of hundreds to thousands of cows, and are managed in groups of roughly 25 to 100 cows.  Since each group of cows contains so many cows, and each herd has so many groups, individual management of each cow within these groups is impossible, as it would take too much time.  Also, feed costs represent roughly 60-70% of cost of milk production, so feed efficiency is extremely important. The team approached this problem by attempting to create a system that would allow farmers to manage individual cows in a timely manner.  This is done by creating a web-based framework where farmers could simply upload data, and view reports generated by data analysis. The Integrated Framework for the Dairy Feeding project aims to provide a central online platform for dairy production monitoring and optimization.  The web platform created by our team provides the farmer with an easy-to-use interface where the farmer can construct new diets for the individual cattle or pens of cattle based on statistical feedback on previous milk composition data and health reports. Throughout the project term, the Dairy Feeding Framework team found that Django is not very compatible with CentOS 7. Rather a containerized virtual machine on the Virginia Tech cloud container cluster was more compatible and flexible than using RLOGIN on CentOS 7.','[\"dairy\", \"feeding\", \"framework\", \"precision\", \"dairy science\", \"django\", \"feeding effiency\", \"commercial robotic dairies\", \"assessing feed\", \"milk production\", \"cows\", \"pens\", \"diets\", \"mixes\", \"ingredients\", \"storage\", \"TMR Tracker\", \"Select Sires\"]','Virginia Tech',13,'dairy',NULL,NULL,NULL,NULL,NULL),(117,'Downloading patent data for service firms and analyzing the data','The primary task was to create a database in Python, using information from either the United States Patent and Trademark Office or Google Patents, which allows efficient lookups using information such as patent assignee and the patent number. Google Patents was chosen because it contained international patent information rather than being limited to just the United States.\nThe Jupyter Notebook was made to use Beautiful Soup to scrape data from Google Patents. The workflow of the code is to start with a user-defined comma-separated values file that specifies names, e.g., of restaurants and hotel firms, that are relevant to the analysis the user wants to conduct. The first tasks were to read in the query, create a dictionary of company names with associated patent numbers, scrape websites for lxml data, and write raw data to JSON and Excel.\nThe next task was to analyze the stored information qualitatively or quantitatively. Here qualitative analysis was chosen in the form of Natural Language Processing (NLP). The goal was to classify the patents using NLP. The key steps included noise removal, stop word removal, and lemmatization.\nWith this database, we can perform numerous types of analyses to study the effect of patents on the total valuation of companies. It is anticipated that Dr. Zach and future Computer Science students will build upon the current work and conduct additional forms of analysis.','[\"NLP\", \"Patent\", \"Google Patents\", \"USPTO\", \"United States Patent and Trademark Office\", \"Web Scraping\", \"Patents\", \"Python\", \"Jupyter\", \"Notebook\", \"Jupyter Notebook\", \"BeautifulSoup\", \"Data Science\", \"BS4\"]','Virginia Tech',13,'NLP',NULL,NULL,NULL,NULL,NULL),(118,'Lyme Disease in the United States','The main goal of this project is to assist our client, Dr. Luis Escobar, in identifying a number of variables that are causing an increased number of Lyme disease cases in the United States. Our client gave us data regarding the amount of Lyme disease cases per county/per state over the course of a number of years, and we were tasked with finding variables that could contribute to the rise of Lyme disease cases.\nWe were tasked to scrape data from various online sources reporting what could potentially impact the rise of Lyme disease cases, clean and merge the data with the Lyme disease data, generate various regression plots to see if there was a correlation, find the best predictor variables out of the ones we have collected, and generate a choropleth graph using the best predictor variables. A final report of the process, the data, the plots, and an explanation of our findings was also requested.\nUtilizing the programming language R and some web scraping aids that are related, our team was able to scrape data on human population density by county, human population count by county, per capita income per county, human development index (HDI) by county, research and development spending per state, and temperature and precipitation per county. This data was parsed to fit with our Lyme disease data, and multiple regression plots were created using the data we collected as the X-axis and the Lyme disease cases as the Y-axis. Once all of the plots were completed, the plots with the highest correlation were picked out to be generated into choropleth graphs.\nThere were some challenges with finding datasets that were able to fit with our Lyme disease cases, so some improvisations were made with the data we scraped to better fit the original dataset we were given. There was not much data pertaining to each county online, so for some cases, our Lyme disease data had to be merged to be per state rather than per county.\nThe work completed by our team can further the research on Lyme disease to decrease the rates in the number of cases for the foreseeable future. By finding predictor variables that show a high correlation to the increasing number of Lyme disease cases, researchers will be able to focus their attention on these predictor variables to find the most efficient methods of decreasing the number of Lyme disease cases. We expect future Virginia Tech students, Lyme disease researchers, and our client will use and improve upon our code to continue the war against Lyme disease.','[\"Lyme Disease\", \"Data Analysis\", \"Temperature\", \"Precipitation\", \"R\", \"Population\", \"Choropleth Map\"]','Virginia Tech',13,'Lyme Disease',NULL,NULL,NULL,NULL,NULL),(119,'A Graph Representation of Viral Genomes','The best way for researchers in the biology field to understand how viruses mutate, especially during the COVID-19 pandemic, is to compare the different genomes. Our project’s goal was to create software that visualized the comparisons to support this endeavor. The software we created is meant to support the researchers working with the SARS-CoV-2 virus during the 2020 pandemic, with the goal of finding out how the virus mutates and the different strains created. Though our primary goal was to work with the SARS-Cov-2 virus, our software does support comparisons for any genome that needs to be studied.\nOur methods include creating an interactive user interface (UI) where the user can upload different genomes to compare in the visualization. The user can then print or download the visualization that was created, search different genomes stored in the CSV file, and delete any unneeded genomes in the file. The research completed for this project was focused heavily on understanding how viral genomes were viewed and compared as well as learning more in the research area of bioinformatics and computational biology.\nWe were challenged to further our understanding of how to easily portray the genomes in an easy-to-read manner. Our research covered many computational areas, though we were tested in understanding the biology side of the project. This was a fascinating endeavor that furthered our education and opened our minds to understanding the basics of the bioinformatics field. We were able to finish a working prototype by the end of the semester and we hope to see the project grow and adapt to stay relevant in the future.','[\"SARS-CoV-2\", \"Genomes\", \"Mutations\", \"Viral\", \"Bioinformatics\", \"Computational Biology\", \"Visualization\"]','Virginia Tech',13,'SARS-CoV-2',NULL,NULL,NULL,NULL,NULL),(120,'Migrating CS5604 Applications from CS Container Cluster to Digital Library Research Laboratory','This submission is about migrating applications developed by the Fall 2020 CS 5604 Information Storage and Retrieval class from the VT Computer Science container cluster to the Digital Library Research Laboratory. These containerized applications  perform natural language processing and analysis on tweets, digital theses/dissertations and web pages. Our goal was to get these applications running on a new cluster in DLRL, so they can be removed from the VTCS cluster. Our team successfully migrated the applications to DLRL, though several bugs within the applications still remain. The report includes details on our methodology, documentation of changes, and final conclusions on our success. The presentation includes a summary of our accomplishments and an overview of future work needed for the applications to successfully run.','[\"Kubernetes\", \"VT Computer Science container cluster\", \"Digital Library Research Laboratory\", \"containers\"]','Virginia Tech',13,'Kubernetes',NULL,NULL,NULL,NULL,NULL),(121,'Thesis Website','The non-profit educational organization Networked Digital Library of Theses and Dissertations, or NDLTD, currently has a website built with Google Sites. They wish to redo this site to ensure that it can be accessed by as many countries as possible, is built with a system that is not frequently changing, and has a more visually appealing design.\nOur team has been tasked with researching and deciding on a new content management system to create the website with, migrating all of the content to the new website, implementing news and events widgets, coming up with new website designs, and writing documentation for future developers of the site.\nWe first had to research different content management systems, and decided that WordPress best suited the needs of our client. We then downloaded WordPress to a server that could be accessed by all developers of the site. We sketched potential website designs and presented them to the NDLTD Board of Directors. After receiving feedback on our designs, we migrated the content of the old NDLTD website to the new one, and implemented the agreed upon design. We met frequently with members of the NDLTD Board and received constructive feedback on our site implementation. We made a few final changes to the design, tested the website from the user’s point of view, and wrote up documentation for future developers of the website to use.\nWe ran into a few obstacles in the early stages of our project, including picking the hosting platform and setting up the Ubuntu Virtual server. We went back and forth between using WordPress and Wix, however, after feedback from our client, we settled on using WordPress. Additionally, with help from our client, we were able to overcome the difficulties with setting up the server and to proceed with our project.\nWe anticipate that this new website will become the primary NDLTD website. We expect that future developers of this site will make slight modifications to the content of the site and so have provided them with resources that should ease this process.','[\"Theses\", \"Dissertations\", \"WordPress\", \"NDLTD\", \"Website Migration\"]','Virginia Tech',13,'Theses',NULL,NULL,NULL,NULL,NULL),(122,'Knowledge Graph','The knowledge graph project is a two-component project: the first component is concerned with the back endGrakn.AIwhile the second component deals with the front end service registration. The goal of the project is to build a knowledge graph that represents how user information goals are connected to one another. The knowledge graph is connected to a workflow management system that allows developers to register their services and add them to the knowledge graph.\nA knowledge graph is a directed graph data model that stores interlinked entities. Storing data into the knowledge graph allows you to see how this data is connected with other entities in the graph as well as how they are connected. Through this we see the power of a fully fleshed-out knowledge graph. A user may wish to complete a task but has no knowledge about how to complete this task or the tools needed to do so. They can use the knowledge graph and query for this task and thus retrieve the workflow necessary to perform this task including the input files, output files, libraries, functions, and environments.\nDuring this project, research was conducted on both the back end and the front end. On the back end, our team researched how to search through the knowledge graph with Grakn. The front end searched for a suitable method to visualize the knowledge graph. As a result, the Grakn database is able to query the knowledge graph and a Python API is connected to Grakn to allow the front end to display an update to date version of the knowledge graph.','[\"Knowledge graph\", \"Visualization\", \"Grakn.AI\", \"Grakn\", \"Workflow\"]','Virginia Tech',13,'Knowledge graph',NULL,NULL,NULL,NULL,NULL),(123,'ABC Drone Team','The ABC Sports Drone capstone team is an extension of the ABC Drone Project which is a group spearheaded by client Charles Kerr and in conjunction with the VT Club Ultimate team, Burn. The goal of the project as a whole is to provide high-quality footage and streaming of amateur sports to the masses. This capstone team is a subsection of the ABC Drone Project that has been tasked with creating software solutions and developing new techniques to help push this drone project to fruition. This report covers the progress of the capstone team in developing new routines for the drone, and the pivots that have been introduced as the team has received new data. The first goal that was tackled was identifying players on a field from an endzone-to-endzone view. This started with the analyzing of contours in addition to their position and attributes to determine if a contour was a player. Artifacts from off the field of play proved to be greatly troublesome, so a field bounding solution was created to eliminate as many artifacts as possible that were not on the field of play. Fairly good accuracy was achieved with this method (~75%), but the goal was set at 85%+ accuracy for identification. After experimenting with motion-detection and object persistence, the best course of action seemed to be identification via a convolutional neural network. No datasets were available that matched the application of this network, so an original dataset needed to be created. An application was developed that allowed for fairly quick extraction of data from sample videos. This data was fed to the neural network and constantly yields around 94% identification accuracy. Although the accuracy is high, it reduces frame rates to approximately 1 FPS. Some market interviews with actual coaches revealed a larger interest in post-processing capability than live-identification, so the client decided to pivot. A system that allows for speed-editing of footage has been developed, and a (proof of concept) companion application will allow coaches to easily track stats and pre-edit film via a GUI. The speed editing program takes in the footage and allows the coach to use a video game controller to create quick cuts to eliminate down time, as well as pan, tilt, and zoom on the footage to ensure the action is always framed. The edits are recorded in an edit-decision-list (EDL) file which is then sent in conjunction with the video file to Amazon Web Services. AWS takes the EDL file and original video and returns a fully-edited game film. With this method, a 90 minute game can be edited in 5 minutes or less. If coaches are recording stats during the game, the footage will also be annotated with important plays which are recorded on a similar EDL for gameplay statistics. Players will then have access to a program that will allow them to click their name to see the timestamps of all of their highlights.','[\"Python\", \"Machine learning\", \"Drone\", \"Drones\", \"Sports\", \"Recording\", \"Flying\", \"Application\", \"OpenCV\", \"Computer Vision\", \"Sports Stats\", \"Video\", \"Highlights\", \"Software\"]','Virginia Tech',13,'Python',NULL,NULL,NULL,NULL,NULL),(124,'US State Tourism','Each state in the United States has its own state-run website, which is used as a means to attract new tourists to that location. Each of these sites is typically used to highlight any big attractions in that state. Any travel tips, facts regarding that location, blog posts, ratings from other individuals that have traveled there, or any other useful information that may attract potential tourists are also included. These websites are maintained and funded directly by occupancy taxes. Occupancy taxes are a form of state tax that an individual pays whenever one stays in a hotel or visits any attractions in that state. As such, the main goal of these websites is to attract new tourists to their location. These websites are maintained and paid for by past tourists who have visited that state.\nFunding for future state tourism is determined by how many previous tourists have visited the state and paid the occupancy tax. Researchers need to be able to determine which elements of the website are most beneficial in attracting tourists. This can be determined by examining past tourism websites and looking for any patterns that would determine what worked well and what didn’t. These patterns can then be used to determine what was successful and use that information to make better-informed decisions.\nOur client, Dr. Florian Zach of the Howard Feiertag Department of Hospitality and Tourism Management, plans to use the historical analysis done by our team, to further help his research on trends in state tourism websites content. Different iterations of each state tourism website are stored as snapshots on the Internet Archive and can be accessed to see changes that took place in that website. Our team was given Parquet files of these snapshots for the states of California, Colorado, and Virginia dating back to 1998. The goal of the project was to assist Dr. Zach by using these Parquet files to perform data extraction and visualization on tourism patterns. This can then be expanded to other states’ tourism websites in the future.\nWe used a combination of Python’s Pandas library, Jupyter Notebook, and BeautifulSoup to examine and extract relevant pieces of data from the given Parquet files. This data was extracted into various different categories, each with its own designated folder. These categories were raw text, images, background colors and background images, internal and external links, and meta tags. With this data sorted into the appropriate folders, we are then able to determine specific patterns such as what colored background was used the most. With our data extraction portion of this project completed along with the visualization, we hope to pass this on to future teams so that they are able to expand on our current project for the rest of the states.','[\"Python\", \"Data Analytics\", \"Visualizaiton\", \"BeautifulSoup\", \"pyarrow\", \"Jupyter Notebook\", \"Matplotlib\", \"Tourism\", \"Web scraping\"]','Virginia Tech.',13,'Python',NULL,NULL,NULL,NULL,NULL),(125,'Authoritative Venues','This submission details the progress made on the Authoritative Venues project. The goal of the Authoritative Venues project was to use machine learning algorithms to create a web application that can accurately recommend fitting ACM-related venues for Computer Science researchers trying to publish their work. By providing a ranked output list of publication venues related to a paper’s topic, we help researchers make more informed decisions about where to submit their work for publication. Additionally, we provide insight into the data collection, virtual machine setup, and website hosting process that allowed for this project to be easily accessible by anyone. This project is particularly useful for CS researchers wanting to gain insight into which ACM-related publication venue would best fit their paper. The recommender is hosted atauthvenue.cs.vt.edu. On this website, there are two input fields that researchers can use to provide the title and abstract of their paper. Once this is inputted, researchers can submit this information and receive recommendations specifically catered to their work.','[\"Venue Recommender\", \"Authoritative Venues\", \"Venues\", \"Machine learning\", \"ACM Venues\", \"ACM\", \"Research Paper\", \"Computer Science Research\"]','Virginia Tech',13,'Venue Recommender',NULL,NULL,NULL,NULL,NULL),(126,'Prion Database','This submission describes the process and implementation of the work undertaken to create a collaborative Chronic Wasting Disease (CWD) database to document the spread and testing history in the United States. Primarily, the data was from around 1999 to the present, as documentation of tests beyond that becomes much more difficult to obtain. The data used for this project was obtained by attempting to contact all 50 states\' Department of Natural Resources (DNR) and requesting their current CWD testing data. This was met with varied success as only about four states provided well-defined data that could be placed into a national database. After communicating with the client and analyzing the data collected, six points of data were selected to be the focus of the project: state, county, year, total tests, positive tests, and negative tests. Utilizing R Shiny as the platform for deploying the database website, and Google Sheets as the persistent database, our team was able to create a private database website that will allow researchers to share and better understand their data using the tools provided. The data must be kept in a private database as many of the states expressed that they do not want their data to be publically shared as they must ensure it is being used responsibly. The database website features the data in a raw, searchable format as well as graphs and maps that allow whitelisted users to view the spread of CWD throughout the country and over time. The goal for this project moving forward is to have CWD researchers join the private database by agreeing to share their data now, and in the future, which will enable better tracking and predicting of CWD in the United States.','[\"prion\", \"chronic wasting disease\", \"database\", \"cervid\", \"deer\", \"R Shiny\", \"CWD\"]','Virginia Tech',13,'prion',NULL,NULL,NULL,NULL,NULL),(127,'AppTrackWildlifeDiseases','Our project is to design a smartphone application and a website to report mange and other wildlife diseases in realtime. Our free smartphone app is designed for both professionals (e.g. hunters) and non-professionals. Our app provides a mini questionnaire to collect the users\' familiarity with the mange, take photos of the wildlife species and potential disease, and get the geolocation and date of the photo. Then, all information collected will be saved to the firebase and used by the website. Our website will summarize the data and images collected and display them on the map. We submit the PDF and the PowerPoint of our final presentation. Our final presentation starts from project Introduction, then to the project design, timeline, work completed, iOS application, website, testing, future works, lessons learned, acknowledgment, and references. We also submit the PDF and the zip project dump from Overleaf of our final report. Our final report covers Executive Summary /Abstract, Introduction, Requirements, Design, Implementation, Testing/Evaluation/Assessment, Users\' Manual, Developer’s Manual, Lessons Learned, and Acknowledgements.','[\"mange\", \"wildlife\", \"disease\", \"epidemic\", \"bears\", \"infectious\", \"free-of-cost\", \"early detection\", \"firebase\", \"swiftUI\", \"smartphone application\", \"track\", \"iOS\"]','Virginia Tech',13,'mange',NULL,NULL,NULL,NULL,NULL),(128,'YouTube Video Analysis','YouTube (youtube.com) is an online video-sharing platform that allows users to upload, view, rate, share, add to playlists, report, comment on videos, and subscribe to other users. Over 2 billion logged-in users visit YouTube each month, and every day people watch over a billion hours of video and generate billions of views. UGC (User-Generated Content) makes up a good portion of the content available on YouTube, and more and more people post videos on YouTube, many of which become well-known YouTubers. A notable trend to look at for these YouTubers is how their channel grows over time.\nWe were tasked with analyzing how certain YouTubers become successful over time, how their early videos differ from later ones in terms of scripts, and how comments change with fame. Such analysis requires us to look into two sets of data. The first set is numerical data of the channels, which consists of view counts of videos, likes and dislikes on videos, published dates of the video, the interactions between the video creator and the audience, etc. The second set is textual data, which consists of the auto-generated scripts from videos as well as comments from the users. With the help of YouTube APIs and other available helper tools, we are able to scrape the metadata from data of videos and output them as CSV files for future studies.\nFor the analysis, we generate some scatter graphs where each dot stands for one instance of the video, where the x-axis represents the published date while the y-axis represents the views it gets, and then the color of the dot represents some other metrics for evaluation (for instance, the duration of videos). With the Python NLTK package, we are able to conduct analyses over the transcripts from the videos and comments, to see what words are spoken the most, what words appear frequently in the comments and if they are positive or negative, how many words the creator says in a minute, etc. Combining these data we can generate a more thorough scatter graph for discovering if there is a pattern on how certain YouTubers become more and more successful.','[\"Comments\", \"Data Analysis\", \"Frequency Count\", \"YouTube\", \"Social Media\", \"Transcripts\", \"Python\", \"Plotly\", \"NLTK\", \"Jupyter Notebook\", \"Data Collection\", \"Web Scraper\"]','Virginia Tech',13,'Comments',NULL,NULL,NULL,NULL,NULL),(129,'sharkPulse Validation Monitor','Abundance and distribution data of global shark populations is necessary for effective conservation and management. While there are operative direct methods to retrieve such data from scientific surveys and fisheries monitoring, species specific indices of population abundance coming from these sources are rare for most shark species. Yet, there is an abundance of unconventional and unstructured information within social networks that is virtually untapped and has great potential to fill the information gap characterizing shark populations. Social networks such as Flickr and Instagram provide data wells of shark sightings that can be data mined, but must be validated as genuine sightings. Despite its modern surge in popularity, there is little research that implements social media for shark conservation. Here, we show the biological importance of creating an application within sharkPulse to facilitate speedy validation by involving citizen scientists. The Monitor allows users to search a world map for potential shark sightings and fill out forms for popup balloons of these sightings. If it is indeed a shark, they may answer \'yes\' and fill out the associative taxonomic information if they are familiar with the common and/or species name. They may also consult a sharkPulse identification guide if they are not familiar. The application was built with RShiny App software. The Validation Monitor can be used by anyone interested in these charismatic group of animals. The application can be found atsharkpulse.cnre.vt.edu/can-you-recognize/.','[\"Validation\", \"sharkPulse\", \"Shiny App\"]','Virginia Tech',13,'Validation',NULL,NULL,NULL,NULL,NULL),(130,'AWS Tobacco Settlement Retrieval','The Tobacco Industry is one of the largest and most influential industries. It has spent hundreds of millions of dollars on advertising and marketing tactics to ensure dominance and control in the economy. This is especially evident when considering tobacco settlement cases where the enormous power and influence of the Tobacco Industry has allowed them to develop key strategies and tactics for trials and settlement cases over the past century.  Our client Dr. Townsend is currently researching the tactics and inner-workings of the Tobacco Industry over the past few decades to expose the marketing and legal strategies as well as the key players who have been influential in the Industry. Dr. Townsend is utilizing the “Truth Tobacco Industry Documents”, a library of documents created and facilitated by the UCSF Library for research purposes. Our project is meant to further enable researchers specializing in business, public health, law or computer science, who will benefit from easier access to tobacco settlement related documents, with enhanced search capabilities, extending the work of the Fall 2019 CS5604 Information Retrieval teams.\nWe studied the 14 million tobacco related documents from UCSF. We improved upon the indexing of the roughly 8000 depositions, to support line-wise as well as page-wise indexing. We modified and updated existing Python scripts to output the results in the required JSON format, and then pushed the documents into ElasticSearch. Furthermore, we also created another tobacco index and added another 3 million tobacco files to this index. All testing and evaluation work was done using Python scripts. We used the existing Kibana tool for the visual representation of the data.','[\"Tobacco Industry\", \"Deposition Documents\", \"Tobacco Settlement Documents\", \"Depositions\", \"ElasticSearch\", \"Kibana\"]','Virginia Tech',11,'Tobacco Industry',NULL,NULL,NULL,NULL,NULL),(131,'CS Information Session Reservation','The CS Information Session Reservation project is an effort to assist recruiting procedures for the Computer Science department at Virginia Tech. The department has an ambassador program that is responsible for hosting information sessions for prospective students and their families when they visit campus. The main goals of this project are to provide individuals with a means of signing up for information sessions online, and to automate behind the scenes processing for the ambassador program.\nOur clients for this project are the leads of the ambassador program for the CS department, Ruth Labbe Hale and Debbie Zier. After meeting with them multiple times, we came up with a list of requirements and proposed deliverables for the project design. Our clients emphasized the desire to have the manual steps they complete on a day to day basis automated. Their current process heavily utilizes common Google Suite applications such as Gmail, Google Sheets, and Google Calendar. We incorporated these applications into our design to facilitate the transition to the new system.\nOur clients sought at least the following pieces: an online form for prospective students and their families to use to sign up for information sessions, a shared online calendar for student ambassadors and our clients to see upcoming events, a spreadsheet containing relevant information for our clients, and various reminder emails for those signed up for events. Our design utilizes Google Suite applications as well as HTML, CSS, and JavaScript to achieve the aforementioned goals. Upon user submission of the signup form, customized confirmation emails are sent to both the user and our clients reminding them of the upcoming information session. The collected form response is used to automatically populate a spreadsheet with information about the attendee, so our clients can utilize the data if needed. The spreadsheet automatically updates the shared Admin Google Calendar for the selected information session, which is then used by student ambassadors to sign up for these events.','[\"Google Suite\", \"Gmail\", \"Google Sheets\", \"Google Calendar\", \"Google Form\", \"Google Apps Script\", \"Scheduling\", \"Computer Science Department\", \"Automation\"]','Virginia Tech',11,'Google Suite',NULL,NULL,NULL,NULL,NULL),(132,'US State Tourism Websites','In the United States, every state has a tourism website. These sites highlight the main attractions of the state, travel tips, and blog posts among other relevant information. The funding for these websites often comes from occupancy taxes, a form of taxes that comes from tourists who stay in hotels and visit attractions. Therefore, current and past tourists fund the efforts to draw future tourists into the state.\nSince state tourism is funded by the success of past tourism efforts, it is important for researchers to spend their time and resources on finding out what efforts were successful and which weren’t. With this comes the importance of seeing trends in past tourism endeavors. By examining past tourism websites, patterns can be drawn about information that changed, from season to season and year to year. These patterns can be used to see what researchers deemed as successful tourism efforts, and help guide future state tourism decisions.\nOur client, Dr. Florian Zach of the Howard Feiertag Department of Hospitality and Tourism Management, wants to use this historical analysis on state tourism information to help with his research on trends in state tourism website content. Iterations of the California state tourism website, among other sites, are stored as snapshots on the Internet Archive and can be accessed to see changes in websites over time. Our team was given Parquet files of these snapshots dating back to 2008. The goal of the project was to assist Dr. Zach by using the California state tourism website,visitcalifornia.com, and these snapshots as an avenue to explore data extraction and visualization techniques on tourism patterns to later be expanded to other states’ tourism websites.\nPython’s Pandas library was utilized to examine and extract relevant pieces of data from the given Parquet files. Once the data was extracted, we used Python’s Natural Language Processing Toolkit to remove non-English words, punctuation, and a set of unimportant “stop words”. With this refined data, we were able to make visualizations regarding the frequency of words in the headers and body of the website snapshots. The data was examined in its entirety as well as in groups of seasons and years. Microsoft Excel functions were utilized to examine and visualize the data in these formats.\nThese data extraction and visualization techniques that we became familiar with will be passed down to a future team. The research on state tourism site information can be expanded to different metadata sets and to other states.','[\"Parquet\", \"Tourism\", \"State Tourism\", \"Data Extraction\", \"Visualization\", \"Natural Language Processing\", \"Pandas\"]','Virginia Tech',11,'Parquet',NULL,NULL,NULL,NULL,NULL),(133,'Stock Returns','The Stock Returns Project was to assist the research of our client, Ziqian Song, to analyze the language used in the financial news and social media discussions surrounding stock-related events, and to derive meaningful insights from this data. Our end goal was to build meaningful tools that can help the client analyze information surrounding the events that may, in the future, predict a stock price move.\nThe project involved collecting, preprocessing, and analyzing textual data as well as working with stock data from the Wharton Research Data Services (WRDS). Data collection included 49 main categories and 335 subcategories of corporate events with 4.6 million related news and press releases. Following data collection, one of the tasks included identifying 20 influential events for case studies. These are events that have many news reports and tweets surrounding them. We picked 10 stocks that saw significant increases in stock price (surge stocks) and 10 that saw significant decreases in stock price (plunge stocks) to run our data analysis on.\nOnce we selected our 20 companies to evaluate, we used Python modules to scrape Twitter and Google data relating to each company and their specific case study event. By collecting different predictive features (e.g., emotions, top words, topics), we could find valuable correlations between the events and their discussion online. In our findings, we identified words that appeared the most for both surge and plunge stocks, sentiment on Twitter surrounding each respective event, and larger market trends that surrounded each event.','[\"financial market\", \"stock returns\", \"stock market\", \"analyze data\", \"stock twitter feeds\", \"predict stock prices\"]','Virginia Tech',11,'financial market',NULL,NULL,NULL,NULL,NULL),(134,'BikeVT - VTTI Cross-Platform Mobile Application','The main goal of the project, VTTI Bike Mobile App (VBMA), is to create a mobile app for bike data collection for the Virginia Tech Transportation Institute. The mobile app will be cross-platform to operate on both iOS and Android devices. To meet these requirements, VBMA utilizes Microsoft’s program, Xamarin. VBMA allows a user to login through Google authentication. Once a user is logged in, they will be prompted to enter personal information such as weight, height, gender, and type of rider. After this data is entered, a user is brought to the app screen.  VBMA has its components modularized, and represented with tabs on the user interface.\nThe main use of the VTTI Bike Mobile App is to record a user’s ride statistics and data via the phone’s sensors. During a user’s bike trip, data regarding the gyroscope, accelerometer, and geolocation will be tracked by Xamarin’s “Essentials” package. The data retrieved over the course of the bike trip is then pushed up to the user’s Google Firebase database entry. In addition, the current weather of a trip is also recorded and stored in the Firebase database. The openWeatherMapsAPI helps get the surrounding weather conditions based on the user’s location.\nThe app has support for a user to look up a destination and to get a route to their destination from their current location. Lastly, our app has support to get the history of past trips that a user has made. These features allow a user to go for a bike trip and collect data. This data will be analyzed by VTTI for future interpretation and research.','[\"Biking\", \"Data Recording\", \"Xamarin\", \"Cross platform application\", \"Mobile application\"]','Virginia Tech',11,'Biking',NULL,NULL,NULL,NULL,NULL),(135,'Airbnb Scraping','Inside Airbnb is a project by Murray Cox, a digital storyteller, who visualized Airbnb data that was scraped by author and coder Tom Slee. The website offers scraped Airbnb data for select cities around the world; historically data is also available.\nWe were tasked with creating visualizations with listing data over Virginia and Austria to see what impact Airbnb was having on the communities in each respective region. The choice was Virginia and Austria because our team was familiar with both regions, with parts of our team being familiar with Virginia and other parts being familiar with Austria. The eventual goal is to expand past analysis of these 2 regions and expand further to say the rest of the United States. Since July 2019, Tom Slee has abandoned the script2 to collect data. To collect data on Virginia and Austria, we needed to update the script to collect more recent data.\nWe began inspecting the script and found it was not collecting as much data as it once was. This was almost certainly due to Airbnb’s website layout changing over time (a common nature of websites). After finding out how the script worked, we eventually found out the various problems related to the script and updated it to the new Airbnb website design. Doing so, we were able to get even more data than we thought possible such as calendar and review data. From there, we were able to begin our data collection process.\nDuring all the time fixing the script, our team was making mock visualizations to be displayed on a website for easy viewability. Once data collection was complete, the data was transferred over to be used for these mock visualizations. We visualized many things such as how many listings a single host had, how many listings were in a given county, etc. The main visualization created was to see where all the listings for Airbnb were on the map. We displayed this on a map. We also made maps to visualize availability, prices, and the number of reviews. Further, we created pie charts and histograms to represent Superhosts, instantly bookable listings, and price distributions.\nWe expect that in the future the script and the data collected and visualized will be used by both future CS Students working on subsequent iterations of the project as well as Dr. Zach himself, our client.','[\"Data Collection\", \"Virginia\", \"Austria\", \"Airbnb\", \"Visualization\"]','Virginia Tech',11,'Data Collection',NULL,NULL,NULL,NULL,NULL),(136,'Food Safety Site','The goal of our project is to provide a resource to empower farmers market vendors to incorporate safe food handling practices. Our client, Minh Duong, would like to target farmers market growers and vendors who do not fall under the Produce Safety Rule or Preventive Controls for Human Food Rule due to their farm size and annual revenue. Our mobile-friendly web application provides our target audience with a customizable experience with easily accessible and quick information. For example, if a farmer has livestock and grows tomatoes, the information and resources accessible on the website will be tailor-made to growing tomatoes and keeping livestock. We ensure this by providing a survey during account creation, which then determines which information to show based on the survey’s results. We aimed to deliver on our project objectives by implementing a mobile-friendly application using a MERN stack, which comprises MongoDB, Express, React and Node.js. The websitefmfoodsafety.cs.vt.educontains a home page, FAQ page, recalls page, survey page, admin functionalities, and the ability for users to log in to save their data. Sourcing for these pages was directly from the client, or the Food and Drug Administration and Center for Disease Control websites. Testing the site occurred over a 10 day span involving both the team and the client. We were able to receive valuable feedback from the client throughout our implementation with weekly demos. We also used Postman to verify that the backend was implemented correctly.','[\"website\", \"food safety\", \"farmer\'s market\", \"mobile friendly\", \"cdc\"]','Virginia Tech',11,'website',NULL,NULL,NULL,NULL,NULL),(137,'Text Data Mining Studio ETDs','The goal of the Text Data Mining Studio ETD project was to develop a software system that allows less technically-minded researchers to be able to easily access a vast amount of data to be used for text data analytics. Specifically, the problem that our team addressed was the lack of a centralized tool for analysis of a large amount of text files that would be valuable data to be used for machine learning and other forms of analytics for long form text documents.\nOur team created a centralized tool using ElasticSearch, JupyterHub, and Jupyter Notebooks, in the Docker Compose architecture, to provide this service to researchers. We envision this tool being used by researchers whose work involves the ingestion and analysis of large bodies of text. This work could involve producing deep learning based systems for textual analysis and search, for automatic abstract production, or perhaps for trend tracking across highly temporally spaced documents. The tool is intended to be a flexible foundation for these and other research tasks.\nDuring the process of producing this tool, our team learned a great deal about Docker systems, compartmentalization, and JupyterHub, as well as working in a widely distributed team using virtual meeting analogs. Unfortunately the tool is as of yet, incomplete, as the ElasticSearch systems are not linked to the JupyterHub frontend. We are hopeful that given the information outlined in the report that completing the tool will be possible with future work.','[\"ETD\", \"OCR\", \"Elasticsearch\", \"Jupyter\", \"JupyterHub\", \"Jupyter Notebooks\", \"Docker\", \"Docker Compose\"]','Virginia Tech',11,'ETD',NULL,NULL,NULL,NULL,NULL),(138,'Twitter Disaster Behavior','The purpose of the Twitter Disaster Behavior project is to identify patterns in online behavior during natural disasters by analyzing Twitter data. The main goal is to better understand the needs of a community during and after a disaster, to aid in recovery.\nThe datasets analyzed were collections of tweets about Hurricane Maria, and recent earthquake events, in Puerto Rico. All tweets pertaining to Hurricane Maria are from the timeframe of September 15 through October 14, 2017. Similarly, tweets pertaining to the Puerto Rico earthquake from January 7 through February 6, 2020 were collected. These tweets were then analyzed for their content, number of retweets, and the geotag associated with the author of the tweet. We counted the occurrence of key words in topics relating to preparation, response, impact, and recovery. This data was then graphed using Python and Matplotlib. Additionally, using a Twitter crawler, we extracted a large dataset of tweets by users that used geotags. These geotags are used to examine location changes among the users before, during, and after each natural disaster. Finally, after performing these analyses, we developed easy to understand visuals and compiled these figures into a poster.\nUsing these figures and graphs, we compared the two datasets in order to identify any significant differences in behavior and response. The main differences we noticed stemmed from two key reasons: hurricanes can be predicted whereas earthquakes cannot, and hurricanes are usually an isolated event whereas earthquakes are followed by aftershocks. Thus, the Hurricane Maria dataset experienced the highest amount of tweet activity at the beginning of the event and the Puerto Rico earthquake dataset experienced peaks in tweet activity throughout the entire period, usually corresponding to aftershock occurrences. We studied these differences, as well as other important trends we identified.','[\"Puerto Rico\", \"earthquake\", \"Hurricane Maria\", \"Topic Analysis\", \"geotag\", \"geolocation\", \"social media\", \"twitter\", \"disaster\", \"behavior\"]','Virginia Tech',11,'Puerto Rico',NULL,NULL,NULL,NULL,NULL),(139,'Deep Learning Course','Deep Learning Course is an open-source course on deep learning topics hosted on GitHub in the Machine Learning Mindset repository built with the guidance of our client, Amirsina Torfi. We have designed and created four modules -- introduction, basic, neural network, and deep neural network concepts -- with each module containing subtopics. This course will introduce users to some key concepts used in developing and using deep learning and neural network models.\nThe approach to constructing this course was to split our time between researching, developing in-depth documentation on topics, and developing source code to go along with some of the topics. Users may navigate through the course, module by module and subtopic by subtopic in a linear fashion within each module, and execute the supplied sample code. In addition to providing documentation on the topics within deep learning, we supply information on various PyTorch and Python libraries used in the source code. This is to provide supplementary information on the specifics of the code. The goal is to have the user gain a further understanding of deep learning and its application in PyTorch and Python.\nOur course addresses the problems of lack of resources and limited availability of open-source courses on deep learning. Our solution includes contextual materials in addition to source code. The main component of our project is the GitHub repository, with reStructuredText documentation. The repository is publicly available for viewing and suggestions. Thus our group provided the desired open-source course deliverable. To use our course, visithttps://github.com/machinelearningmindset/deep-learning-course','[\"Deep learning (Machine learning)\", \"Python\", \"PyTorch\", \"reStructuredText\", \"Machine learning\", \"Neural Networks\", \"Course\", \"GitHub\", \"Open Source\"]','Virginia Tech',11,'Deep learning (Machine learning)',NULL,NULL,NULL,NULL,NULL),(140,'Music Survey','The main goal of this project was to create an interactive website to survey students about their course perceptions as related to the MUSIC model of learning. The MUSIC Model of Motivation Inventory is a survey developed by Dr. Brett Jones that teachers use to assess their students\' perceptions of the class. The interactive website allows instructors to select which parts of the survey they would like to give their students. After completing the survey, instructors can visit the website to view individual results as well as averages and visualizations.\nThe website is for both teachers and students. Instructors can generate survey links for students, and view results and visualizations. Students can access the survey to enter their answers. The information and results gathered is stored in a database. Based on the gathered results, instructors receive suggestions and feedback on improving their instruction.\nThe surveys are created using HTML, and a database is used to store the answers. Since the website is hosted on BlueHost, phpMyAdmin and MySQL are employed. For the visualization of the results, D3.js, a JavaScript library, is used to provide dynamic and interactive visualizations.','[\"HTML\", \"Website\", \"D3.js\", \"CS4624\", \"PHP\", \"Servers\", \"MySQL\"]','Virginia Tech',11,'HTML',NULL,NULL,NULL,NULL,NULL),(141,'AWS Document Retrieval','In the course CS5604 Information Retrieval, the class built a functioning search engine/information retrieval system on the Computer Science Container Cluster. The objective of the original project was to create a system that allows users to request Electronic Theses and Dissertations (ETDs) and Tobacco Settlement Documents using various fields, through their queries. The objective of our project is to migrate this system onto Amazon Web Services (AWS) so that the system can be stood up independently from Virginia Tech’s infrastructure. AWS was chosen due to its robust nature.\nThe system itself needs to be able to store the documents in an accessible way. This was accomplished by setting up a pipeline that will stream data directly to the search engine using AWS S3 buckets. Each of the two document types were placed into their own S3 bucket. We set up an RDS instance for login verification. This database is used to store user information as they sign-up with the front-end application and will be referenced when the application is validating a user’s login attempt. This instance is publicly accessible and can connect to developer environments outside of the AWS group with the right endpoint and admin credentials. We worked with our client to set up an ElasticSearch instance to ingest the documents along with communicating and manage the health of the instance. This instance is accessible to all of us with permissions and we are able to manually ingest data using cURL commands in the command line. Once the login verification database and ElasticSearch search engine were properly implemented, we had to connect both components to the front-end application where users could create accounts and search for desired documents. After both were connected and all features were working properly, we used Docker to create a container for the front-end application. To migrate the front-end to AWS, we used the Elastic Container Registry (ECR) to push our front-end container image to AWS and store it in a registry. Then we used an ECS cluster running AWS Fargate, a serverless-compute engine for containers, to deploy the front-end to the network for all users to access. Additionally, we implemented data streaming using AWS Lambda so that new entries can be automatically ingested into our ElasticSearch instance. We note that the system is not in a fully demonstrable state due to conflicts with the expected data fields. However, the infrastructure around the various components is established and would just need proper data to read.\nOverall, our team was able to learn many aspects of standing up and building the infrastructure of the project on AWS, along with learning to utilize many different Amazon services. The new system serves as a functioning proof of concept that would allow a feasible alternative other than relying on Virginia Tech’s system.','[\"AWS\", \"Document Retrieval\", \"ElasticSearch\", \"ETD\", \"Tobacco Settlement Documents\", \"Docker\", \"MySQL\", \"Login Verification\", \"Common Storage\", \"Flask Application\", \"Kibana\", \"UCSF Deposition Documents\"]','Virginia Tech',11,'AWS',NULL,NULL,NULL,NULL,NULL),(142,'Efficient Web Archive Searching','The field of efficient web archive searching is at a turning point. In the early years of web archive searching, the organizations only use the URL as a key to search through the dataset, which is inefficient but acceptable. In recent years, as the volume of data in web archives has grown larger and larger, the ordinary searching methods have been gradually replaced by more efficient searching methods.\nThis project will address the theoretical and methodological implications of choosing and running some suitable hashing algorithms locally, and eventually to improve the whole performance of web archive searching in time complexity. At the same time, our project introduces the design and implementation of various hashing algorithms to convert URLs to a sortable and shortened format, as well as demonstrates the corresponding searching efficiency improvement with benchmark results.','[\"Internet Archive\", \"Short URL\", \"Web Archive\", \"searching efficiency\", \"WARC records\", \"Database\", \"Digital Library\"]','Virginia Tech',11,'Internet Archive',NULL,NULL,NULL,NULL,NULL),(143,'Vocalization Detection','DeepSqueak is a deep-learning based system for detection and analysis of ultrasonic vocalizations.\nThe original DeepSqueak model was created by Kevin R. Coffey, Russel G. Marx, and John F. Neumaier.\nRodents engage in social communication through ultrasonic vocalizations, and Dr. Bowers is utilizing DeepSqueak\'s technology to study rats in his lab.\nAviSoft is another software package that has been used by Dr. Bowers, to record and manually analyze sound files gathered from the rats.\nDr. Bowers would like to use all available data to train DeepSqueak\'s classification model, to further improve its accuracy, and to reduce manual analysis and labeling work.\nThe purpose of the Vocalization Detection project is to assist with that effort, leveraging the available data, the two software packages, and our processing.\nInitial efforts involved studying DeepSqueak, AviSoft, and the available data files.\nFurther exploration considered automating use of the tools, and helping with the training of DeepSqueak models.\nThen the work pivoted, to develop matching methods to take data processed with AviSoft, to transform that into labeled data to improve the training of DeepSqueak models.','[\"Deep learning (Machine learning)\", \"Classification\", \"Rat\", \"Vocalization\", \"DeepSqueak\", \"AviSoft\", \"MATLAB\"]','Virginia Tech',11,'Deep learning (Machine learning)',NULL,NULL,NULL,NULL,NULL),(144,'Twitter-Based Knowledge Graph for Researchers','The Twitter-Based Knowledge Graph for Researchers project is an effort to construct a knowledge graph of computation-based tasks and corresponding outputs. It will be utilized by subject matter experts, statisticians, and developers. A knowledge graph is a directed graph of knowledge accumulated from a variety of sources. For our application, Subject Matter Experts (SMEs) are experts in their respective non-computer science fields, but are not necessarily experienced with running heavy computation on datasets. As a result, they find it difficult to generate workflows for their projects involving Twitter data and advanced analysis. Workflow management systems and libraries that facilitate computation are only practical when the users of these systems understand what analysis they need to perform. Our goal is to bridge this gap in understanding. Our queryable knowledge graph will generate a visual workflow for these experts and researchers to achieve their project goals.\nAfter meeting with our client, we established two primary deliverables. First, we needed to create an ontology of all Twitter-related information that an SME might want to answer. Secondly, we needed to build a knowledge graph based on this ontology and produce a set of APIs to trigger a set of network algorithms based on the information queried to the graph. An ontology is simply the class structure/schema for the graph. Throughout future meetings, we established some more specific additional requirements. Most importantly, the client stressed that users should be able to bring their own data and add it to our knowledge graph. As more research is completed and new technologies are released, it will be important to be able to edit and add to the knowledge graph. Next, we must be able to provide metrics about the data itself. These metrics will be useful for both our own work, and future research surrounding graph search problems and search optimization. Additionally, our system should provide users with information regarding the original domain that the algorithms and workflows were run against. That way they can choose the best workflow for their data.\nThe project team first conducted a literature review, reading reports from the CS5604 Information Retrieval courses in 2016 and 2017 to extract information related to Twitter data and algorithms. This information was used to construct our raw ontology in Google Sheets, which contained a set of dataset-algorithm-dataset tuples. The raw ontology was then converted into nodes and edges csv files for building the knowledge graph.\nAfter implementing our original solution on a CentOS virtual machine hosted by the Virginia Tech Department of Computer Science, we transitioned our solution to Grakn, an open-source knowledge graph database that supports hypergraph functionality. When finalizing our workflow paths, we noted some nodes depended on completion of two or more inputs, representing an ”AND” edge. This phenomenon is modeled as a hyperedge with Grakn, initiating our transition from Neo4J to Grakn. Currently, our system supports queries through the console, where a user can type a Graql statement to retrieve information about data in the graph, from relationships to entities to derived rules. The user can also interact with the data via Grakn\'s data visualizer: Workbase. The user can enter Graql queries to visualize connections within the knowledge graph.','[\"Knowledge Graph\", \"Ontology\", \"Subject Matter Experts\", \"Twitter\"]','Virginia Tech',11,'Knowledge Graph',NULL,NULL,NULL,NULL,NULL),(145,'Cholera Database','This project involved work toward a database of Cholera records from 2010 – 2020. The WHO repository was used to extract and normalize data to build CSV files. Each year where data is available has a CSV file containing location and total number of cases in the location. The ProMED repository was used to collect data for the same timeframe. The data was extracted, condensed, and tagged for easier manual viewing. Data for all years available is given in one CSV file. Data from WHO can be viewed in logarithmically colored maps based on the number of cases in each location. These visualizations are produced for each year in the study. The data from ProMED can be viewed in bar graphs which graph the number of articles that occur and in what weeks the articles are written for each country. These visualizations can be seen or downloaded atcholeradb.cs.vt.edu. Additionally, all the CSV files of data produced are available for download on our website. Due to the complexity of NLP and the inconsistencies in the ProMED articles, our data is not completely normalized and requires some manual work. Unforeseen circumstances, including the COVID-19 crisis, slowed the project’s progress. Therefore, the ProMED data extraction did not proceed further, other data repositories have not been explored, and interactive visualizations have not been built. The results of this project are compiled datasets and data visualizations from the WHO and ProMED repositories. These are useful to our client for future analysis as well as anyone else who may be interested in the trends of Cholera outbreaks. The results of data collection are formatted for easy analysis and reading. The graphics provide a simple visual for those who are more interested in higher level analysis. This project can be useful to developers who are working on data extraction and representation in the field of epidemiology or other case based global studies. In the future, more repositories can be explored for more extensive results. Additionally, further work can be done with the ProMED set developed in order to condense it further and eliminate the need for any manual analysis after our program is run. The results of this project are all available publicly oncholeradb.cs.vt.edu, including for download. All code is open source and available on Gitlab.','[\"WHO\", \"World Health Organization\", \"ProMED\", \"Cholera\", \"Cholera Database\", \"Database\", \"Python\", \"spaCy\", \"NLP\", \"BeautifulSoup\", \"Website\"]','Virginia Tech',11,'WHO',NULL,NULL,NULL,NULL,NULL),(146,'Deep Learning - Predicting Accidents','The Deep Learning Predicting Accidents project was completed during the Spring 2020 semester as part of the Computer Science capstone course CS 4624: Multimedia, Hypertext, and Information Access. The goal of the project was to create a deep learning model of highway traffic dynamics that lead to car crashes, and make predictions as to whether a car crash has occurred given a particular traffic scenario. The intended use of this project is to improve the management and response times of Emergency Medical Technicians so as to maximize the survivability of highway car crashes.\nPredicting the occurrence of a highway car accident any significant length of time into the future is obviously not feasible, since the vast majority of crashes ultimately occur due to unpredictable human negligence and/or error. Therefore, we focused on\nidentifying patterns in traffic speed, traffic flow, and weather that are conducive to the occurrence of car crashes, and using anomalies in these patterns to detect the occurrence of an accident.\nThis project’s model relies on: traffic speed, which is the average speed of highway traffic at a certain location and time; traffic flow, which is a measure of total traffic volume at a certain location and time that takes into account speed and number of cars; and the weather at all of these locations and times. We train and evaluate using traffic incident data, which contains information about car crashes on all California interstate highways. This data is obtained from government sources.\nThe relevant data for this project is stored in a SQLite database, and both the code for data organization and preprocessing, as well as the deep learning model, are written in Python. The source code for the project is available athttps://github.com/Elias222/DeepLearningPredictingAccidents.','[\"deep learning\", \"Machine learning\", \"traffic\", \"traffic accident\", \"car accident\", \"Python\", \"California\"]','Virginia Tech',11,'deep learning',NULL,NULL,NULL,NULL,NULL),(147,'Twitter Role Classification','The main goal of this project is to provide a web application that will host the existing TWIROLE model. According to its originators, “TWIROLE, [is] a hybrid model for role-related user classification on Twitter, which detects male-related, female-related, and brand-related (i.e., organization or institution) users. TWIROLE leverages features from tweet contents, user profiles, and profile images, and then applies the hybrid model to identify a user\'s role.” The main use of TWIROLE is to aid future evaluation efforts and research studies relative to investigations that rely upon self-labeled datasets. The web application is made to be easy to use and navigate allowing for a wide range of audiences including researchers or common Twitter users. Other goals of the project were to add a new classifier to the model that will improve the accuracy of TWIROLE. The model previously had only one advanced feature that used the k-top words method to analyze users and classify them. It looked at all of a user\'s tweets and ranked the k-top words used, classifying a user based on the words. The final model includes the previously mentioned advanced feature and an additional advanced feature that analyzes k-top emojis similarly to the k-top words feature. Once features were added, the model was then trained again on the existing data set, improving the accuracy. The website is made up of an HTML page using React on the front end. The backend (TWIROLE) is made using Django to render the HTML page, host images and other resources, and expose a GraphQL API. The front end makes AJAX calls to the GraphQL API which obtains the information that will be displayed on the website. The website is aimed at being simple to manage and update by those with experience in web development specifically Django, React, and GraphQL. The website is hosted by the Digital Library Research Laboratory. using theirdlib.vt.edudomain.','[\"Twitter\", \"Machine learning\", \"Python\", \"User Classification\", \"TWIROLE\", \"Marketing\", \"WebDev\"]','Virginia Tech',10,'Twitter',NULL,NULL,NULL,NULL,NULL),(148,'Automated Exercises','The goal of the Automated Exercises project is to create an automated assessment exercise framework which will allow instructors to build a number of different exercises for the Formal Languages course by uploading a JFLAP file to an OpenDSA textbook. The project will impact both instructors and students. Instructors will use it to build different exercises for students that will eliminate the time and effort to grade these exercises manually. Students will use these exercises to practice more on different topics, as time is available. The final product will eventually have to complete generating exercises, auto-grade exercises, and store students\' answers and grades in an OpenDSA database.\nTo complete the project, we needed to utilize some basic web design language, such as HTML or JavaScript. We had to complete being able to generate, complete, and grade the exercises for all the topics required, including NFA/DFA and PDA. However, the editor in Turing Machine needs more work. The platform should be easily manageable and configurable because the clients, Dr. Mostafa Mohammed and other instructors and students, could make heavy use of this software. We need to make it as easy as possible. The UI part of the platform is mostly designed, and we added more buttons and features to make it useful. However, aesthetics are not our focus since only the instructor would be dealing with our designed platform, whereas the student would be doing exercises on the OpenDSA platform. Data input is for the instructor to upload a JFLAP file to the site and it will be converted to JSON file for the auto grading system. Then, the auto grading system takes the answers of the student to check the correctness in each case, and the result will be shown underneath the graph in a table. The result of the test would be stored into an object inside the exercise grader and shown on the screen as a form of alert, containing attempts, test results, highest scores, and time consumed.\nThe attached presentation and report give details of the project, including our milestones, objectives, methods and lessons learned.','[\"Automated Exercises\", \"JSON\", \"JFLAP\", \"Formal Languages\", \"Multimedia, Hypertext, and Information Access\"]','Virginia Tech',10,'Automated Exercises',NULL,NULL,NULL,NULL,NULL),(149,'PrepLab: Web','PrepLab is a proposed online repository to aid instructors in the remixing and reuse of instructional materials (e.g., syllabi, assignments, learning activities, facilitated discussions, etc.). To facilitate easy adoption it was designed to be as simple as possible for content creators to upload existing material. This project set the constraints of what content may and may not be easily included. When uploading content, users shall select what creative commons license they wish to apply (the system may impose a minimum license) and confirm that they are the original content creator and/or own the rights to any material they are submitting. Instructions to content creators for structuring new material also are included. Upon initial upload of an item the results of the parser/indexer shall be displayed to the user who is given the opportunity to confirm/edit the results.\nThis project is dedicated to working specifically on the Graphical User Interface (GUI) for the PrepLab. React.js, a front end JavaScript framework, was chosen as the design method upon which the front end of PrepLab is built. Other design elements added to the website include Bootstrap as well as traditional CSS libraries. Amazon Web Service (AWS) is used to host the website until a more suitable replacement can be determined when the other sections of this project are completed.\nAs the backend pertains to a different part of the project, the database holding the instructional materials that will populate the site will be covered in future projects related to PrepLab. The other section that is not covered in this part of PrepLab will include parsing and indexing of documents as well as including an API in order to increase functionality and search capabilities.\nOther files that are uploaded are the code base in a Zip file, a poster shown at VTURCS, and a presentation we gave about the final result of this project.','[\"Virginia Tech\", \"PrepLab\", \"Front End\", \"Web\", \"React\"]','Virginia Tech',10,'Virginia Tech',NULL,NULL,NULL,NULL,NULL),(150,'Information and Computer Technology for Indigenous Knowledge Healthcare','The Information and Computer Technology for Indigenous Knowledge Healthcare project is an effort to create a searchable database of complementary healthcare practices accessible on desktop and mobile. As traditional doctors may not be easily reachable in all parts of the world, our database aims to be a digital library to help lay healers and citizens in a more accessible way. The website is accessible athttps://health.cs.vt.edu/.\nThrough meetings with our client, Dr. Agozino, we came up with a list of requirements for the final website. First, it must be easy to administer even for those who don’t have a technical background, to facilitate transferring the website over to another team after we finish the initial implementation. Next, the website must be mobile friendly to increase accessibility around the world. The website should allow for user uploads so that the database of knowledge can grow, with the caveat that user submissions should be moderated as to prevent anything harmful from appearing on the site. Lastly, the site should have external links to a method to donate and references for the posts on the site. Sourcing our posts will give users more confidence in the credibility of the content.\nOur design of the website is a wiki-styled site usingWordpress.org. Each post will correspond to a particular remedy and be sorted into one of the categories of body work, energy work, exercise, mindfulness, food & diet, sound & music, and western. Each post will also be tagged with various symptoms it helps with, to allow for further categorizing of posts. Our design also includes a search bar on the home page, and static pages about references, disclaimers, and a privacy policy.\nOur website is implemented on a virtual machine hosted at Virginia Tech, allowing for flexibility in customizing what we needed for hosting. Our site has an HTTPS connection, and a MySQL database for managing the posts and pages. The frontend of our site implementsWordpress.orgwith the MyWiki theme that fulfills many of our client’s requirements right out of the box.\nFor the future, we plan to implement plugins to cover the other requirements of our project, as well as continue to populate the database based on our research. After we have completed the basic requirements, we have plans for continued moderation and administration of content, which will be covered in our developer’s guide.\nFiles included are:\nIndigenousKnowledgeHealthcareFinalReport.pdf - Our final report\nIndigenousKnowledgeHealthcareFinalReport.docx - Editable version of our final report\nIndigenousKnowledgeHealthcareFinalPresentation.pdf - Our final presentation\nIndigenousKnowledgeHealthcareFinalPresentation.pptx - Editable version of our final presentation\nIndigenousKnowledgeHealthcareVTURCSPoster.png - PNG version of our poster for VTURCS\nIndigenousKnowledgeHealthcareVTURCSPoster.pptx - PowerPoint version of our poster for VTURCS','[\"complementary healthcare\", \"indigenous healthcare\", \"digital library\", \"database\", \"website\"]','Virginia Tech',10,'complementary healthcare',NULL,NULL,NULL,NULL,NULL),(151,'Louisiana French: An Analysis of the Verb Frequency within 28 interviews in Louisiana French','The goal of our project is to assist in the sociolinguistic research of Dr. Carmichael and Dr. Gudmestad. Their project involves the research of Louisiana French, a dialect of the French language, spoken in Louisiana. When we first contacted our clients expressing our interest in this project, they sent us a corpus of interviews conducted by Dr. Carmichael. These transcribed dialogues analyze the dialogue of the interviewees speaking in Louisiana French.\nOur group’s goal is to help our clients analyze verb frequencies in the interviews given to us. We are initially given a verb bank, and our responsibilities are to find and count each conjugation of each verb in the bank. To find each conjugation, we had to account for both regular and irregular verbs in the bank. Irregular verbs are conjugated differently, and for that reason, it involves a different conjugation process. Our first phase of the project involved getting familiarized with the French language in general and how conjugations of a verb, in French, take place. During this time, we have also decided to use Python to build scripts that would extract data from our corpus.\nThough generating Python scripts was the original implementation, we soon realized that using TreeTagger software would be the best and most efficient way to go about the next steps of our project. TreeTagger is a software that can annotate text with part of speech information. Using this software, we are able to calculate the frequency of specific verbs and verb conjugations by parsing the data returned from TreeTagger. TreeTagger uses parameter files for specific languages in order to determine the parts of speech for sentences. Our group is currently using the French parameter file.\nIn some instances, however, French and Louisiana French can differ. For this reason, we have decided that we need to implement a parameter file specifically for that dialect. Our clients, in this way, can make use of the TreeTagger specifically for their dialect of interest. As an additional requirement, we will create a training corpus for the TreeTagger and turn it into a parameter file so it can assist our client for their future endeavors in researching Louisiana French.','[\"French\", \"Linguistics\", \"Frequency Analysis\", \"Verb Conjugations\", \"TreeTagger\"]','Virginia Tech',10,'French',NULL,NULL,NULL,NULL,NULL),(152,'Timeline of Everything','The goal of The Timeline of Everything is to provide a proof of concept for an alternative way for intuitively and quickly viewing history. Using the idea of “walking through history”, a timeline is used where the user can move step by step through the events of key points in history. In addition, users can create their own timelines utilizing already created events or creating their own new events. Each event in a timeline can have a multimedia element associated with it alongside text. Since there are multiple viewpoints of events, three ways to compare timelines were created. First, a simple merging of the two timelines where all events from both would be displayed; second, a contrast of the timelines showing the differences between the two; and lastly, a comparison showing the similarities. Upon doing one of these, a new timeline is generated showing the events, thus allowing an easy way to modify this new timeline.\nBecause this proof of concept might be used to show potential investors, a visually appealing site was the priority for this project. In the future, high traffic optimization and security need to be higher priority goals. The database is currently utilizing SQLite due to its ease of use, however for future implementation, MySQL or similar database should be used. Due to their similarities migrating from one to the other should be fairly easy. Another future goal is utilizing multiple date/time systems; by using a hh:mm:ss or frame system a movie maker could use this system to storyboard and arrange scenes. Using custom time systems, book, TV, and movie lovers could create timelines for their favorite stories.\nThe site is currently available athttp://timeline.cs.vt.edu/','[\"Multimedia/Hypertext\", \"CS4624\", \"timeline\", \"website\", \"history\", \"Multimedia, Hypertext, and Information Access\"]','Virginia Tech',10,'Multimedia/Hypertext',NULL,NULL,NULL,NULL,NULL),(153,'Vertebrate Map Visualization','Our client, Dr. Mims, and a team of researchers, collected trait data on lesser-known vertebrate species in the northwestern United States. The goal of this research was to find links from trait to climate change vulnerability. She then published her data in a report that was made available through VertNet. Since the research comes from publicly available museum records it is only fitting to create a publicly accessible website to not only access the research but to engage the public on this important issue.\nThe goal of our project was to make a multiple page website with quick links, resources, and research all attached to their respective vertebrate/species. We also made sortable lists of the species based off of their trait data. Also to be included with our website is a manual on how to extend or maintain the website for future use and extensibility when we are no longer working on the website.\nAnother focus of the website is an informative visualization/infographic map that allows users to investigate the data of the species and their populations in different regions. Different parts of the map should be linked from each species individual page for easy association of information. Advancement on the infographic/visualization map that allows for input to clarify or maintain interest in the relevant data. Easy to understand controls that allow for detailing or generalizing parts of the map to meet criteria for different areas of interest or research.\nIncluded are the files of trait data given to us by Dr. Mims and our final presentation. This trait data is for the species represented by our website.',NULL,'Virginia Tech',10,'N/A',NULL,NULL,NULL,NULL,NULL),(154,'Tourism Destination Websites','This submission resulted from the semester-long team project focused on obtaining the data of 50 DMO websites, parsing the data, storing it in a database, and then visualizing it on a website. We have worked on this project for our client, Dr. Florian Zach, as a part of the Multimedia / Hypertext / Information Access course taught by Dr. Edward A. Fox. We have created a rudimentary website with much of the infrastructure necessary to visualize the data once we have entered it into the database.\nWe have experimented extensively with web scraping technology like Heretrix3 and Scrapy, but then we learned that members of the Interrnet Archive could give us the data we want. We initially tabled our work on web scraping and instead focused on the website and visualizations.','[\"Tourism\", \"WaybackMachine\", \"Python\", \"Scrapy\", \"Parsing\", \"Web Scraping\"]','Virginia Tech',10,'Tourism',NULL,NULL,NULL,NULL,NULL),(155,'ADS Assessment Video','In our Multimedia/Hypertext/Information Access capstone course, we worked with Adult Day Services to create a training video system to teach new instructors in their organization how to conduct recurring interviews with the adult clients. Adult Day Services is an organization at Virginia Tech that provides person-centered care to older adults who need assistance.\nAdult Day Services also aims to promote the physical, social, emotional, mental, and cognitive health of its participants, and they use a variety of assessments to measure overall well-being and participant progress. These assessments are conducted in the form of interviews, and the body language, tone, and speech of the interviewer are key to performing them successfully.\nThe training video system we created covers five different types of assessments and is designed to efficiently train new instructors to conduct these interviews. We filmed an Adult Day Services instructor conducting interviews with five different participants, each completing the five assessments. We edited the footage and compiled all of the clips of each type of assessment together including transitions and titles. We later created a menu system which allows a user to select to play all of the training videos at once, or to play just the training video for a specific type of assessment. We have also included sub-categories within each type of assessment so the user can decide to view a specific participant as opposed to all. We delivered this project in the form of a Blu-ray .iso file on a USB drive which contains the menu system and the associated videos. We have also included instructions on how to download the VLC media player, which is the optimal software for viewing the contents on the .iso file.\nFinally, we have included our final presentation from our capstone course that goes over the final product as well as the lessons learned and our future plans.','[\"Adult Day Services\", \"ADS\", \"Multimedia\", \"Video\", \"Assessment\"]','Virginia Tech',10,'Adult Day Services',NULL,NULL,NULL,NULL,NULL),(156,'TreeViz','The TreeViz project was developed as part of the capstone course CS 4624 Multimedia, Hypertext, and Information Access and was completed over the course of the Spring 2019 semester. The goal of this project is to allow students taking the CS 2114 data structures classes to visualize code written for tree data structures through CodeWorkout homework exercises. Students will be able to make changes to their code to a tree data structure in real time as they click through visualization steps in the existing interface and our changes to accommodate tree data structures. This project is an extension of the existing work of the client, Mostafa Mohamed, to create visualizations for linked list data structures for exercises for the CS 2114 and 3114 data structures classes.\nOur final visualization can replicate a given tree used to test a student’s sample tree exercise code and step through the code’s execution showing them the changes made to the tree as a result of their solution. The visualization also provides a pointer to the current location of the root of the tree for exercises that require recursion for tree traversal, like the sample exercise we used to test our visualization. The student will also be able to see their code alongside our visualization so they can step through the visualization to see what each line of code specifically changes to the tree in memory. In the future, our work will be able to handle all tree operations, and can be expanded to provide visualizations for many more data structures taught in these courses and ultimately help to improve students’ understanding of the concepts taught to them in class. An explanation of how this works and how to set it up can be found in the TreeVizFinalReport files and the code to test and see the visualizations now can be done with the capstone.zip file. Lastly a brief presentation explaining the work completed during the semester is available through the TreeVizFinalPresentation files.','[\"Visualization\", \"Binary Trees\", \"OpenDSA\", \"OpenPoP\", \"Code Workout\", \"JSAV\", \"JavaScript\"]','Virginia Tech',10,'Visualization',NULL,NULL,NULL,NULL,NULL),(157,'Behavioral Science Android App Development','There is a misconception that gaming and societal excellence are opposing forces, i.e., that a child or young adult who spends time gaming is spending time away from study and chores. Odjin’s Behavioral Rewards Mobile Application, Learn2Game (working name only), will help shift that perception.\nChildren and young adults - Gamers - and their parents and authority figures - Coaches - will be able to use our app to help change the conflict around balancing perceived good and bad behavior. Coaches will be able to send Gamers on quests to do specific chores or to achieve good grades. For doing these quests, Gamers will be promised games by their Coaches. Over time, this will work as a dialog between the Coaches and Gamers where the Gamers see Quests as opportunities for fun rewards, and Coaches see Quests as opportunities to instill good behavior.\nOver the course of the semester, we developed an Android app fulfilling the function of the requirements of the project. This app was built from the ground up as a new project in Android Studio in Java, utilizing four platforms working together: Android Studio, Firebase Authentication, Firebase Live Database, and a game distribution API.\nAny content stored in VTechWorks is not confidential.  Source code has been omitted to preserve company confidentiality.','[\"Android App\", \"Gamer\", \"Coach\", \"Behavioral Rewards\", \"Behavioral Psychology\"]','Virginia Tech',10,'Android App',NULL,NULL,NULL,NULL,NULL),(158,'Tobacco Settlement Documents','Tobacco companies have had some of the best marketing strategies over the past century. It is well documented and well known that tobacco produces both mental and physical health issues, and yet these companies have found ways to remain as one of the largest businesses. The goal of our project is to assist Dr. Townsend in his research to understand Big Tobacco’s strategies.\nThis is done by taking some of the fourteen million documents released by tobacco companies online and presenting the data in a meaningful way so they can be analyzed. This project is hosted on a Virtual Machine provided to the team by Dr. Fox and the VT Computer Science department. The idea for the project is to begin by gathering the documents from online, turning them into a usable text format, then feeding these documents to a Doc2Vec-based machine learning tool that was created with Gensim. Using a pre-trained model, we then need to take this data and cluster it so that it is presentable in a usable manner. Thus Dr. Townsend and many others can use this system to further their research.\nThis submission includes a report on how to use the system and maintain it. This way Dr. Townsend can do what he wants with the system, and any future developers can understand how the system works. This system is comprised of different online components such as a Gensim doc2vec model and a fast approximate nearest neighbor similarity package from Gensim to do the clustering of the data. This has all been stored and set up on the virtual machine provided by the CS department so it should be accessible as long as the user is connected to the campus wifi. Through this project our team learned many things about working with a client, working with new technologies, and how to go about tracking and presenting progress to others.','[\"tobacco settlement documents\", \"Doc2Vec\", \"clustering\"]','Virginia Tech',10,'tobacco settlement documents',NULL,NULL,NULL,NULL,NULL),(159,'Python4ML: An open-source course for everyone','Our project yielded a modular, open-source course on machine learning in Python. It was built under the advisement of our client, Amirsina Torfi. It is designed to introduce users to machine learning topics in an engaging and approachable way. The initial release version of the project includes a section for core machine learning concepts, supervised learning, unsupervised learning, and deep learning. Within each section, there are 2-5 modules focused on specific topics in machine learning, including accompanying example code for users to practice with.\nUsers are expected to move through the course section-by-section, completing all of the modules within the section, reading the documentation, and executing the supplied sample codes. We chose this modular approach to better guide the users as far as where to start with the course. This is based on the assumption that users starting with a machine learning overview and the basics will likely be more satisfied with the education they gain than if they were to jump into a deep topic immediately. Alternatively, users can start at their own level within the course by skipping over the topics they already feel comfortable with.\nThe two main components of the project are the course website and Github repository. The course uses reStructuredText for all of its documentation so we are able to employ Sphinx to generate a fully functioning website from our repository. Both the website and repository are publicly available for both viewing and suggesting changes. The design of the course facilitates collaboration in the open-source environment, keeping the course up to date and accurate.','[\"Machine learning\", \"Python\", \"open source\", \"education\"]','Virginia Tech',10,'Machine learning',NULL,NULL,NULL,NULL,NULL),(160,'Adult Day Services Memory Masterclass Promotional Video','The goal of the project was to create a promotional video for Virginia Tech’s Adult Day Services center, specifically to advertise for their Memory Masterclass program. Adult Day Services is a center located within the Human Development and Family Sciences Department at Virginia Tech. They are licensed by the Department of Social Services to offer personal care, health monitoring, meals, therapeutic activities, dementia care, and recovery assistance. They serve typically 18 participants each operating day who average about 75 years of age. According to ADS’s mission statement, the center is dedicated to providing a center focused on the well-being and optimal functioning of its participants, a resource for caregiver support, an education opportunity for students, and a community among generations of children, college students, and adults. One of ADS’s main service offerings is their Memory Masterclass course. This course is offered in 6-week sessions to participants over 55 years of age who want to maximize their brain health. The focus of the course is to educate and serve people who have been diagnosed with Mild Cognitive Impairment (MCI). MCI is not a symptom or precursor to Alzheimer\'s or dementia, but rather a condition that occurs as aging changes brain function. In the 6-week course participants learn strategies for application to daily life that can strengthen brain reserve as you age and get connected with others who have similar concerns about memory. Our main objective was to create a promotional video that Adult Day Services could use on their website to inform and attract people to take the class.\nThis project was broken up into several different stages. The first stage was to meet with our clients, Adult Day Service professionals, to gain a better understanding of the project requirements. Our clients described to us that they would like a video that showcased the active, healthy lifestyle of one of their Memory Masterclass participants. This would include footage of men and women doing outdoor activities, participating in class, and doing mentally-stimulating activities. From meeting with our clients, we came to realize that they wanted a specific type of aesthetic to their video - a combination of active and “homey” footage. An important goal for our clients was to have the video ready to be presented at an AARP event in mid-March, so the first stage of this project had to be completed by that deadline.\nThe second stage was scheduling time to physically shoot the videos. This involved renting camera and sound equipment, coordinating with our clients and course participants, deciding on filming locations, and collecting the raw footage. Once we had shot all of the raw footage, the third stage was comprised of condensing, cleaning, and enhancing the raw footage to create a preliminary draft of the video. The video was delivered to the client, we received feedback, and have begun work revising the video to meet client specifications. The client will be able to use this video for advertising on the ADS website, as well as at different events where their services are promoted. The fourth stage of this project is what we are currently working on right now. Another recommendation was that we prepare another video that was a bit shorter, approximately 90 seconds long, that could be used as a shorter promotion. This shorter video will likely be a condensed version of highlights from the 4-minute video.\nThe third stage was to revise the initial version of the video based on client feedback. This involved sitting down with our client and gaining specific insight as to what details they liked and what they wanted to have modified. After we acquired feedback, we were able to reshoot footage that was not preferable and take more shots of outdoor activities. The final version of the video incorporated footage from both stages of filming and incorporated the client\'s desired changes. This version of the video was also shown to an applicable user pool of Memory Masterclass students who gave us further feedback.','[\"Memory masterclass\", \"promotional video\", \"adult day services\", \"interim report\", \"ADS\", \"stages of filming\", \"videography\", \"video editing software\", \"iMovie\", \"Memory\", \"masterclass\", \"video\", \"technology structure\", \"technology flow\", \"required technologies\", \"required programs\", \"inventory of files\", \"video length\", \"video dimensions\", \"video format\"]','Virginia Tech',10,'Memory masterclass',NULL,NULL,NULL,NULL,NULL),(161,'Chemistry Website Enhancement','The client, Karen Iannaconne, maintains three websites for Dr. Kingston.  As her main duties are those of an administrative assistant and not a website admin, she cannot spend all her time trying to figure out all the ins and outs of the Ensemble Content Management System (Ensemble CMS) or troubleshooting migrating a website.  Therefore, the main goal of this project is to improve the client’s quality of life using Ensemble’s features and teach the client how to implement them herself.The Kingston website needed a site migration done and was not working due to the paid plugins and themes on WordPress not being automatically migrated.  The bulk of this project is geared towards the Ensemble CMS for the features that the client will be needing instead of a basic tutorial of it as the client already has experience with the system.  User testing was conducted on the Virginia Tech Center for Drug Discovery website as there was a major overhaul of the website.  User testing is also conducted on the Virginia Drug Discovery Consortium website as it was already a complete website and did not need any changes.\nUser testing was done by creating tasks for the subjects in order to navigate the website.  By assigning these tasks and getting feedback from the subjects, we can see what areas of the website need to be changed in order for users to more comfortably navigate through the website.  Although the feedback given sometimes means something is wrong, this does not necessarily mean that we can change what is needed.  This is because the inherent capabilities of the CMS do not allow us to do so in most cases, or doing a complete overhaul is not feasible in the timeframe given.\nInitially, I had taken more of a teaching role in this project.  However, nearing the end of the semester, we needed to get the work finished so that I would have something tangible to show, so the client and I decided to allow me to do all the repetitive work required while still writing up documentation for the client to use in the future.  Most of this work was just creating webpages for the news stories as well as adding information to each of the biography pages for the faculty.  In most of the biography pages, there are hyperlinks that lead to other websites. The client and I decided that these hyperlinks should create new pages whenever a redirection to a new site is made.','[\"Ensemble\", \"Website\", \"Virginia Tech Center for Drug Discovery\", \"WordPress Migration\"]','Virginia Tech',10,'Ensemble',NULL,NULL,NULL,NULL,NULL),(162,'Conversation Facts','The Conversation Facts project is a part of Dr. Fox\'s CS 4624: Multimedia, Hypertext, and Information Access class; it was proposed by Saurabh Chakravarty as a way to help his research in natural language processing. The goal of the Conversation Facts project is to be able to take a summary of a conversation and link it back to where it occurs in the conversation dialogue. We used the Argumentative Dialogue Summary Corpus: Version 1 from Natural Language and Dialog Systems as our dataset for this project. This project was created in Python due to its natural language processing libraries which include spaCy and the Natural Language Toolkit (NLTK) libraries. These two contained the methods and techniques used in the project to parse the data and process it into the parts of speech for us to work with.\nOur general method of approach for this project was to create knowledge graphs of the summaries and the conversation dialogues. This way, we could connect the two based on the entity-relation-entity (ERE) triples. We can then compare the summary triple which would point us back to a corresponding conversation triple. This will link back to the section in the dialogue text that the summary is referencing.\nUpon completion of the project, we have found that our methods outperform naïve implementations of simply running our data through industry standard software, but there are still many things that could be improved to get better results. Our program focuses on utilizing natural language processing techniques, but we believe that machine learning could be applied to the data set in order to increase accuracy.\nThe report explains the requirements set for the team to accomplish, the overall design of the project, the implementation of said design, and evaluation of results. It also includes a User’s Manual and Developer’s Manual to help illustrate how to either run the source code or continue development on the project. Finally, we describe the lessons learned throughout completing the project and list the resources used.','[\"Natural Language Processing\", \"Python\", \"Knowledge Graph\", \"Conversation\", \"Summary\"]','Virginia Tech',10,'Natural Language Processing',NULL,NULL,NULL,NULL,NULL),(163,'Sustainability Values Interactive Web Diagnostic','Sustainability Values Diagnostic, an Interactive Website for Values Diagnostic Reporting and Analysis, is a project for CS 4624 with Dr. Fox. The goal of this project is to develop a website that supports a survey created by our client, Dr. Hull. This survey is designed to help sustainability professionals and students learn about their own preferences, values, and opinions as well as how they compare to others around them. This will in turn help them understand themselves, become better listeners, build productive collaborative efforts, and manage conflict. This website must allow users to take Dr. Hull’s survey, analyze their answers, and provide them with feedback about where they fit in relation to their sustainability values. After giving the user their results, all the data needs to be compiled in such a way that Dr. Hull can download and analyze it for his own research. All of the data accumulated will be kept anonymous to maintain the user’s privacy. This data will allow Dr. Hull to see where his students’ and colleagues’ values lie and how to tackle these sustainability issues in the future.\nThe survey was created by Dr. Hull to analyze a person’s sustainability values and compare them to other people who have also taken the survey. There are two types of questions in the survey. The first asks the user to divide $100 among different options to determine outcomes of sustainable development efforts. The second question type asks users to react to a scenario with a degree of agreement from strongly disagree to strongly agree. These answers are used to perform data analysis and give the user a report, as a downloadable PDF file, that clearly states the user’s values and biases. In addition to this, the report displays how the user compares to previous users’ responses.\nThe website is hosted on the PythonAnywhere platform, which is very easy to use and manage, for both ourselves and Dr. Hull. It also allows us to have a lot of functionality without having to set up the server, database, and other components ourselves. The website’s current URL ishttps://www.sustainabilityvalues.com/​.\nThis project was started by a CS 4624 group in 2018, by Brizuela, X., Stewart, C., Mistry, H., & Eltepu, S. That group’s effort is the base for the current project. We have used some of their previous work for our project, but have also rebuilt the website to better fit our goals. This project will also be passed on at the end of the semester to Dr. Hull, and if needed to another CS 4624 group. This increases the need for detailed and thorough documentation. With this project we will be creating a User Manual, Developer\'s Manual, and well commented code on Github. This will allow for easy editing of the website if needed.\nIncluded in the submission is the group\'s final report. This contains all the documentation the group created over the semester. The group\'s final presentation, given to the CS 4624 class on May 2nd, 2019, is also included in the submission. The group presented at the VTURCS Research Symposium on April 30, 2019 and has included their poster from the event.','[\"Sustainability Values\", \"CS 4624\", \"Diagnostic Reporting and Analysis\", \"Dr. Bruce Hull\"]','Virginia Tech',10,'Sustainability Values',NULL,NULL,NULL,NULL,NULL),(164,'Mayfair Project','For this project, our team is working with Mayfair Group LLC to help create three business websites: Mayfair Project (mayfairproject.com), Better Deposition (betterdep.com), and Electronic Summary (esumry.com). Each website addresses a different purpose for the company. The Mayfair Project site serves the goal of providing key information including the company mission, the staff of Mayfair Group LLC, and the specific projects they currently are working on. It also will link to pages to provide deposition file summarization services for its clients, such as insurance companies and law firms, to help them to achieve better efficiency when processing deposition files. Better Deposition (Betterdep, for short) is to help with better depositions, with experts invited to guide others through a blog. Electronic Summary (Esumry, for short) provides text or file summarization services for all users. Our team’s responsibilities include designing and implementing the front end and user experience for all three websites and hosting them through Amazon Web Services. The six team members have worked in parallel so each site has different members assigned. A similar design language has been applied that will be followed for each of the three websites, to achieve consistency, while still meeting key objectives.','[\"Web development\", \"User Experience Design\", \"Frontend Development\", \"Software as a Service\"]','Virginia Tech',10,'Web development',NULL,NULL,NULL,NULL,NULL),(165,'Interactive Website for Values Diagnostic Reporting and Analysis','The goal of this project is to help sustainability professionals and students learn about their values and biases that impact their work as facilitators. This project focuses on an interactive website which contains a values diagnostic used for analysis and reporting through a survey constructed by Dr. Bruce Hull. The results of the survey will show the survey taker where they stand on sustainability issues. The website was created to optimize the process of parsing the data and outputting a result that Dr. Hull can use as a learning tool.\nThe project is broken up into three different parts: the website platform, data input, and the data output. The website platform needs to be easy manageable. WordPress is a free and open-source content management system, that was the best choice. The Qualtrics survey is the main source of data input. The survey will be static to ensure accurate comparisons of previous data to a user’s current data. It contains two different types of questions: in the first type, the user will be asked to allocate a total global budget of $100 among six choices to determine the outcomes of sustainable development efforts, while in the second type the user is given different scenarios and they must choose their degree of agreement on a scale from strongly disagree to strongly agree. The data analytics will be automated using the Qualtrics API and the Pandas library in Python. The data output is a report which clearly communicates the user’s values and biases, while also displaying how they compare to previous users, to assist the user in learning about their stance on sustainability.','[\"Sustainability\", \"Center for Leadership in Global Sustainability\"]','Virginia Tech',9,'Sustainability',NULL,NULL,NULL,NULL,NULL),(166,'Cloud Digital Repository Automation','The Cloud Digital Repository Automation project uses AWS services to create a Continuous Integration and Continuous Deployment pipeline for the Fedora4 digital repository system.  We have documented our process, services, and resources used so that the pipeline can be modified and expanded upon further.\nThis project is for our course CS4624: Multimedia, Hypertext, and Information Access. We create an automated deployment pipeline using AWS resources.  The overall purpose of this project is to automate some of the more mundane yet essential aspects of building and deploying a codebase.  Taking source code from a repository and updating based on the recent changes can be a hassle as well as manually time consuming.  This process of updating and bug fixing source code is not a new concept but can be made easier if some, if not all, of the building, testing, and deploying is done automatically.  This project aims to help the Fedora4 development team by providing a baseline pipeline configuration that can handle updates to source code, and subsequently build, test, and deploy the new updates and changes.','[\"Automated Pipeline\", \"Amazon Pipeline\", \"Deployment\", \"Fedora 4\", \"CodePipeline\", \"CodeBuild\"]','Virginia Tech',9,'Automated Pipeline',NULL,NULL,NULL,NULL,NULL),(167,'Tracking VT Student Production Experience','Our client, The Virginia Tech Theatre Department is required by their accrediting body (National Association of Schools of Theatre), to maintain records of their students’ experience in various productions. They asked us to modernize their previously existing system, which was impractical and had fallen into disuse. We decided that the optimal solution would be a web-based interface to collect and store records entered by the students.  Our main priority was that the system require minimal upkeep, while still having the flexibility and scalability to meet the growing needs of the School of Performing Arts. To this end, we designed and built a a front-end web page and a back-end database.\nThe website is built using PHP and JavaScript/jQuery. The database is a MySQL database. It consists of a login-in screen, for students and admins. Students can submit records, view/delete their own records, and download a copy. Admins can view/delete all students records, as well as download a copy. All data is submitted by the students. The project source code files contain the files for the website and the SQL commands to build the database tables.','[\"Theatre Department\", \"Student Logging\", \"Production Tracking\", \"Database\", \"PHP\", \"mySQL\"]','Virginia Tech',9,'Theatre Department',NULL,NULL,NULL,NULL,NULL),(168,'Cloud Digital Repo Optimization','The goal of the project is to scale down the CloudFormation templates for deploying the Hyku digital repository application. We have attempted to reduce the cost of running the Hyku application with a base level of performance, essentially reducing it to the minimum viable scale. We have accomplished this by changing these templates and their configuration parameters to use less instances at smaller sizes. After evaluating a number of different options for reducing the base cost, including using other AWS offerings, we have settled on a number of parameters that work well at the base level of performance. In testing these changes, we used a qualitative method of testing the functionality of the existing feature set on the original deployment and comparing that to the functionality of the new deployment. We have seen no changes in functionality from the original deployment.\nThe cost reduction we see with these reduced instance sizes is to about one third of the original cost, resulting in massive savings given that the original cost of running the application was about $800-900 a month. The new cost of running our modified templates with the parameters we have tested is about $300 a month. Given that the original feature set is still functioning as it was before, we believe that we have achieved a satisfactory reduction of cost from the original deployment, and therefore have accomplished the goal we set out to complete.\nWe provide documentation on our process and the changes we made, including on how to reproduce in the future the changes we have made. Since the templates require some level of maintenance, this documentation is vital for deploying them in the future. The documentation provided by the report gives future maintainers the ability to quickly get up and running with the potential problems encountered when working with the templates, and gives future groups the insight to predict the kinds of challenges they will face when working on the Hyku CloudFormation templates.','[\"CloudFormation\", \"Hyku\", \"Hydra-in-a-Box\", \"Samvera Labs\", \"AWS\", \"Amazon Web Services\"]','Virginia Tech',9,'CloudFormation',NULL,NULL,NULL,NULL,NULL),(169,'Tweet URL Analysis','The goal of the GETAR project is to devise interactive, integrated, digital library/archive systems coupled with linked and expert-curated web-page/tweet collections. In this class team project, the URL analysis system we designed takes a tweet collection as input and uses Hadoop and Spark to extract short URLs. We expanded them, fetched their web-page with the corresponding long URL, and applied the WayBack CDX Server API to attempt to restore the most likely snapshot. Then, we conducted a systematic URL analysis, for different types of events. We analyzed nine tweet collections in four categories: Nature, Health, Man-made, and Particular Event.  Each tweet collection contains the tweet content from 2013-2017 that related to a specific keyword. For each collection, we analyzed several characteristics in URLs, top-k domains of the URLs, URL retrieve rate, and URL retrieve rate boosted by using the WayBack CDX Server API. We provided several visualizations of the results we analyzed from these nine tweet collections. We have refined this project so that it is easy to build on; see section 5 (Developer Manual) in the final report for details.','[\"Digital Library\", \"Web-page\", \"Data Mining\", \"Hadoop\", \"Scala\", \"Tweet Collections\", \"URLs\"]','Virginia Tech',9,'Digital Library',NULL,NULL,NULL,NULL,NULL),(170,'Opinion Mining Summarization','Opinion Mining Summarization is a Multimedia, Hypertext, and Information Access capstone project proposed by Xuan Zhang to Dr. Edward Fox.  The purpose of the project is to generate a suite of tools that can generate useful data about products sold on the internet.  The final goal is a suite of tools that, when given a product, can scrape the web for review data on that product and create easily accessible summaries of these reviews.  This will allow the user to see the general opinion of online consumers on a given product.\nIn our design phase, we divided the overall project into four main deliverables: a web-crawler, database, web application, and a suite of summarization tools (see inventory for more details).  To begin our development process, we identified open source libraries that performed some of the functionality our tools would need.  From these libraries, we were able to begin developing tools specific to the needs we identified during our research phase.\nWe practiced test-driven development, frequently testing our tools on example websites and sample data, in order to ensure correctness and identify any needed design changes.  For example, as the project progressed, simulated user testing identified the need for a more user-friendly way to interact with the tools.  This led us to design a web application to provide a GUI for the program.  Through this web application, it was planned that the user would be able to generate and browse product review summarizations, as well as start web-crawling requests in real time.\nAt the conclusion of the project, we have a full, cohesive tool.  Through the web application, the web-crawler Python scripts can be used, review and summarization data is stored in a MySQL database, a variety of Python summarization scripts can be run on review sets, and the results can be cleanly viewed.\nThroughout the process of this project, we learned a great deal about full-stack development.  Everything we interacted with provided us with a new opportunity for learning and growth, whether it was Python scripting or the .NET framework.  As well, integrating multiple tools written in different languages provided a new challenge for our team, beyond what we had experienced in previous classes.  Overall, the start-to-finish completion of a major project was an excellent learning experience that will serve us well as we approach graduation and our future careers.','[\"crawler\", \"summarization\", \"opinion mining\", \"product reviews\", \"LDA\", \"extract\"]','Virginia Tech',9,'crawler',NULL,NULL,NULL,NULL,NULL),(171,'Visual Displays of School Shooting Data','In order to understand and track emerging trends in school violence, there is no better resource than our current population. Sixty-eight million Americans have a Twitter account and with the help of the GETAR (Global Events and Trend Archive Research) project, we were able to create datasets of tweets related to 10 school shooting events. Also, we have retrieved the URLs of news headlines relating to the same shootings. Our job is to use both datasets to develop visualizations that may depict emerging trends.\nBased on the data that we had available, we were able to come up with a few ideas such as word clouds, maps, and timelines. The goal was to choose appropriate representations that would provide insight into the changing conversation America was having about gun violence. We have been successful in creating these visuals and shifted our focus to cleaning our data.','[\"Tweets\", \"Twitter\", \"Visualizations\", \"Data Collection\", \"Data Mining\", \"Multimedia\", \"Hypertext\", \"Information Access\", \"URL\", \"Sentiment\"]','Virginia Tech',9,'Tweets',NULL,NULL,NULL,NULL,NULL),(172,'Tweet URL Extraction Crawling','In the report and supplemental code, we document our work on the Tweet URL extraction project for CS4624 (Multimedia/Hypertext/Information Access) during the spring 2018 semester at Virginia Tech. The purpose of this project is to aid our client Liuqing Li with his research in archiving digital content, part of the Global Event and Trend Archive Research (GETAR) project supported by NSF (IIS-1619028 and 1619371). The project requires tweet collections to be processed to find links most relevant to their respective events, which can be integrated into the digital library. The client has more than 1,400 tweet collections with over two billion tweets, and our team found a solution that used machine learning to deliver event related representative URLs.\nOur client requested that we use a fast scripting language to build middleware to connect a large tweet collection to an event focused URL crawler. To make sure we had a representative data set during development, much of our development has centered around a specific tweet collection, which focuses on the school shooting that occurred at Marshall High School in Kentucky, USA on January 23, 2018. The event focused crawler will take the links we provide and crawl them for the purpose of collecting and archiving them in a digital library/archive system.\nOur deliverables contain the following programs:extract.py,model.py, create_model.py, andconversion.py. Using the client’s tweet collection as input,extract.pyscans the comma separated values (CSV) files and extracts the links from tweets containing links. Because Twitter enforces a character limit on each tweet, all links are initially shortened.Extract.pyconverts each link to a full URL then saves them to a file. The links at this stage are separate from the client’s tweet collection and are ready to be made into testing and training data.\nAll of the crucial functionalities in our program are supported by open source libraries, so our program did not require any funds to develop. Further developments of our software could create a powerful solution for our client. We believe certain functions within our code could be reused and improved upon, such as the extractor, model, and the data we used for testing and training.','[\"Machine learning\", \"Data Mining\", \"Web Crawling\", \"Tweet Collections\", \"GETAR\", \"Twitter\", \"Events Archive\", \"Web Scraping\", \"Article Filtering\"]','Virginia Tech',9,'Machine learning',NULL,NULL,NULL,NULL,NULL),(173,'Blog and Forum Collection for Trail Study','This project is focused on the culture and trends of the Triple Crown Trails (Appalachian Trail, Pacific Crest Trail, and Continental Divide Trail). The goal of this project is to create a large collection of forum and blog posts that relate to the previously stated trails through the use of web crawling and internet searching. One reason for this project is to assist our client with her Master’s Thesis. Our client, Abigail Bartolome is focusing her thesis on the different trends and different ways of life on the Triple Crown Trails, and the use of our tool will help her. The impact of our project is that it will allow our client to be able to sift through information much faster in order to find what she does and does not need for her thesis, instead of wasting time searching through countless entries with non-relevant information. Abigail will also be able to sift through what kind of information she wants specifically through the use of our tagging system. We have provided the dates, titles, and author of each post so she can immediately see if the article has relevant information and was posted in a time frame that is applicable.\nThe project will have two main focuses, the frontend and the backend. The frontend is an easy-to-use interface for Abigail. It will allow her to to search for specific tags, which will filter the blog posts based on what information she seeks. The tags are generated automatically based on the content of all of the forums and blogs together, making them very specific which is good for searching for the kind of content desired by our client. When she finishes adding tags, she can then search for blogs or forums that relate to the topics tagged. The page will display them in a neat format with the title of the article that is hyperlink-embedded so she can click on it to see the information from the article, as well as the author, date, and source of the post.\nThe backend is where all the heavy lifting will be done, but obviously is invisible to the client. This is where we will go through each of the blog or forum websites fed into the web crawler to store all of the relevant information into our database. The backend is also where the tagging system is implemented and where tags are generated and applied to blog posts. WordPress and BlogSpot (for the most part) have a uniform way of going through blogs, so our web crawler acts accordingly based on which website it is, and is able to go through until there are no more blogs on that site. All of the blog posts, contents, pictures, tags, URLs, etc. are stored in the backend database and then linked to our frontend so that we can display it neatly and organized to the liking of Abigail. From 31 sources we have collected 3,423 blog posts to which have been assigned 87,618 tags.\nTogether, the frontend and the backend provide Abigail with a method to both search and view blog post content in an efficient manner.','[\"Trail\", \"Blogs\", \"Web Scraping\", \"Django\", \"Python\", \"Presentation\", \"Report\", \"Application\", \"Trail Culture\"]','Virginia Tech',9,'Trail',NULL,NULL,NULL,NULL,NULL),(174,'Geo Spatial Database Activity Visualization Tracker','In today’s business environment, companies should be empowered to explore their data through a simple, visual medium, rather than overwhelming and time-expensive tabular reports. Virginia Tech’s IT department provides services to supply international and domestic students, faculty, staff, parents, and alumni with IT and computer solutions. Maptivity is a tool that our group built for the IT department. Our focus was to provide them with a way to more effectively see connections and patterns among data collected from these services. This was accomplished by developing a software framework that displays a world map and shows pings for data events.\nOur client, Claire Gilbert, executive director of IT experience and engagement at Virginia Tech, supplied us with data of incoming and outgoing phone call services. We designed this project with the intention of receiving a real-time data feed; however, due to the permissions required and timeframe given, we instead worked with previously logged data as a proof of concept.  The end result was a unique playback simulation feature.  This feature provides customizable playback and analysis of any data set.\nWe currently have the entirety of the call center’s phone data for the years 2016 and 2017 stored on our server. Our server parses the phone data into a common object format first, then the client can run simulations on this data at up to 100,000x speed.  After the data points ping on the map, the points are then retained on the map as translucent bubbles, providing a heatmap effect over time.','[\"Data Visualization\", \"Web\", \"JavaScript\", \"ReactJS\", \"Datamaps\"]','Virginia Tech',9,'Data Visualization',NULL,NULL,NULL,NULL,NULL),(175,'EmoViz - Facial Expression Analysis & Emotion Data Visualization','The report describes the EmoViz project for the Multimedia, Hypertext, and Information Access Capstone at Virginia Tech during the Spring 2018 semester. The goal of the EmoViz project is to develop a tool that generates and displays visualizations made from Facial Action Coding System (FACS) emotion data.\nThe client, Dr. Steven D. Sheetz, is a Professor of Accounting and Information Systems at Virginia Tech. Dr. Sheetz conducted a research project in 2009 to determine how human emotions are affected when a subject is confronted with analyzing a business audit. In the study, an actor was hired to record a five minute video of a simulated business audit in which they read a pre-written script containing specific visual cues at highlighted points throughout the duration of the audit. Participants of the study were divided into two groups, each of which was given a distinct set of accounting data to review prior to watching the simulation video. The first group received accounting data that had purposely been altered in a way that would indicate the actor was committing fraud by lying to the auditor. The second group received accounting data that correctly corresponded to the actor’s script so that it would appear there was no fraud committed. All participants watched the simulation video while their face movements were tracked using the Noldus FaceReader software to catalog emotional states. FaceReader samples data points on the face every 33 milliseconds and uses a proprietary algorithm to quantify the following emotions at each sampling: neutral, happy, sad, angry, surprise, and disgust.\nAfter cataloging roughly 9,000 data rows per participant, Dr. Sheetz adjusted the data and exported each set into .csv files. From there, the EmoViz team uploaded these files into the newly developed system where the data was then processed using Apache Spark. Using Spark’s virtual cluster computing, the .csv data was transformed into DataFrames which helps to map each emotion to a named column. These named columns were then queried in order to generate visualizations and display certain emotions over time. Additionally, the queries helped to compare and contrast different data sets so the client could analyze the visualizations. After the analysis, the client could draw conclusions about how human emotions are affected when confronted with a business audit.','[\"Visualization\", \"Business Audit\", \"Emotion\", \"Facial Recognition\", \"Python\", \"Spark\"]','Virginia Tech',9,'Visualization',NULL,NULL,NULL,NULL,NULL),(176,'Tweet Collections','Over the past decade, social media use has grown exponentially.  More and more people are using social networks to connect and communicate with one another, which has given birth to a new source of data: social media analysis.  Since Twitter is one of the largest platforms for text based user input, many tools have been created to analyze data from this social media network.\nThe TweetCollections project is designed to analyze large amounts of tweet collection metadata, and provide additional information that makes the tweet collections easy to categorize and study.  Our clients, Liuqing Li and Ziqian Song, have provided our team with a set of tweet collections and have asked us to assign metadata to them so that future researchers are able to easily find relevant collections. This includes assigning tags and categories, as well as a description with an accompanying source. Formerly, this process had been done by hand.  While this improves the accuracy of the data collected, it is too expensive and time consuming to maintain.  Our team has been tasked with speeding up the process, using scripts to find information for these fields and fill them out.\nThe majority of technology used in our approach has been concentrated on Python and its many libraries.  Python has made it easy to quickly parse through our tweet collection data by treating the input as an Excel file, as well as pulling other relevant information from third party sources like Wikipedia.  The driver will create a new, updated Excel file with the additional data, categories, and tags. Additionally, an ontology will be produced and serve as reference for categorizing topics listed in the fields from the input.\nThe GETAR team has created over 1400 tweet collections, containing over two billion tweets.  To help categorize this data, they also store metadata about these collections in a Comma Separated Value (.csv) file.  This project will result in a product that will take in a CSV file of the archive of tweet collections metadata as input, with the required fields (such as “Keyword” and/or “Date”) filled in, and produce a separate Comma Separated Value file as output with missing fields filled in. The overarching problem is that each category term is rather vague, and more data will need to be pulled out of this term. Additionally, an ontology will be produced and serve as reference for categorizing topics listed in the fields from the input. The completed project contains three Python scripts: csv_parser.py, search_wikipedia.py, andGUI.py. Together, these create a program that can take in an input CSV file and integer range for which lines to run, and then return a new CSV file with the additional metadata filled in. Also included with the deliverable is a populated Excel file, with over 150 additional entries of metadata, and an error file containing recommendations for the ontology.  These recommendations are generated from any results our driver determines as ‘low relevance’, and returns options with a higher term frequency.','[\"Tweet Collections\", \"CSV Parsing\", \"GETAR\"]','Virginia Tech',9,'Tweet Collections',NULL,NULL,NULL,NULL,NULL),(177,'Paleontology Topic Trends','The purpose of the project was to run modern data analysis on abstracts created by the Society of Vertebrate Paleontology. The Society of Vertebrate Paleontology has a yearly convention in which members from all over the world gather together and present their studies from the appropriate year. Our client, Professor Sterling Nesbit, provided our group with a collection of abstracts dating back to 1987. Our job was to take all of the abstracts from each year and run analyses to see the trends and patterns spanning over all the years that the Society of Vertebrate Paleontology had been publishing abstracts in collections. The method the team has employed changed throughout the span of the project. In the beginning, the team planned on using Latent Dirichlet Allocation or LDA to summarize the abstracts. This would find the topics prevalent in the collection, and show the mix of those topics found in each of the abstracts. After further discussion with our client, the team decided on providing more straightforward analysis, based off graphing hierarchies in the abstracts. In order to properly run the graphing analysis on the abstracts our team had to scrape the abstracts to ensure the most useful data was not overlooked in the analysis. The process of scraping the abstracts began with removing all the hypertext markup tags from the abstract text files (which were converted from PDF). Then the team eliminated any English stop words in the text files to remove words that are not commonly needed for analysis. The next step was to customize and add words to this list of stop words, based on yearly differences. For example, in some years the Society of Vertebrate Paleontology required its members to create their abstracts referencing the United States as “The United States of America” while in other years they were required to reference it as “United States.” These slight changes required our team to alter our method of stop word elimination to be specific to each year. Once the scraping was done, the team created graphing scripts to produce graphs based off Vertebrate Paleontology hierarchies. After meeting with our client multiple times to further refine our analysis, we created the final analysis script version. These graphs helped our client visualize the patterns in findings made by the Society of Vertebrate Paleontology. The project should be further developed to automatically extract abstracts from the convention’s PDF collection, as well as some sort of update to stop words based off of the society’s yearly modifications.','[\"Paleontology\", \"Word Clouds\", \"Topic Analysis\", \"Python\", \"Data Analysis\", \"Graphical Analysis\"]','Virginia Tech',9,'Paleontology',NULL,NULL,NULL,NULL,NULL),(178,'Event Trend Detector','The Global Event and Trend Archive Research (GETAR) project is supported by NSF (IIS-1619028 and 1619371) through 2019. It will devise interactive, integrated, digital library/archive systems coupled with linked and expert-curated webpage/tweet collections. In support of GETAR, the 2017 project built a tool to scrape the news to identify important global events. It generates seeds (URLs of relevant webpages, as well as Twitter-related hashtags and keywords and mentions). A display of the results can be seen from the hall outside 2030 Torgersen Hall.\nThis project extends that work in multiple ways. The quality of the work done has been improved. This is evident in changes done to the clustering algorithm and the user interface changes to the clustering display of global events. Second, in addition to events reported in the news, trends have been identified, and a database of trends and related events were built with a corresponding user interface according to the client’s preferences. Third, the results of the detection are connected to software for collecting tweets and crawling webpages, so automated daily runs find and archive webpages related to each trend and event.\nThe final deliverables include development of a trend detection feature with Reddit news, integration of Google Trends into trend detection, an improved clustering algorithm to have more accurate clusters according to k-means, an improved UI for important global events according to what the client wanted, and an aesthetically pleasing UI to display the trend information. Work accomplished included setting up a table of tagged entities for trend detection and configuring the database for clustering and trends to work with our personal machines, and completing the deliverables. Many lessons were learned regarding the importance of using existing tools, starting early, doing research, having regular meetings, and having good documentation.','[\"Trend Detection\", \"Trends\", \"Python\", \"GETAR\", \"Reddit\", \"Google\", \"News trends\"]','Virginia Tech',9,'Trend Detection',NULL,NULL,NULL,NULL,NULL),(179,'Artificial Immune System (AIS) Based Intrusion Detection System (IDS) for Smart Grid Advanced Metering Infrastructure (AMI) Networks','The Smart Grid is a large system consisting of many components that contribute to the bidirectional exchange of power. The reason for it being “smart” is because vast amounts of data are transferred between the meter components and the control systems which manage the data. The scale of the smart grid is too large to micromanage. That is why smart grids must learn to use Artificial Intelligence (AI) to be resilient and self-healing against cyber-attacks that occur on a daily basis. Unlike traditional cyber defense methods, Artificial Immune System (AIS) principles have an advantage because they can detect attacks from inside the network and stop them before they occur.\nThe goal of the report is to provide a proof of concept that an AIS can be implemented on smart grid AMI (Advanced Metering Infrastructure) networks and furthermore be able to detect intrusions and anomalies in the network data. The report describes a proof of concept implementation of an AIS system for intrusion detection with a synthetic packet capture (pcap) dataset containing common Internet protocols used in Smart grid AMI networks.\nAn intention of the report is to provide the necessary background for understanding the implementation in the later sections. The background section defines what a smart grid is and how its Advanced Metering Infrastructure (AMI) works, describing all three networks the AMI consists of. The Wide Area Network (WAN) is one of the three networks and we were scoping down to WAN for our project. The report goes on to discuss the current cyber threats as well as defense solutions related to the smart grid network infrastructure today. One of the most widely used defense mechanisms is the Intrusion Detection System (IDS), which has many important techniques that can be used in the AIS based IDS implementation of this report.\nThe most commonly used AIS algorithms are defined. Specifically, the Negative Selection Algorithm (NSA) is used for our implementation. The NSA algorithm components used in the implementation section are thoroughly explained and the AIS based IDS framework is defined. A list of AIS usages/values in enterprise networks is presented as well as research on current NSA use in AIS implementations.\nThe latter portion of the report consists of the design and implementation. Due to data constraints and various other limitations, the team wasn’t able to complete the initial implementation successfully. Therefore, a second implementation design was created, leading to the main implementation which meets the project’s objective. The implementation employs a proof of concept approach using a C# console application which performs all steps of an AIS on user created network data.\nIn conclusion, the second implementation has the ability to detect intrusions in a synthetic dataset of “man-made” network data. This proves the AIS algorithm works and furthers the understanding that if the implementation was scaled up and used on real-time WAN network data it would run successfully and prevent attacks. The report also documents the limitations and problems one can run into when attempting to implement a solution of this scale. The ending sections of the report consists of the Requirements, Assessment, Assumptions, Results, and lessons learned followed by the Acknowledgments to MITRE Corporation which helped immensely throughout the development of the report.','[\"Smart Grid\", \"Artificial Intelligence\", \"Artificial Immune System (AIS)\", \"Advanced Metering Infrastructure\", \"Wide Area Network\", \"Negative Selection Algorithm\"]','Virginia Tech',9,'Smart Grid',NULL,NULL,NULL,NULL,NULL),(180,'Graph Query Portal','Prashant Chandrasekar, a lead developer for the Social Interactome project, has tasked the team with creating a graph representation of the data collected from the social networks involved in that project. The data is currently stored in a MySQL database. The client requested that the graph database be Cayley, but after a literature review, Neo4j was chosen. The reasons for this shift will be explained in the design section.\nSecondarily, the team was tasked with coming up with three scenarios in which the researchers’ queries would fit. The scenarios that were chosen were the following: Single Participant query (give me all the information about person X), Comparative Study query (let me compare group x with group y), and Summarization query (let me see all the engagements that exist in the network).\nThese scenarios would be used to form an API that used Node.js. Queries executed within the API would be cached in a Mongo Database in order to save time. The API handles fetching data from the graph, conducting multiple queries to synthesize a comparative study, updating nodes in the graph, and creating nodes in the graph.\nThe developers migrated the data from a MySQL database to Neo4j. Then, the team created an API endpoint using Node.js that researchers can use to conduct simple repeatable queries of the database. The procedure for this is discussed in the user manual. Finally, the team created a front-end website to further streamline the steps they will have to take. The team delivered to the client a graph-based database, a user and developer manual, a graph interface, a graph querying interface (the API), logging capabilities, and a front-end.  The team learned lessons along the way, mostly relating to milestone scheduling and leaving a buffer of time for when things go wrong. They also learned how to work with Neo4j and Node.js.','[\"Neo4j\", \"Node.js\", \"Cypher\", \"RDF\", \"social network\", \"ontology\", \"graph database\", \"graph visualization\", \"Semantic Web\", \"Social Interactome\", \"subject, predicate, object\"]','Virginia Tech',9,'Neo4j',NULL,NULL,NULL,NULL,NULL),(181,'Website Psychology Diversity','This semester, Spring 2018, in CS 4624, Multimedia, Hypertext, and Information Access, our team pursued a project with the Virginia Tech Psychology Department and their Diversity Committee. We were tasked with enhancing the current Diversity and Inclusion webpages that are offered on the Psychology Department’s website. The clients desired for us to create new webpages that would enhance the recruitment of potential students and faculty, bring better awareness to the Department’s Diversity and Inclusion endeavors to current students and faculty, and allow users to easily and conveniently utilize the Department’s resources concerning Diversity and Inclusion.\nWe developed a new website for the Department that included eleven distinct pages with content related to Diversity and Inclusion. The information for the pages was gathered from the InclusiveVT Inclusion and Diversity Strategic Planning Guide [4], provided to us by our clients. We gathered a library of multimedia content for use on the website from the Department. We performed user walkthroughs and distributed a survey to members of the Department in order to gauge user satisfaction and areas requiring improvement. This survey also allowed users to submit additional multimedia content for our use.\nThe Department had also faced the issue of the ease of editing the current webpages that they are hosting. They wanted to be able to edit their webpages in an easier fashion, being able to upload news articles and new events that the Department was holding. They requested that we consider this as we approach the development of the new webpages.\nWe utilized WordPress to allow for a user-friendly environment for editing the webpages in the future. We made all of the clients administrators for the website so that they can make future changes and add other individuals to become administrators. We also created video tutorials that detail the step-by-step instructions to edit each portion of the website. These seven videos will allow for the clients and anyone with administrative access in the Department to be able to easily edit content without having to spend time learning the technology on their own.\nThey also were interested in updating their entire Psychology Department website, upgrading all of the current webpages to be modernized. If they found our design to be favorable to the current website’s design, then they would proceed to convert the entire website. This was a consideration for us as we proceeded, making sure to think about how the entire Department website could be converted and the effort that would involve for them or anyone who undertakes that task.','[\"Diversity\", \"Inclusion\", \"Website\", \"WordPress\", \"Psychology\", \"InclusiveVT\"]','Virginia Tech',9,'Diversity',NULL,NULL,NULL,NULL,NULL),(182,'Satellite Image Finder Parking Lot & Spots','Satellite imagery in recent years has drastically increased in both quality and quantity. Today, the problem is too much data. Map features such as roads, buildings, and other points of interest are mainly extracted manually, and we just don’t have enough humans to carry out this mundane task.\nSatellite imagery in recent years has drastically increased in both quality and quantity. Today, the problem is too much data. Map features such as roads, buildings, and other points of interest are mainly extracted manually, and we just don’t have enough humans to carry out this mundane task.\nThe goal of this project is to develop a tool that automates this process. Specifically, the focus of this project is to extract parking lots using Object Based Imagery Analysis. The final deliverable is a Python tool that uses Machine Learning algorithms to identify and extract parking lots from high resolution satellite imagery.\nThis project was divided into two main steps: labeling data and training an algorithm. For the first step, the project team gathered a large dataset of satellite imagery in the form of GeoTIFFs, used GDAL to convert these files into JPEG image files, and used labelImg to label the images. The labelling consisted of creating an XML layer corresponding to each GeoTIFF image, where the XML layer contained bounding boxes outlining each parking lot. With all of the training data labeled, the next step was training the algorithm. The project lead tried several different models for the learning algorithm, with the final model being based on Faster RCNN.\nAfter training, the project team tested the model and determined the accuracy was too low, so the team decided to obtain and label more images to improve it. Once the accuracy met the determined standards, a script was built that would take an input of a GeoTIFF image, convert this to a JPEG image, run the image on the model to detect any parking lots and output bounding boxes depicting those parking lots, and finally, convert these bounding boxes into a single GeoJSON file. The main use case of the application is quickly finding parking lots with relative accuracy in satellite imagery. The model can also be built upon to be improved or used in related tasks, for example detecting individual parking spots.\nThe project has managed to achieve the expected goals using labelImg and a Faster RCNN model. However, due to a limitation of labelImg, the model cannot detect parking lots that are not horizontal or vertical. The project team researched several methods to solve this problem but were not able to fully implement a suitable solution due to time and infrastructure constraints. The team has described all of its research in this final report so that those who want to improve on this project will have a good starting point.\nNote that there are some additional files that had to be uploaded onto Google Drive:https://drive.google.com/open?id=1istIOQqsQdw43Ty08KdoY64qBUUlI9D_andhttps://drive.google.com/open?id=1_EPq0hgRfSsLOPsXiVWV4d2Dz8yxmm4h','[\"Machine learning\", \"Imagery analysis\", \"Python\", \"Parking lots\", \"Jupyter Notebook\", \"Tensorflow\"]','Virginia Tech',9,'Machine learning',NULL,NULL,NULL,NULL,NULL),(183,'Twitter Equity Firm Value','We analyzed how a company\'s response on social media (Twitter) can affect their stock market value following a data breach. Given a list of all data breaches since 2006 we collected their stock value for 150 days before the data breach and 120 after. Using a Fama French Model we came up with an abnormality value that demonstrated how the stock would have performed if no data breach occurred. While doing this we simultaneously collected tweets from the companies and customers about the data breach. We wanted to compare the stock performance to things such as the number of replies from a company, customer tweet sentiment, and links tweeted by the company. The way we did all of this work was by building Python scripts for all of the functionalities. When scraping the tweets the user would just need to supply a CSV file with the company\'s Twitter handle and company name. The other Python scripts used, do things like compute the abnormality difference from the client\'s Fama French Model, scrub the stock data to only have the date range needed, compute tweet sentiment, and grab client profiles. Our conclusion was that companies need to make few but comprehensive announcement tweets to decrease reply tweets. This could keep the sentiment of client tweets positive. Lastly, companies need to focus on replying to customer tweets to also keep sentiment positive.','[\"Security\", \"Breach\", \"Twitter\", \"Equity\", \"Firm\", \"Analysis\", \"Python\", \"Sentiment\", \"Stock\", \"Data\"]','Virginia Tech',9,'Security',NULL,NULL,NULL,NULL,NULL),(184,'Neural Network Doc Summarization','This is the Neural Network Document Summarization project for the Multimedia, Hypertext, and Information Access (CS 4624) course at Virginia Tech in the 2018 Spring semester. The purpose of this project is to generate a summary from a long document through deep learning. As a result, the outcome of the project is expected to replace part of a human’s work.\nThe implementation of this project consists of four phases: data preprocessing, building models, training, and testing.\nIn the data preprocessing phase, the data set is separated into training set, validation set, and testing set, with the 3:1:1 ratio. In each data set, articles and abstracts are tokenized to tokens and then transformed to indexed documents.\nIn the building model phase, a sequence to sequence model is implemented by PyTorch to transform articles to abstracts. The sequence to sequence model contains an encoder and a decoder. Both are implemented as recurrent neural network models with long-short term memory unit. Additionally, the MLP attention model is applied to the decoder model to improve its performance.\nIn the training phase, the model iteratively loads data from the training set and learns from them. In each iteration, the model generates a summary according to the input document, and compares the generated summary with the real summary. The difference between them is represented by a loss value. According to the loss value, the model performs back propagation to improve its accuracy.\nIn the testing phase, the validation dataset and the testing dataset are used to test the accuracy of the trained model. The model generates the summary according to the input document. Then the similarity between the generated summary and the real human-produced summary are evaluated by PyRouge.\nThroughout the semester, all of the above tasks were completed. With the trained model, users can generate CNN/Daily Mail style highlights according to an input article.','[\"Deep learning (Machine learning)\", \"Natural Language Processing\", \"Text Summarization\", \"Recurrent Neural Network\", \"Sequence to sequence\"]','Virginia Tech',9,'Deep learning (Machine learning)',NULL,NULL,NULL,NULL,NULL),(185,'Autism Support Portal','The Autism Support Portal project involves the creation of a portal site that helps users find information they need about autism. The primary goal of the project is to help users quickly find credible information for their specific need. With the amount of information available online, it can be hard for those interested in autism to find information that is not only credible but useful and updated to reflect current research. The site needs to be easy to use both for the users and for the future administrators of the site. The site also needs to help guide people towards reliable resources while potentially exposing users to new resources. To ensure that our project meets the needs of our potential users, the project was divided into different phases involving data collection, research, design, and implementation.\nTo gather data for our project, we used resources such as the Virginia Tech Center for Autism Research and their connections, to send out anonymous surveys to some of our potential users. We asked several questions pertaining to their interests in the site, what they needed from the site, and what resources were useful to them. This data allowed us to implement a site as specific to the user needs as possible while also giving us other resources to collect credible information from. In addition, Dr. Scarpa provided a lot of other resources that allowed us to solve some of the needs of users, with other resources allowing this project to focus entirely on the implementation of our search engine and the guiding of our users towards effective answers, solutions, and resources. Upon entering the site, users have direct access to the search and are provided with search tips and external resources to help them.\nThe site is set up entirely usingWordPress.org. WordPress was chosen to be the CMS or content management system for the site because it is very easy to use and allows administrators to do a lot for the site without the need for extensive technical knowledge. The site needs to be very easy to modify and change after its initial set up so that those who work on it at the Virginia Tech Center for Autism Research can do so quickly. However, using solely WordPress and its plugins created a variety of new obstacles stemming from the different uses of different plugins. To save time and money, research needed to be done on several different plugins to find the ones that not only met the needs of the site but that were also affordable. Even with these obstacles, using WordPress not only allows for easier creation and maintenance, but also easy modification of the site if additional features are wanted or needed.\nThe design of the site allows users to find necessary information very quickly through alphabetically sorted lists that will expose the user to terms that may have been unknown previously. One of the problems with researching autism is asking the right questions. For example, a child with a special need such as autism needs an IEP or individualized education program, which requires a specific search for an IEP. When a user explores education information, the user also needs to be shown some specifics such as IEPs. This example also serves as an example of the need to have our site easily modifiable, as a change in law or name would require someone to change the resource in the site.\nUsing the data and implementation techniques discussed, the end result portal is composed of help and resource pages as well as a refined search that links questions to reliable answers. In addition, the site is designed such that any user without prior technical experience can use the site and adjust the sites that are searched and any other information within the site that is changed.','[\"Autism\", \"Autism Support Portal\", \"Virginia Tech Center for Autism Research\", \"VTCAR\"]','Virginia Tech',9,'Autism',NULL,NULL,NULL,NULL,NULL),(186,'CEED Phone Application','This project addresses a problem that is often faced by many current and prospective Center for Enhancement of Engineering Diversity (CEED) members and staff. Members may range from pre-college students (e.g., high school) to parents of students. CEED needs an easier way to communicate information about their programs to current and prospective members and their parents as well. Our solution to this problem is a cross-platform mobile application for an end user. In our application, a user can learn more about CEED, and familiarize themselves with all the services offered. Thereafter they can make an informed decision about their program choices, and also can reach out to CEED employees and fellow students with any questions that they might have. Key features that are included in the mobile app are: enabled push notifications, a forum that allows users to interact with one another, and the ability to view embedded content within the app (e.g., Google Calendar and videos).\nThe implementation is through a cloud platform for cross-platform app development known asAppery.io. This platform provides the ability to create databases and implement custom HTML/CSS/Javascript code for the front-end. Prospective CEED members and parents will be able to download this app from Google Play and/or the App Store once CEED finalizes approval of fees that are required.\nBelow is a list of all the functionalities implemented in our application:','[\"Mobile Phone Application\", \"CEED\", \"Center for Enhancement of Engineering Diversity\", \"Appery.io\", \"android application\", \"IOS application\"]','Virginia Tech',9,'Mobile Phone Application',NULL,NULL,NULL,NULL,NULL),(187,'Product Defect Mining','This project is focused on customer reviews on various product defects. The goal of the project is to use machine learning algorithms to train on sets of these customer reviews in order to be able to easily identify the different defect entities within an unseen review. The identification of these entities will be beneficial to customers, product manufacturers, and governments as it will shed light on the most common defects for a certain product, as well as common defects across a class of products. Additionally, it will bring to light common resolutions for defect symptoms, including both correct and incorrect resolutions. This project also aims to make contributions to the opinion mining research community.','[\"Product Defect\", \"Data Mining\", \"Opinion Mining\", \"Classifier Training\", \"Machine Learning\", \"Data Analytics\", \"Web Crawler\"]','Virginia Tech',8,'Product Defect',NULL,NULL,NULL,NULL,NULL),(188,'Human Potential Program for Professionals','This project describes the Human Potential Program for Professionals (HPPP) CS4624 Multimedia, Hypertext, and Information Access capstone project and its deliverables.  The goal of the HPPP project centers around assisting Dr. Anna Pittman by creating introductory video material for her HPPP program.  The HPPP program consists of eight one hour modules, each of which has an accompanying two to five minute introductory video provided by this capstone project.  These videos feature Dr. Anna Pittman giving a brief overview of the module and highlighting the main topics.\nAfter several meetings with Dr. Anna Pittman to discuss her vision for the introductory videos, a schedule was devised for filming.  Dr. Anna Pittman also wanted a logo for HPPP which our team provided.  Another aspect of the videos was the accompanying background music.  This music was original and used Cycling ‘74’s Max/MSP software to create the final three tracks used in the videos.  The raw footage was then edited within Apple’s iMovie software and combined the logo and original music.  The material was provided to Dr. Anna Pittman for review.  After receiving Dr. Anna Pittman’s comments, the team was able to address each concern and make adjustments.  This was an iterative process requiring the team to work very closely with Dr. Anna Pittman.','[\"Human Potential Program for Professionals\", \"HPPP\", \"HPPP Background Music\", \"HPPP Logo\", \"Dr. Anna Pittman\", \"Consciousness Psychology\", \"Behavioral Science\", \"Communication\", \"Compassion\", \"Emotional Intelligence\", \"Empathy\", \"Mindfulness\", \"Motivation\", \"Self-Awareness\", \"Self-Regulation\", \"Video\"]','Virginia Tech',8,'Human Potential Program for Professionals',NULL,NULL,NULL,NULL,NULL),(189,'Common Crawl Mining','The main goal behind the Common Crawl Mining system is to improve Eastman Chemical Company’s ability to use timely knowledge of public concerns to inform key business decisions. It provides information to Eastman Chemical Company that is valuable for consumer chemical product marketing and strategy development. Eastman desired a system that provides insight into the current chemical landscape. Information about trends and sentiment towards chemicals over time is beneficial to their marketing and strategy departments. They wanted to be able to drill down to a particular time period and look at what people were writing about certain keywords.\nThis project provides such information through a search interface. The interface accepts chemical names and search term keywords as input and responds with a list of web page records that match those keywords. Included within each record returned is the probable publication date of the page, a score relating the page to the given keywords, and the body text extracted from the page. Though it was one of the stretch goals of the project, the current iteration of the Common Crawl Mining system does not provide sentiment analysis. It would be relatively straightforward to extend the system to perform it, given the appropriate training data.\nThe final Common Crawl Mining system is a search engine implemented using Elasticsearch. Relevant records are identified by first analyzing Common Crawl for Web Archive (WARC) files that have a high frequency of records from interesting domains. Records with publication dates are then ingested into the search engine. Once the records have been indexed by Elasticsearch, users are able to execute searches which return a list of relevant records. Each record contains the URl, text, and publication date of the associated webpage.\nIncluded in this submission are Microsoft Office and PDF versions of the Common Crawl Mining project\'s final presentation and final report. The final presentation outlines the project\'s history. The final report outlines the progress made on the project and includes a developer\'s and user\'s manual for the system. This submission also includes a compressed folder which contains all of the source code associated with the Common Crawl Mining project.','[\"Common Crawl\", \"Elasticsearch\", \"Content Mining\", \"Eastman Chemical Company\"]','Virginia Tech',8,'Common Crawl',NULL,NULL,NULL,NULL,NULL),(190,'Identifying Drug Related Events from Social Media','The overall goal of the project was to establish an innovative information system, which can automatically detect and extract content related to side effect of drugs from user reviews, determine whether they are talking about effectiveness or adverse drug events, extract keywords or phrases related to effectiveness or adverse drug events, and visualize the resulting information to doctors and patients. Our group was provided with crawled Twitter reviews and social network forum reviews on drugs that are used to treat diabetes. The raw data were manually labeled in four different label for named entity recognition in order to create training, testing, and validation sets. Using the training data set, a side effect dictionary was created using PamTAT. Side effect dictionary was then refined by removing neutral words to increase accuracy. To validate the accuracy of the generated side effect dictionary, the results of side effect analysis based on the generated dictionary and two other general negative word dictionaries were compared. The generated side effect dictionary performed better in recognizing side effect entities. After validation, the generated dictionary was further tested with a set of user reviews on a drug that is used to treat stroke. Using generated dictionary, the project accomplished to accurately determine if any reviews relates to the mention of side effect of specific drugs. The project successfully delivered to accurately detect mention of side effect from the reviews in > 90% accuracy. Resulting algorithm can be used to create innovative information system to detect and extract content related to side effect of drugs for any other drugs with creation of problem specific dictionary. The project should be further developed to incorporate automatic extraction of user reviews, analysis of data, and visualization of results.','[\"Machine learning\", \"pamTat\", \"multimedia\", \"hypertext\", \"web crawling\", \"confusion matrix\", \"smoke list\", \"statistical learning\", \"statistical model\"]','Virginia Tech',8,'Machine learning',NULL,NULL,NULL,NULL,NULL),(191,'Pathways Web','Virginia Tech is updating its general education requirements, replacing the Curriculum for Liberal Education (CLE) with the Pathways Program. With this effort the Office of General Education is trying to improve the way that courses are publicized. The goal of this push is to allow students to make a more informed decision about which courses to select to fulfill their general education requirements. Our team worked with a team in the Office of General Education to revamp the current Pathways website by producing new content for courses. For each course, this included creating a course trailer, taking photographs of the course, writing synopses, and interviewing students about their experiences with the course. Additionally, we produced a series of more faculty oriented podcasts to provide some context for the program on the whole. This new content was added to an updated version of the website we created which at the time of writing is awaiting approval as a part of a larger Pathways web presence overhaul.','[\"video\", \"podcast\", \"website\", \"Pathways\", \"courses\"]','Virginia Tech',8,'video',NULL,NULL,NULL,NULL,NULL),(192,'Food Waster','Approximately 40% of all edible food is wasted each year, costing family approximately $1,500 a year. Consequently, we undertook a task for our client, Susan Chen, in an effort to combat this issue. Our client, currently a first year, graduate student at Virginia Tech pursuing a Master’s degree in Human Nutrition, Food, and Exercise, requested that we create an online-based, public service announcement tool to raise awareness.\nAfter several rounds of concept and design refinement, the solution was realized in the form of a website. The purpose of this website is to allow visitors to visualize the current and long term, extrapolated impacts to them and society from food wasted in just a single meal. Two videos were also created for this website to provide both an educational and entertaining experience while they learn more about wasted food in the United States.\nThe front end, i.e., webpage experienced by visiting users is ultimately an HTML document. It is also powered by JQuery to add a number of useful functionalities. One such function is an auto-complete feature so that users can dynamically see available options as they search for food types. On submitting their inputs, visitors will be shown a statistics page powered by D3.js, a JavaScript library for data-driven documents.\nNode.js is used on the server-side to provide the user input and statistics webpages. When a visitor submits the food types and amounts wasted to the server, the server queries the MySQL database for the appropriate data. The MySQL database is built on top of two datasets, one from a study by the USDA Agricultural Research Service and another from a separate study by the USDA Economic Research Service. This provides 7-year spanning, nationwide unit price averages for numerous food groups to calculate the desired statistics.\nCertainly, there were a few challenges that appeared during the project’s development. One was attempting to fuse two independently gathered datasets. Another was dealing with improper user inputs. However, as the issues were debated, they were eventually solved one by one.\nUltimately, the current product fulfills the required task and goal. Users can calculate their wasted food and from a single source, see the result of that and the impact on them and those around them. Nevertheless, the current richness of the data in the database and modernity of the webpage design means that there is still untapped potential for improvement for this product.','[\"Food Waste\", \"Food\", \"Public Service Announcement\", \"website\", \"statistics\", \"video\", \"infographics\"]','Virginia Tech',8,'Food Waste',NULL,NULL,NULL,NULL,NULL),(193,'Rdoc2vec CS4624 Project for Spring 2017','This submission includes deliverables for the capstone project Rdoc2vec. It was created by Jake Clark, Austin Cooke, Steven Rolph, and Stephen Sherrard for their client, Eastman Chemical Corporation. Doc2Vec is a machine learning model to create a vector space whose elements are words from a grouping or several groupings of text. By analyzing several documents, all of the words which occur in these documents are placed into the vector space. The distance between these vectors indicates how similar they are. Words which appear in similar contexts have a small distance between them in this vector space. This algorithm has been used by researchers for document analysis, primarily using the Gensim Python library.\nOur client, Eastman Chemical Corporation, would like to use this approach, when working in a language more suited to their business model.  A lot of their software is statistical, written in R. Thus, our job had the following components: become familiar with Doc2vec and R, develop Rdoc2vec, and apply it to parse documents, create a vector space, and make tests.\nFirst, to become familiar with the language, we spent a few weeks with tutorials including the Lynda library, which was provided by Virginia Tech. After we felt we were familiar with the language, we learned about two of the dominant algorithms used, called Distributed Bag-of-words (DBOW) and Distributed Memory (DM).  After learning these two algorithms, we felt that we were prepared to begin development.\nSecond, we developed a class structure similar to that of Gensim. Keeping this as a skeleton, we developed a parsing algorithm which would be used to train the model. The parser analyzes the documents and computes a frequency for the occurrence of each word. The parser itself takes a list of physical documents stored on the system and completes the analysis, passing the frequency of words along the pipeline.\nThe next step was to create a neural network for training the model. We elected to use the built-in neural network library written in R called nnet. A neural network takes an initial input vector as a parameter. For our purposes, it made sense to use a “one-hot” vector, which has only one input. This can cut down on later calculation because the input vector is only of size one. Then this input is multiplied by several weights to be put into a hidden layer, handled by the nnet library. The values in the hidden layer are multiplied again by several weights to go into the output layer. After creating functions which called the nnet library, we began work on testing. In the meantime, we decided to begin a design on our own implementation of a neural network. By creating a neural network anew, we get around the major problem with nnet, which is optimization. Since nnet is a black box that we cannot affect, we cannot be sure that it is optimized for our application. Since we use “one-hot” vectors, which are not a default application, it is likely that there is some way we can improve the speed in our library. We were not able to finish and test our neural net, so it is something left for future groups to work on.\nFinally, we began testing. We created a Web scraper which grabbed a number of articles from Wikipedia. We used this scraper to get a number of different documents. Specifically, we scraped information on the congressional districts of several states. This gave us document sets which can be quite large when using several states, or smaller by analyzing individual states. We performed tests on these datasets, the results of which we kept with our code.','[\"R\", \"Doc2Vec\", \"Machine Learning\", \"Software\", \"Open Source\"]','Virginia Tech',8,'R',NULL,NULL,NULL,NULL,NULL),(194,'SIRecommender Project','This project was a continuation of efforts conducted by the CS4624 team of Spring 2016. The previous team built a procedure to find the top k friends based on information submitted via surveys. This year’s project dealt with analyzing data received through these surveys in order to draw conclusions about different characteristics and demographics that are prominent between recovery buddies on a live social network, Friendica. This year’s team of students worked with data that had been collected from the social network, which contain information about articles read, modules liked, and other information that may be used to find commonalities and form a friendship.\nAfter parsing the information, we evaluated friendships and the homophily-based measures that the two people have in common. We analyzed them to find trends through the visualization of data (histograms) and a top-down approach. Our main focus was our top-down approach, in which we determined the similarity scores of two recovery buddies given their similarities in demographics. When we identified pertinent demographics, we calculated the probabilities of similarities so that we can statistically describe how friendships are driven by similarities on Friendica. This was part of our final deliverable. We also focused on diffusion. We analyzed the tendency of a user to attend a meeting, watch a video module, or complete other tasks because another recovery buddy did so. This helped us identify how the network experiences diffusion. We used diffusion to identify users that experience high or low amounts of interaction with other users and can identify their similarities through homophily-based measures.\nOne of this team’s focuses included different aspects of weighting different feature types. This mainly meant tuning parameters and observing the changes that those parameters produced. We needed to understand how to tune these parameters and how to improve the outcome. Another focus this semester was making predictions on friendships based on the answers submitted by participants through the surveys given. These findings gave the team insight to distinguish contact from homophily. The team gained a visual understanding of this information through histograms. Socio-economic status, gender, and number of addictive substances are key parts of homophily in this project that were visually observed.\nOur results were trends in our top-down and diffusion approaches. Top-down resulted in 55 close relationships with many of them being of the same gender and income level. Our diffusion results gave us the level of influence particular users had on each other. Our final deliverables consisted of documentation of these results and the code that went into finding them. Our top-down and diffusion results, along with our analysis on homophily are our main deliverables.','[\"Social Interactome\", \"SIRecommender\", \"Homophily\", \"Friendica\", \"addiction recovery\", \"clinical trial\", \"lattice network\", \"small-world network\", \"diffusion\"]','Virginia Tech',8,'Social Interactome',NULL,NULL,NULL,NULL,NULL),(195,'Global Event Crawler and Seed Generator for GETAR','Global Event and Trend Archive Research (GETAR) is a research project at Virginia Tech, studying the years from 1997 to 2020, which seeks to investigate and catalog events as they happen in support of future research. It will devise interactive and integrated digital library and archive systems coupled with linked and expert-curated web page and tweet collections. This historical record enables research on trends as history develops and captures valuable primary sources that would otherwise not be archived. An important capability of this project is the ability to predict which sources and stories will be most important in the future in order to prioritize those stories for archiving. It is in that space that our project will be most important.\nIn support of GETAR, this project will build a powerful tool to scrape the news to identify important global events. It will generate seeds that contain relevant information like a link, the topic, person, organization, source, etc. The seeds can then be used by others working on GETAR to collect webpages and tweets using tools like the Event Focused Crawler and Twitter Search. To achieve this goal, the Global Event Detector (GED) will crawl Reddit to determine possible important news stories. These stories will be grouped, and the top groupings will be displayed on a website as well as a display in Torgersen Hall.\nThis project will serve future research for the GETAR project, as well as those seeking real time updates on events currently trending.\nThe final deliverables discussed in this report includes code that scrapes Reddit and processes the data, and the webpage that visualizes the data.','[\"GETAR\", \"CS 4624\", \"Global Event Detector\", \"D3.js\", \"SNER\", \"NLTK\", \"Cluster\", \"News\", \"Reddit\", \"digital library\", \"webpage\", \"tweet\"]','Virginia Tech',8,'GETAR',NULL,NULL,NULL,NULL,NULL),(196,'VR4GETAR','Global Event and Trend Archive Research (GETAR) is supported by NSF (IIS-1619028 and 1619371) through 2019. It will devise interactive, integrated, digital library/archive systems coupled with linked and expert-curated webpage/tweet collections. This project will act as a supplement to GETAR by providing a Virtual Reality (VR) interface to visualize geospatial data and extrapolate meaningful information from it. It will primarily focus on visualizing tweets and images obtained from the GETAR data archive on a globe in a VR world. This will be accomplished using tools like Unity, HTC Vive, C# and Git. In order to ensure that the product meets the end user’s specification, this project will use an iterative workflow with a very short feedback loop. The feedback obtained from Dr. Fox and our team members will be used to make subsequent prototypes and the final product.\nOur project is intended to be used as a demo by school children interested in data analytics and data sciences. Additionally, this project can also be extended to add features to our end product.\nOur final product can display images and tweets on a globe in a VR world provided that they have location information. As part of our final deliverable, we delivered a report, a presentation, a video demo and a GitHub repository containing the source code for our project. During this project, our team learned that building a 3D application is very different from building a 2D desktop application. We also learned that it is crucial to meticulously document all parts of product development to assist future development.','[\"VR\", \"GETAR\", \"Data exploration\", \"Tweet\", \"Video\", \"Geolocation\", \"Unity\", \"HTC Vive\"]','Virginia Tech',8,'VR',NULL,NULL,NULL,NULL,NULL),(197,'Drug Discovery Website Redesign','The client, Karen Iannaccone, designed, developed, and maintained a website for the Virginia Tech Center for Drug Discovery. Virginia Polytechnic Institute and State University ceased the operations of a Percussion Content Management System in favor of a new system based on the Ensemble Content Management System. Karen Iannaccone does not have strong technical background and therefore requested a new website be created with an emphasis on three design goals:\n•	Maintainability\n•	Performance\n•	Usability\nWordPress, along with the Avada theme, allows for a responsive and intuitive site to be designed, developed, and maintained efficiently and inexpensively. Karen Iannaccone requested we develop a WordPress website to allow her to learn through tutorials in a much simpler setting. Virginia Tech Hosting set up a sandbox environment where the team could develop and refine the WordPress website while not affecting the old website created by the client.\nAs a team, we went through the content of the old website and utilized a web crawler to determine all linking throughout the website. We created a list of web pages that needed to be migrated, and split the list of pages amongst team members. After determining the workload, we designed the website to allow for a better user experience. We delivered an entire website consisting of informational pages, donation page, and image galleries to deliver all of the old content on the new website.\nBefore the WordPress was launched, the client critiqued the website and provided initial feedback on text, images, page layouts, and navigation, which was corrected, refined, and resubmitted to the client. Karen Iannaccone approved the site and it was launched March 30th.\nAfter the site’s launch, it was tested by third party users for acceptance and usability and byGTMetrix.comfor performance. Feedback from test participants has been consolidated and given to the client in order to address any further revisions that can be made. The performance metrics will be analyzed and the website’s performance will be improved.','[\"WordPress\", \"Avada\", \"CMS\", \"Drug Discovery\", \"Website\", \"Website Redesign\", \"Science\"]','Virginia Tech',8,'WordPress',NULL,NULL,NULL,NULL,NULL),(198,'Data Archive - Zenodo','The goal of this project was for students of the Multimedia, Hypertext, and Information Access course in the Data Archive group to work with their client, Professor Seungwon Yang, to create a maintainable Zenodo repository to manage multimedia and datasets collected for scientific research at LSU. Zenodo is an open source data repository created for managing research data.\nThe students have worked with the client and the LSU IT Department to get Zenodo and potentially DSpace (another open source research data repository) installed and running on the LSU server. This process involved configurations and dependencies detailed below in this report. The LSU server has a firewall which blocks access to pages hosted by the server to users outside of the local network. Because of these obstructions, the students determined it would be more feasible to work through the above tasks locally and leave the rest of the installation process to the LSU IT Department which has more experience with the server’s configurations and dependencies.\nThe students wrote a script to do both a single and a batch deposit of datasets through the Zenodo API. The datasets that will be included in the future batch deposits will be provided by the client and will be relevant to scientific research taking place at LSU. The script is generic and it accepts a separate, easily updated document that contains the datasets to be uploaded to facilitate the process for non-technical users to update the repository.\nFinally, the students have created a guide for the future Zenodo administrators detailing how to install and manage the repository and add datasets to the script to be automatically deposited as well as a guide for the users who will be using the datasets in Zenodo for their work.','[\"CS 4624 Multimedia/Hypertext/Information Access\", \"Zenodo\", \"Data Archive\", \"Docker\", \"DSpace\", \"datasets\", \"Python\", \"PostgreSQL\"]','Virginia Tech',8,'CS 4624 Multimedia/Hypertext/Information Access',NULL,NULL,NULL,NULL,NULL),(199,'Fusality for Stream and Field','This project provides users with a means to organize, graph, and analyze specific data recorded from Stroubles Creek. The website will be utilized by an Undergraduate Biological Systems Engineering class to help with their labs that deal with the health of Stroubles Creek.\nOur team was designated with the task of improving a website that was created by a past Computer Science capstone team. The website we started with was barely functional and could not yet be used by the Undergraduate Biological Systems Engineering class. The website required many modifications in both the front end interface as well as the backend. Our team split up into three two-person groups based off of skill and desired learning objectives. These teams include a backend team, a front end user-interface team, and a data graphing team.\nThe main front-end improvements that were enacted on the website include a complete overhaul of the entire user-interface and the addition of a usable navigation bar that enables users to easily use all features of the website.\nBackend improvements include major changes to the tables in the MySQL database as well as PHP functions that make utilizing the database extremely easy for the data graphing team. The changes made to the database tables allowed for a more straightforward representation of the data and enabled saving graphs for a specific experiment.\nMost of the improvements were on the data graphing aspect of the website. Users are now able to analyze six years of data collected from Stroubles Creek. They can analyze this data by creating either line graphs or scatter plots of whatever specific creek data they want. The graphs provide users with the ability to see trends in creek health over the course of many years.\nCurrently, the website is ready to be used by Undergraduate Biological Systems Engineering classes. It provides all the functionality that our client required and does so in a clean, easy-to-use manner.\nEven though the website is ready for use, there are still areas that can be improved upon. These areas include more graph options, easier ways to upload new data sets, graphing large amounts of data points, and the aesthetics of the graphs.','[\"Stroubles Creek\", \"Experiments\", \"website\", \"graph\", \"dataset\"]','Virginia Tech',8,'Stroubles Creek',NULL,NULL,NULL,NULL,NULL),(200,'Background Check for R4 OpSec, LLC','The main project deliverable was a website for R4 OpSec (r4opsec.com). The purpose of this website is to display information about the company’s services and be able to accept résumés for new hires. The company is owned by Joe Romagnoli and is based in Chantilly, VA. The company works in the field of background investigation checks for the federal, state, and local government, as well as the civilian sector. The background investigation process starts with a company or a government agency reaching out to independent companies that handle an investigation of a new hire to that company. A background investigation usually includes verifying identity, past employment, credit history, and criminal history. The process can take anywhere from a week to a month, depending on how quickly the company is able to verify a person’s information given what the person provides to the company (i.e., proof of past education, W2 forms, date of birth, etc.). The website has a home landing page that displays images and text. There is a section explaining what services the company provides. Another section to display a simple about-us description. Finally, there is a button that brings a user to another page to upload a résumé. There is an admin login page, too, where employees at R4 OpSec can view past submissions. An admin can download the résumé, delete the submission information, search past submissions, or mark submissions as “pending”, “accepted”, or “rejected”. The admin is also able to create new admin accounts, edit their email address, or change their password from the same screen. The client needed the website to be fully functional in about 90 days. The client did not have a basic design in mind. Though, the client did provide a basic website that we could reference for when we were thinking of designs for this website. In November, the client had purchased a year subscription fromGoDaddy.comto host his website. We did raise concerns we thought the client should know about when it comes to shared web hosting, which we shall discuss in the report (Section 3.2.5). Lastly, the client wanted to make sure that this project would be expandable, and in the future, other groups or employees of R4 OpSec would be able to build upon what we delivered.','[\"HTML\", \"JavaScript\", \"CSS\", \"PHP\", \"MySQL\", \"GoDaddy\"]','Virginia Tech',8,'HTML',NULL,NULL,NULL,NULL,NULL),(201,'Analyzing Microblog Feeds to Trade Stocks','The goal of this project is to leverage microblogging data about the stock market to predict price trends and execute trades based on these predictions. Predicting the price trends of stocks with microblogging data involves a complex opinion aggregation model. For this, we built upon previous research, specifically a paper called \"CrowdIQ\" submitted by a team consisting of some Virginia Tech faculty. This paper details a complicated method of aggregating an accurate opinion by modeling judge reliability and interdependence. Once the overall sentiment of the judges was deduced, we built trading strategies that take this information into account to execute trades.\nThe first step of the project was a sentiment analysis of posts on a microblogging site named StockTwits. These messages can contain a label indicating a bullish or bearish sentiment, which will help indicate a specific position to take on a given stock. However, most users choose not to use these labels on their StockTwits. A classification of these unlabeled tweets is required to autonomously utilize StockTwits to drive the proposed trading strategies.\nWith a working sentiment analysis model, we implemented the opinion aggregation model described by CrowdIQ. This can gauge an accurate market sentiment for a particular stock based on the collection of sentiments that are received from users on StockTwits.\nThe next step was the creation of a trading simulation platform, including a complete virtual portfolio management system and an API for retrieving historical and current stock data. These tools allow us to run quick and repeatable tests of our trading strategies on historical data. We can easily compare the performance of strategies by running them with the same historical data.\nAfter we had a viable testing environment setup, we implemented trading strategies. This required research and analysis of other attempts at similar uses of microblogging data on predicting stock returns. The testing environment was focused on a set of stocks that is consistent with those used in CrowdIQ. The implementation of the CrowdIQ strategy served as a baseline against which we compared our results.\nDevelopment of new trading strategies is an open-ended task that involved a process of trial and error. It is possible for a strategy to find success in 2014, but not perform quite as well in other years, because market climates can be fickle. To assess the dependence of the market climate on our strategy\'s success, we also tested against data for the year of 2015 and compared the performance.\nThe final deliverable is a viable trading simulation environment coupled with various trading strategies and an analysis of their performance in the years of 2014 and 2015. The analysis of each strategy\'s performance indicated that our sentiment-based strategies perform better than the index in bullish markets like that of 2014, but, when they encounter a bear market, they typically make poor trading decisions which result in a loss of value.','[\"crowd-sourcing\", \"sentiment analysis\", \"stock trading\", \"stock market\", \"microblog\", \"scala\", \"spark\", \"hbase\", \"opinion aggregation\"]','Virginia Tech',8,'crowd-sourcing',NULL,NULL,NULL,NULL,NULL),(202,'IDEAL Tweet Collection Categorization','The major goal of the Event Based Categorization of Tweet Collections project is to help the IDEAL team to access tweet collections easily. Categorizing over 1,000 collections will aid organization, browsing, searching, and other activities. The focus of this project is to categorize each collection. The original method for categorization was to use a taxonomy scheme, but that was refined to use a tag system. This way the users will be able to see all of the collections in an organized way.\nIn the original planning, in addition to the categorizing, we planned to implement a user interface, as an extension of the current table of collections, to make it more interactive and easier to browse.  The design for the interface also is described.','[\"IDEAL\", \"tweets\", \"tagging\", \"categorization\", \"collection\"]','Virginia Tech',7,'The project has been refined several times. First, the categorization scheme was refined. In the beginning, the categorization scheme was to use a taxonomy based on event types. Also the GUI was to change the original static table that shows all of the tweet collections; it would add a search bar and column ordering functions. Then the categorization scheme was changed to use a tag system. It would contain an event tag that describes the event type, a place tag that shows the place(s) that the event happened, and a date tag that displays the date the event occurred. After that there was also a refinement that changed back to use a taxonomy scheme again but with better GUI system that will show all of the categories. Clicking the category will filter the tables to only show the related tweet collections. After that there was final refinement that uses a tag system that uses the tags from above, but also contains a taxonomy-based scheme for each tag. During the final refinement, the project was shifted to focus on creating a categorization scheme and applying it to the data file.\n\nUploaded files include the Word and PDF of the final report, and the PowerPoint and PDF of the final presentation.',NULL,NULL,NULL,NULL,NULL),(203,'Microaggression Expressions Submission Site','According to Dr. Stephanie Adams, Department Head of Engineering Education at Virginia Tech, “The value of having diversity is that it brings multiple perspectives to solving problems. If everyone on the team looks the same, and has had the same experiences, then they can’t think about it from another person’s perspective. But when you bring people together with diverse backgrounds, races, genders, and sexual orientations, the group can approach a problem from a range of perspectives that makes for a much better solution.”\nA more diverse community at Virginia Tech brings multiple perspectives to solving problems. Creating a cultural climate that is supportive of everyone, can be difficult because of microaggressions. Microaggressions are the seemingly harmless snubs or insults communicated verbally or nonverbally that target an individual based on their group membership. Eliminating the occurrence of microaggressions maintains and promotes diversity at Virginia Tech. The College of Engineering is particularly homogeneous and is thus an excellent starting point for addressing microaggressions. The College consists of 85% male professors, and 15% female professors, of which 0.3% are American Indian, 21% Asian, 2% Black, 65% White, 6% Hispanic, 0.3% Multiracial, and 7% non-resident alien.\nTo bring about a widespread understanding of how microaggressions affect people, a website (http://microaggressions.cs.vt.edu/) was created that allows faculty members in the College of Engineering to anonymously share their personal experience with microaggressions. By submitting anonymous posts, faculty can freely express their concerns, which can then be viewed publicly. This helps bring to the surface the kinds of hurtful comments that repeatedly get dismissed in the work environment. It is important to be able to study these posts – uncovering hidden patterns, correlations, and other insights – in order to better understand the problem. That is why we have incorporated filtering and visualization tools like a word cloud, N-grams, and graphs to allow the user to better understand patterns underlying the problem. We also implemented a simple anonymity checker that analyzes the post upon submission, looking for keywords that could identify a person. This, combined with a panel that allows the administrator and moderators to manually approve and flag posts prior to publication, ensures anonymity and privacy.\nMaking faculty feel more included at Virginia Tech is the goal of this website. Our goal is for ALL faculty to feel accepted and included at Virginia Tech. We will accomplish this through our website by raising awareness of the damaging impact of microaggressions.','[\"microaggressions\", \"website\", \"engineering faculty\", \"PHP\", \"HTML\", \"CSS\", \"MySQL\", \"PHPMyAdmin\", \"JavaScript\"]','N/A',7,'The main objective of this project is to create a website so that those concerned with diversity related microaggressions can report, search, share, and study microaggressions. Josh Iorio, the principal faculty in the Myers-Lawson School of Construction, will serve as the client for this project. He will communicate with the College of Engineering to help them understand the effects of microaggressions on its targets and on the organizational climate of the department, helping ensure that the new website is publicized and supported.\n\nFiles uploaded include the final report (Word, PDF), final presentation (PowerPoint, PDF), poster shown for VTURCS (with Word version of abstract and PDF of poster itself), and a Zip archive of the website.\n\nMicroaggressions are the everyday verbal, nonverbal, and environmental slights, snubs, or insults, whether intentional or unintentional, that communicate hostile, derogatory, or negative messages to target persons based solely upon their marginalized group membership” (UCLA, 10). The scope of our project currently consists of the faculty members in the Virginia Tech’s College of Engineering.\n\nThe website will be non-interactive that has anonymity as its key feature. Users should be able to post anonymously to the website. Any information that could reveal their identity must be flagged. We will store all the posts in our database table. We will also dynamically collect and related to microaggression. The website will enable the study and analysis of microaggression based on gender, ethnicity, and department.\n\nThe core functions include submitted, storing, viewing, and searching for posts. Advanced functions include collecting tweets, creating bigrams/trigrams, creating tag clouds, and sharing the website’s content on Facebook. We will use Foundation front-end framework which is a collection of HTML, CSS, and JavaScript to build upon the website. The back-end of the project will use PHP and MySQL to store, manage, and search submitted posts and PHP and MySQL to manage tweets. We will use an instant-on approach for implementation. The site will be deployed on a server maintained by the computer science department.  This machine is a virtual machine with 2 cores, 4GB RAM, and 500GB storage running 64-bit CentOS 6.7.  Apache, MySQL, PHP, FTP software, and PHPMyAdmin are installed.\n\nWe will use the Twitter API to collect tweets. We will search for specific hashtags (e.g. @microaggressive, @racist @sexist) that have content that\'s often related to microaggressions. Having anonymity as the key feature, we will not be asking any contact information from our users on the submission page. The post will then be reviewed by the moderator staff for acceptability. Specifically chosen faculty members will serve as the moderators, as decided by Josh Iorio. The exact number is not yet identified but we are assuming the number of moderators to be from 7 to 12. Since the site is public, much of the information submitted will not necessarily be disclosed, provided they don’t reveal the identity of any individual or place. We will not have any non-disclosure agreement. \n\nWe will also consider the implementation of the non-functional requirements. We will make our website compatible with different screen resolutions on different types of devices, such as laptop, smartphones, and tablets. We will device a way to deal with malicious post by using reCAPTCHA. We will aim to provide a better quality of service by publishing posts to the website within 24 hours. Since we are assuming all the posts to be real-life events, we will not be checking the truthfulness of the post.\n\nFiles included:\nFinal Project Report – includes everything you need to know about the website as well as how it was implemented and useful information for developers wanting to continue or expand the project\nWebsite (zipped file) – includes all the website files and resources for both the main website and administration panel. Note that this does not include a backup of the database.\nPresentation – includes a brief overview of the project and its features\nVTURCS Poster and Abstract – the poster and abstract used to present the project during VTURCS Spring Symposium 2016',NULL,NULL,NULL,NULL,NULL),(204,'Multimedia Database','We studied the entire process for the creation of a website to interact with a multimedia database.  Our team intended to create a centralized system so both undergraduate and graduate Computer Science students at Virginia Tech could upload their personal projects for all instructors who are affiliated with the university to view.  The overall goal of this project was to create a system to allow a user a way to view other user\'s profiles in order to facilitate the ability to find research candidates and opportunities.  Our team hopes that this proof-of-concept project can have a greater impact in the future by being expanded to include all university students.  However, since our goal was to create a prototype, we have limited the number of students who have access to the system to Computer Science students.  In order to create a successful prototype for our client, we had to first, through careful discussion, create the requirements for both the database and website.  Next, came the design phase which outlines the prototype’s technologies and overall structure.  In the implementation and prototype sections of the report, we detail aspects of the prototype’s capabilities.  Finally, in the testing section, our team discusses metrics deployed to determine the successfulness of the prototype.  Furthermore, we discuss the future plans for the project once we have graduated and the valuable lessons our team learned from this experience.  The current status of the project is that our team has fully built the back end database.  In addition, the webpages for the website have also all been created.  The main remaining task is that the website is only partially integrated with the database and needs to be fully integrated.','[\"Multimedia\", \"Database\", \"MySQL\"]','N/A',7,'The images uploaded are a variety of screen dumps, wireframes, tests, and architecture diagrams.\nThe report is provided in Word and PDF; the presentation is provided in PowerPoint and PDF.',NULL,NULL,NULL,NULL,NULL),(205,'Social Interactome Recommender Project','Our team is working with the Social Interactome team to assist in coding the recommender functionality for the Social Interactome network. That is supported by a website and system (modified version of Friendica) designed by the Social Interactome team to help recovering addicts. The team used Python to parse participant’s answers to survey questions, and applied an algorithm to that data to show each participant\'s most favorable friend matches. The team is working in concert with Prashant Chandrasekar, a Graduate Research Assistant (GRA). He provided us access to the participant’s answers to survey questions. As more and more surveys are filled out by users the team will continue to refine their algorithm to accommodate that extra data. As a further step we will work towards a hybrid recommender which will incorporate not only content, but also collaborative-based recommending.','[\"Friendica\", \"SIRecommender\", \"Similarity Matrix\", \"Homophily\", \"Social Interactome\", \"recommender\", \"Addiction Recovery Research Center\"]','N/A',7,'Contains a Word document and PDF of the final report, and the Powerpoint and PDF for our final presentation.',NULL,NULL,NULL,NULL,NULL),(206,'Micro-Aggression Video Vignettes','The goal of our project is to construct video vignettes of scenarios illustrating different types of micro-aggressions. Micro-aggressions are the everyday verbal, nonverbal, and environmental slights, snubs, or insults, whether intentional or unintentional, that communicate hostile, derogatory, or negative messages to target persons based solely upon their marginalized group membership (from Diversity in the Classroom, UCLA Diversity & Faculty Development, 2014).\nInteractions and conversations between peers and faculty are never-ending. The biggest concern related to micro-aggression is that individuals may not even know that they are committing a micro-aggression, which is why we want to inform as many individuals as we can about this topic. In fact, a micro-aggression can even occur when someone is giving a compliment. By raising micro-aggression awareness we can have safer, more alert and more intelligent interactions.\nWe created videos displaying different types of micro-aggression events. We have completed shooting and editing of three videos, each with length in the 1-3 minute range. The editing was done in the Innovation Space Center in Torgersen Hall, and the resulting videos are available through YouTube.  We have also completed preliminary work on the fourth video. The raw files for the videos are located in the Innovation Space in the \"Save Work Here\" folder under the names \"Sharma\" and \"Patha\". With these videos the overall goal is to garner attention and awareness regarding micro-aggressions that take place on a day-to-day basis. We hope that our videos can be a stepping-stone to finding a solution to an everyday problem, possibly inspiring others to produce additional videos on this important topic.','[\"Micro-Aggression\", \"Video Vignettes\"]','N/A',7,'The submission includes a Word and PDF version of the final report and a Powerpoint and PDF version of the final presentation.',NULL,NULL,NULL,NULL,NULL),(207,'Ebola RDF Database Validator','In 2014, the Ebola virus, a deadly disease with a fatality rate of about 50 percent, spread throughout several countries. This marked the largest outbreak of the Ebola virus ever recorded. Our client is gathering data on the Ebola virus to create the largest database of information ever made for this disease. The purpose of our project is to verify our client’s data, to ensure the integrity and accuracy of the database.\nThe main requirements for this project are to locate multiple sources of data to be used for verification, to parse and standardize multiple types of sources to verify the data in our client’s database, and to deliver the results to our client in an easy-to-interpret manner. Additionally, a key requirement is to provide the client with a generic script that can be run to validate any data in the database, given a CSV file. This will allow our client to continue validating data in the future.\nThe design for this project revolves around two major elements: the structure of the existing database of Ebola information and the structure of the incoming validation files. The existing database is structured in RDF format with Turtle7 syntax, meaning it uses relational syntax to connect various data values. The incoming data format is in CSV format, which is what most Ebola data is stored as. Our design revolves around normalizing the incoming validation source data with the database content, so that the two datasets can be properly compared.\nAfter standardizing the datasets, data can be compared directly. The project encountered several challenges in this domain, ranging from data incompatibility to inconsistent formatting on the side of the database. Data incompatibility can be seen clearly when the validation data matches the date range of the database data, but the exact days of data collection vary slightly. Inconsistent formatting is often seen in naming conventions for the data and the way that dates are stored in the database (i.e., 9/8/2014 vs. 2014-09-08). These issues were the main hindrance in our project. Each was addressed before the project could be considered complete.\nAfter all data was converted, standardized, and compared, the results were produced and formatted in a CSV file to be given to our client. The results are given individually, for each time the script is run, so if the user runs the script for 4 different datasets over 4 different sessions, there will be 4 different result files.\nThe second main goal of our project, to produce a generic script that allows the user to validate data on his own, uses all previously mentioned design elements such as parsing RDF and CSV files, standardization of data, and printing results to a CSV file. This script builds a GUI interface on top of these design elements, providing a validation tool that the users can employ on their own.','[\"database\", \"validation\", \"GUI\", \"dataset\", \"RDF\", \"Ebola\", \"Python\"]','N/A',7,'The contents of this submission include the script which validates the VT Ebola database and the resources needed by the script, the readme for the end user of the script, the presentation slides used for our final presentation on our project, and our project report.',NULL,NULL,NULL,NULL,NULL),(208,'New River Valley (NRV) Time Bank Development Report','TimeBanks is an international non-profit organization whose goal is to foster strong community relationships and an economy based on home, family, and community instead of money.  The primary goal of the system developed for the NRV Time Bank is to provide a more usable interface than is provided by established time bank management software.  This report and accompanying presentation detail the requirements, design, prototyping, implementation, and testing processes involved in the development of the software used by NRV Time Bank of Blacksburg, VA as an interface to their services.','[\"TimeBanks\", \"Wordpress\", \"Usability\", \"Web Development\", \"Iterative Design\"]','N/A',7,'The website portion of the software system developed for NRV Time Bank provides users with the means to initiate service exchanges with other users, using hours as currency.  Users earn hours by performing jobs, and may spend them by requesting the services of other users.  PHP routines are included, and described on p. 38-39 of the report.',NULL,NULL,NULL,NULL,NULL),(209,'Virginia Tech Center for Autism Research Website Renovation','The main goal of this project was to improve the web presence of the Virginia Tech Center for Autism Research (VTCAR).  With that intention, a team of 3 Computer Science undergraduate students from Virginia Tech was put in contact with Dr. Julee Farley, who was the client for this project. Dr. Farley is a member of the administration at the VTCAR and worked as the research coordinator. At the time, she maintained the center’s old website by periodically updating events and research opportunities. After seeing the websites of other organizations and comparing them to the center’s old website, Dr. Farley wanted the center’s website to be redesigned.  She wanted a website that was better organized, easier to maintain, and more aesthetically appealing than their current layout. For example, the new site was to provide information to parents who are looking to have an assessment conducted, donors who wish to give aid or volunteer, and trainees who seek educational materials. Furthermore, the overall layout and design of the website was to be attractive and modern, but also functional and usable. The site was to demonstrate how active the VTCAR is in the community through attractive pages and pictures while also showcasing the personnel that work at the center with descriptive, biographical pages.  Ultimately, in pursuit of these enhancements, the team gathered requirements, designed a completely new website, prototyped the designs, and performed usability and performance testing on such prototypes.  All of this was done before a final version was implemented, which was then accepted by the client.  The finished product has been delivered and is accessible at:www.vtcar.science.vt.edu. The report describes the documentation, processes, and results related to this effort.','[\"website\", \"ensemble\", \"CMS\", \"renovation\", \"Virginia Tech Center for Autism Research\", \"VTCAR\", \"CS4624\", \"content management system\", \"website development\", \"Ensemble CMS\", \"Autism\", \"formative evaluation\", \"usability study\"]','N/A',7,'A collection containing the final report and presentation of the team that renovated and redesigned the Virginia Tech Center for Autism Research\'s website in the Spring of 2016.  The report and presentation both contain an overview (at different levels of detail) of the work that the team completed.  The report also contains guides to assist users and developers of the website in the future.  Finally, this collection includes a .zip file that contains the photographic media that was used while developing the website.',NULL,NULL,NULL,NULL,NULL),(210,'3-Dimensional Weather Visualisation','Project deliverables are provided, including a detailed description of the creation of a polling and parsing system for keeping track of severe weather warnings, as delivered by the National Weather Service, and an interface to allow the user to view a representation of Doppler radar data in three dimensions. The report describes the roles of the team members, the work accomplished over the Spring 2016 semester, and the methods by which the team accomplished this work.','[\"Weather\", \"Doppler\", \"ICAT\", \"The Cube\", \"C#\", \"Unity\", \"Regular Expressions\", \"Regular Expression\", \"Regex\", \"Virtual Reality\", \"Augmented Reality\", \"Severe Weather Statement\", \"Weather Warning\", \"Warning\"]','N/A',7,'Senior Design Project presentation and report for CS 4624 Multimedia/Hypertext Capstone. Two Zip archives also are provided, with software for each of the front and back end parts of the system.',NULL,NULL,NULL,NULL,NULL),(211,'SafeRoad','Described is a project on the development of the SafeRoad software application; the report provides a reference for future work. This project was completed as a capstone requirement for CS 4624 (Multimedia, Hypertext, and Information Access) at Virginia Tech, guided by the client.\nSafeRoad is a software application designed to to analyze the NHTSA vehicle complaint database, determine the most common complaints, and predict recalls based on these complaints. The goal of the application is to make our vehicles and roads safer and prevent the loss of life.\nThe software has been developed to be used by data analysts for automotive manufacturers and governing agencies such as the NHTSA. The software can be run either preemptively or in response to a complaint or series of complaints regarding an automobile or one of its components. The results of the program can lead to the issuing of a recall before more serious consequences occur.\nThe project has been developed using Java, database connectivity, and machine learning algorithms. A classifier training set has been created and included with the source code. The final product has proven to predict recalls with an accuracy level that is significantly higher than what was required.','[\"Machine learning\", \"Natural Language Processing\", \"Database\", \"NHTSA\", \"vehicle complaint database\", \"automobile recalls\", \"automotive manufacturers\", \"classifier training set\", \"Naive Bayes\"]','N/A',7,'SmartRoad is developed in Java. The NHTSA complaint and recalls databases are imported into a MySQL database. The Java application connects to the MySQL databases using the Java Database Connection (JDBC). Using the JDBC, the Java application sends SQL queries to the databases and determines the most commonly complained about vehicle makes, models, and components. Once these attributes are determined, the most common complaints are compiled into a comma separated value called \"CommonComplaints.csv\". Upon compilation to the file, all text values are converted to a unique corresponding numeric value for compatibility with the classifiers, and all complaints are initially unclassified. A Hash Map is used to map text to numeric values and back.\n\nOnce compiled, a classifier is instantiated and is built on the already classified training set. The classifier that is used is the Naive Bayes classifier from the Java Machine Learning (JML) Library. Upon building, the classifier learns what patterns and attributes contribute to a recall. Once built, the classifier then works on \"CommonComplaints.csv\" and classifies each complaint in the file. All complaints that are classified as a recall are then compiled to \"PredictedRecalls.csv\" and numerical values are converted back to text for ease of reading. Comma separated value format is used for its exceptional organization and compatibility. \n\nAlso included in this submission is the final report for the SafeRoad project. Submissions have been made in both PDF and Microsoft Word Document format. The final report contains a more detailed summary of the project as well as a user manual and developers manual. Documentation of the development process, usage instructions, and development information are all available in the report. PowerPoint and PDF files are provided for the final project presentation.',NULL,NULL,NULL,NULL,NULL),(212,'PIAT - Poison Ivy Appalachian Trail Mega-Transect Data Collection Application','Provided are details and specifications for the Poison Ivy Appalachian Trail Mega-Transect Data Collection Application, referred to as PIAT. Described in the report are the software requirements, design, implementation, prototype, refinement, and testing for this application. Additionally, two manuals are included, for the users and developers, so they will be able to interact with and properly use this application as well as understand how it works and how it can be maintained or extended.\nThis project is a collaboration between Virginia Tech undergraduate students and Dr. John Jelesko, an associate professor working in Virginia Tech’s Department of Plant Pathology, Physiology, and Weed Science. The initial details and expected goals arose from meetings with Dr. Jelesko. The primary objective of PIAT is so he and his team can track the growth of poison ivy along the Appalachian Trail. Users of the application can make entries about poison ivy’s presence or absence as they see it along the trail. PIAT has a local database on the phone that can be synced with a larger database. This will allow for multiple users to track data and upload it to the main server. An external device that pairs with the user’s Android device allows them to log entries without interacting with the phone itself.','[\"Poison Ivy\", \"PIAT\", \"Appalachian Trail\"]','N/A',7,'The report provides a user and developer manual along with references. There is also a presentation provided that reflects some of the material presented in the report. The SQL file provided is the code required to create the table needed for the PIAT app. The APK file is the Android app that actually gets installed on the device. Lastly, the ZIP file contains all the source code for the application.',NULL,NULL,NULL,NULL,NULL),(213,'ICAT North Cross Exhibit','For eight days in mid-March, Virginia Tech\'s Institute for Creativity, Arts, and Technology (ICAT) held a camp for two hundred children from age 3 to the 5th grade at North Cross School in Roanoke, Virginia. The purpose of the camp was to engage children in a creative process with the use informal learning. Our group documented the camp, providing an impression of what happened over those eight days. On May 2, 2016, at the annual ICAT Day we presented an exhibit of our documentation. We have contributed to a portion of a much larger exhibit, containing other student projects and demonstrations by the ICAT staff, of informal learning methods they have used and taught community teachers to use.\nWe documented certain parts of the informal learning process using two methods. First, we filmed 360 degree video of both indoor and outdoor workshops. We have displayed the video using an Oculus Rift so that during ICAT Day a visitor to the exhibit is able to interactively view the children in action. Our second method of documentation is a set of audio reflections from students who participated in the workshops. Towards the end of the camp we had children reflect and then speak about their experience, creating art based on the Rudyard Kipling poem “If.” During ICAT Day, visitors connected with the children’s camp experience by browsing these experiences digitally using their smartphone or similar device.','[\"ICAT\", \"North Cross\", \"360 Degree Video\", \"CS4624\", \"Multimedia\"]','N/A',7,'Files uploaded:\n\nProject Report (Word)\nProject Report (PDF)\nProject Presentation (PowerPoint)\nProject Presentation (PDF)\nZip file of 360 Video Unity Project\nZip file of Web Application\nZip file of Oculus Rift Windows Executables\nThe 360 Video File (MP4)',NULL,NULL,NULL,NULL,NULL),(214,'Breathe-EZ','The breathalyzer application is part of an ongoing research project Mikhail Koffarnus is pursuing as a research professor at the Addiction Recovery Research Center (ARRC). Participants, which may number in the hundreds, will participate in an alcohol addiction recovery program which includes random breathalyzer tests. For each test a participant passes, they will be monetarily rewarded. The amount they are rewarded will increase with each successive passed test. The hope is that continued clean tests incentivized by monetary rewards will aid and motivate users on their road to recovery.\nIn order for the ARRC to be able to identify participants, the application must take pictures. The camera is set to take three photos of the participant as they use the BACtrack breathalyzer. By taking these pictures during the measurement process, the application ensures that the BACtrack device will be in the picture with the user. The application will then store the picture that it finds has the highest confidence of face detection as determined by an algorithm. This picture is important because it will help the ARRC confirm the user’s identity, keeping users from easily exploiting the system by having a sober friend blow for them.','[\"Android\", \"ARRC\", \"CS 4624\", \"BACtrack\", \"Breathe-EZ\", \"NIH\", \"National Institutes of Health\"]','N/A',7,'The submission includes the source code for Breathe-EZ, our Final Report and our Final Presentation. \nInformation regarding the structure and content of the zip file can be found in section 3.1 of the Final Report.',NULL,NULL,NULL,NULL,NULL),(215,'SIMobile','Social Interactome (SI) is a research project through the Addiction Recovery Research Center (AARC) at the Virginia Tech/Carilion Research Institute. This center and institute are located in Roanoke, Virginia. The idea is to have an anonymous, online social networking support system that can be accessed by participants recovering from addictions. This system started with a desktop Web application based on the Friendica API, tailored to support research on best methods for providing support. It has a minimalistic design that provides appropriate functionality. This project follows the process of using responsive web-development in order to make the system mobile-friendly.','[\"mobile responsive web development\", \"Social Interactome\", \"Addiction Recovery Research Center\", \"Virginia Tech Carilion Research Institute\"]','N/A',7,'PowerPoint and PDF versions of the final presentation, and Word and PDF versions of the report are included.',NULL,NULL,NULL,NULL,NULL),(216,'Max Video Tutorials','In MUS 3065, Computer Music and Multimedia, students learn how to use the Max programming environment in order to compose interactive digital music.  This project aims to assist these students by making video tutorials about the more useful aspects of Max in order to reiterate core concepts and methods that Dr. Nichols teaches in class. The selected topics for the videos are coll object, additive synthesis, audio modulation, quad-speaker spatialization, timing in Max, and Max basics. Each video describes the recorded screen of the Max environment as well as voice-overs explaining what the narrator is doing in the video and why.','[\"Max\", \"computer music\", \"music\", \"MIDI\", \"audio\", \"signal processing\", \"video tutorial\"]','N/A',7,'Files included in this submission are the final paper (both Word and PDF versions), final presentation (both PPT and PDF versions), and final video tutorials (in .MOV format).',NULL,NULL,NULL,NULL,NULL),(217,'Searchable IDEAL Climate Change Collections','The IDEAL Climate Change submission to VTechWorks contains five types of files: Final Project Report, Presentation Powerpoint, and Python Scripts for extraction of URL and indexing (two files) to SOLR.\nThe Final Project Report has two formats: Word document and PDF. It includes all work done for the project including seven chapters on the following topics: User\'s Manual, Design & Requirements, Developer\'s Manual, Prototype & Refinement, Testing, Future Work, and Lessons Learned. In addition to these chapters, the report also includes tables, figures, acknowledgements, and bibliography. It is organized well and contains tables of contents, tables, and figures.\nThe Presentation has two formats: PowerPoint and PDF. It includes the objective of the project, high level system map, functionality descriptions with screenshots of features, and description of testing and future work. The purpose of his document is to present the overall progress and accomplishments of the project to interested parties. It contains more visual aids and figures than text for a better cognitive understanding and interest to the audience.\nThe first Python script is used to extract URLs from the raw tweet data. It is well commented to help future developers and parties interested in further developing the project. The second and third Python scripts are used to index tweets and webpages to SOLR; they\'re also well commented for readability.','[\"IDEAL\", \"Climate Change\", \"SOLR\"]','N/A',7,'This submission includes: 1) Project Final Report, 2) Project Final PowerPoint Presentation, and 3) Three Python Scripts',NULL,NULL,NULL,NULL,NULL),(218,'Cinemacraft: Virtual Minecraft Presence Using OPERAcraft','Cinemacraft is an interactive system built off of a Minecraft modification developed at Virginia Tech, OPERAcraft. The adapted system allows users to view their mirror image, as captured by Kinect sensors, in the form of a Minecraft avatar. OPERAcraft, the foundation of the project, was designed to engage K-12 students by allowing users to create and perform virtual operas in Minecraft. With the advanced functionality of Cinemacraft, the reinvented system aims to alter the perspective of how real-time productions will be produced, filmed, and viewed.\nThe system uses Kinect motion-sensing devices that track user movement and extract the data associated with it. The processed data is then sent through middleware, Pd-L2Ork, to Cinemacraft, where it is translated into avatar movement to be displayed on the screen, resulting in a realistic reflection of the user in the form of an avatar in the Minecraft world.\nWithin the display limitations presented by Minecraft, the avatar can replicate the user’s skeletal and facial movements; movements involving minor extremities like hands or feet cannot be recreated because Minecraft avatars do not have elbows, knees, ankles, or wrists. For the skeletal movements, three dimensional points are retrieved from the Kinect device that relate to specific joints of the user and are converted into three dimensional vectors. Using geometry, the angles of movement around each axis (X, Y, and Z) for each body region (arms, legs, etc.) are determined. The facial expressions are computed by mapping eyebrow and mouth movements within certain thresholds to specific facial expressions (mouth smiling, mouth frowning, eyebrows furrowed, etc.).','[\"Institute for Creativity Arts and Technology\", \"OPERAcraft\", \"Cinemacraft\", \"Minecraft\", \"Motion Sensing\", \"Kinect\", \"ICAT\", \"Computer Science\"]','N/A',7,'Uploaded files include the Word and PDF versions of the final report, and the PowerPoint and PDF versions of both the final presentation and the poster shown for VTURCS. In addition there are files covering the Kinect implementation (both facial and skeletal programs) of Cinemacraft.',NULL,NULL,NULL,NULL,NULL),(219,'English Wikipedia on Hadoop Cluster','To develop and test big data software, one thing that is required is a big dataset. The full English Wikipedia dataset would serve well for testing and benchmarking purposes. Loading this dataset onto a system, such as an Apache Hadoop cluster, and indexing it into Apache Solr, would allow researchers and developers at Virginia Tech to benchmark configurations and big data analytics software. This project is on importing the full English Wikipedia into an Apache Hadoop cluster and indexing it by Apache Solr, so that it can be searched.\nA prototype was designed and implemented. A small subset of the Wikipedia data was unpacked and imported into Apache Hadoop’s HDFS. The entire Wikipedia Dataset was also downloaded onto a Hadoop Cluster at Virginia Tech. A portion of the dataset was converted from XML to Avro and imported into HDFS on the cluster.\nFuture work would be to finish unpacking the full dataset and repeat the steps carried out with the prototype system, for all of WIkipedia. Unpacking the remaining data, converting it to Avro, and importing it into HDFS can be done with minimal adjustments to the script written for this job. Continuously run, this job would take an estimated 30 hours to complete.','[\"Wikipedia\", \"Hadoop Cluster\", \"Solr\", \"XML\", \"Avro\", \"Apache\"]','N/A',7,'CS 4624 Multimedia/Hypertext/Information Retrieval Final Project\n\nFiles submitted:\nCS4624WikipediaHadoopReport.docx - Final Report in DOCX\nCS4624WikipediaHadoopReport.pdf- Final Report in PDF\nCS4624WikipediaHadoopPresentation.pptx - Final Presentation in PPTX\nCS4624WikipediaHadoopPresentation.pdf - Final Presentation in PDF\nwikipedia_hadoop.zip - Project files and data',NULL,NULL,NULL,NULL,NULL),(220,'IDEALvr Word Cloud: IDEAL Data Visualization using Virtual Reality','This report (including code) provides an explanation of the implementation of the IDEALvr application and an analysis of the methodology to create the system.  The methods of implementation include 3D virtualization onto an Android application using the game engine Unity and Google Cardboard 3D virtualizer.  The methods of analysis include gathering metadata from the IDEAL database and creating new methods of analysis of pre-collected datasets.  Other analysis includes understanding the different limitations and advantages of using Google Cardboard and Android to develop the user experience.  Results of the analysis show that the data can be interpreted more interactively using virtual reality.\nThe goal of this project was to provide an easy to use interface that allows researchers to visualize collections in a way that increases understanding and allows them to draw conclusions from the data without having to read through massive collections of tweets. We created a hierarchical structure that allows users to filter collections by categories. We developed a frequent word analyzer to parse through the collections and generate a list of the most frequent words in a collection which is also visualized in the project. The virtual reality user interface allows for full immersion and interaction with as many IDEAL collections as the user desires.\nThe IDEALvr project was created with Unity and developed to work on Android devices that are supported by Google Cardboard. Work on a web application was attempted and issues are documented in the included files. Future work and plans are also included in the attached reports.\nOverall, the IDEALvr project was very successful. It provides a way to visualize IDEAL collections in a way that allows for meaningful analysis and conclusions to be made.','[\"IDEAL\", \"Twitter\", \"Virtual Reality\", \"VR\", \"Data\", \"Word Cloud\", \"Android\", \"Unity\", \"Cardboard\", \"Google Cardboard\", \"Google\"]','N/A',7,'Included in this submission are a zip archive of the code and IDEAL data samples, a project report outlining how to run the application and details on how the application was built in PDF form and editable text form, and a project presentation in PDF and PowerPoint forms.',NULL,NULL,NULL,NULL,NULL),(221,'CS 4624 Simplysent Final Report','SimplySent is an easy way for busy professionals to strengthen their business relationships by sending thoughtful gifts in less time and with less hassle. We connect to various contact managers (salesforce, sugarcrm, highrise and gmail) so people can use their existing contacts in these systems when sending gifts to one or dozens of people. The site also has a dashboard feed of Calendar events, LinkedIn updates and CRM leads/deal closings so you never forget an important event for people in your network. Based on these updates, you can send a gift today, or schedule one for the future. In addition, there is a limited selection of curated items so people can easily get in, and get out. We are working with a large wine & champagne distributor in Napa, a few high end chocolate stores throughout the U.S. and a boutique baby clothing company. The goal is to create a scalable network of curated offerings throughout the world and a fully integrated platform that takes care of remembering countless events going on in your network.','[\"CS4624\", \"Simplysent\", \"eCommerce\"]','N/A',6,'For CS4624, additional work was done to further improve the previous version of SimplySent. Word and PDF versions of the final report and final presentation describing this term project are attached.',NULL,NULL,NULL,NULL,NULL),(222,'New Event Detection','The purpose of this system is to be able to automatically detect when a new world event has occurred using RSS feeds of popular world news websites.  This will aid social scientists in their analysis of world trends.  This system is being designed specifically for integration with the the IDEAL (Integrated Digital Event Archiving and Library -http://www.eventsarchive.org/) project.  The system is a collection of Python scripts each of which fulfills an objective outlined in greater detail in the final report, Section 3, System Architecture.','[\"new event detection\", \"natural language processing\", \"topic detection and tracking\", \"cs 4624\", \"IDEAL\"]','N/A',6,'There are PDF and Word versions of the final report, and PDF and PowerPoint versions of the final presentation. A zip archive has the source code developed.',NULL,NULL,NULL,NULL,NULL),(223,'Tracking FEMA','The finished product is a website visualizing the efforts of disaster response organizations (such as FEMA). The visualizations are driven by a Javascript based library used to display various aspects of a disaster. The data used in the visualizations is parsed from HTML in Virginia Tech’s Integrated Digital Event Archiving and Library (IDEAL). The project has led to scripts that extract information from IDEAL and then process them into set fields. Those fields (data) then manually have to be converted into the proper (intended) visualization and entered in the CMS. The future hope for this project is that the whole process outlined above can be automated.','[\"Information visualization\", \"FEMA\", \"Tracking disaster response\", \"IDEAL\"]','N/A',6,'The zip archive attached to this project is the compressed TrackingFEMA Git repository. It contains the CMS (RefineryCMS - Rails), processing scripts, as well as visualization sample code. The processing scripts are in a folder called TrackingFEMAProcessing. The visualizations are contained in Visualization. The rest of the rails files are contained within the usual Ruby on Rails file system structure.',NULL,NULL,NULL,NULL,NULL),(224,'THATCamp Documentary','For the CS4624 class, our clients wanted to record and document the atmosphere and events of THATCamp VA 2015 that was held at Virginia Tech. We gathered extra footage of the camp events as well as interviews of camp attendees about their views of camp, and digital humanities in general.','[\"THATCamp\", \"THATCampVA\", \"Unconference\", \"Documentary\", \"Digital Humanities\", \"Humanities\"]','N/A',6,'CS4264Spring2015 Multimedia MPG4 - Brief overview of the benefits of MPEG4, its standards, origins, and uses, in PDF and .pptx form. CS4624Spring2015THATCampPresentation - Closing presentation of our project and its accomplishments, in PDF and .pptx form. Includes details about clients, key aspects, lessons learned, and acknowledgements. \nTHATCamp video link.pdf - THATCamp video. This video utilizes footage from THATCamp and interviews from attendees to get an overall feel for what THATCamp is and how it functions. \nHumanities and Technology video link.pdf - Humanities and Technology video. This video answers the question of what the Humanities are and how technology is influencing them. \nTHATCamp Documentary project - Final breakdown and overview of the entire process of the project, in PDF and .docx form. Reporting starts before the project began and the various sections were updated as the project went along.',NULL,NULL,NULL,NULL,NULL),(225,'Contemplative Practices Interviews','This technical document covers the contemplative practices interview project.  This project is a part of the CS 4624 Multimedia, Hypertext, and Information Access capstone course at Virginia Tech.This report aims to describe our requirements, design, outcomes, implementation, prototype, solution refinement phases, testing and evaluation, deliverables, plan, and more.The goal of this project is to raise the visibility of contemplative practices on campus and provide support for developing proposals for contemplative practices.  We aim to achieve this goal through a composite video collection containing interviews with various individuals around campus about their contemplative practice and its impact on their lives.The majority of this report was written in increments over the semester as a check-in and documentation every few weeks between the team, course advisor, and client.  Therefore, sections one through seven are written from a perspective while still working on earlier stages of the project between January and April 2015.  The user’s manual contains sections one and two.  The developer’s manual consists of section three.  The lessons learned portion is contained in sections four through seven and the final presentation PowerPoint.   The acknowledgements are contained at the end before the references.When the project was all finished, we found students had experience in many different disciplines. We met a lot of great people and conduct our interviews to get some excellent footage.  Generally, students feel more relaxed and stress-free after practicing.  There are endless benefits for their quality of life.  Students highly recommended that other students try out a contemplative practice.  Performance in the classroom is even helped through practicing.   Once we were done filming, we put together an intriguing composite video uploaded to YouTube for the public to view the final product.Some of the problems faced and lessons learned include: finding interviewees, convincing random students that it was worth their spare time to help with our project for free, coordinating our schedules with interviewee’s schedules, equipment availability, originally learning to use the equipment, originally learning to edit the footage, asking the “right” questions that provide us with the information the client is looking for in the final video, making the interviewees feel comfortable enough to open up on camera, creating a memorable storyboard, editing the video so that it actually captures the attention of viewers, rather than boring them with an interview.  This journey is chronicled below.As a developer, to continue this project, you should contact the client.  Future developers can film their own footage and interviews and then edit their own videos to continue the goals of the project.  It will be added to the YouTube collection with past year’s videos.','[\"video\", \"YouTube\", \"contemplative practices\", \"student interviews\"]','N/A',6,'The final video can be viewed at https://www.youtube.com/user/ContempVideo',NULL,NULL,NULL,NULL,NULL),(226,'Tweets Metadata','The previous CTRnet and current IDEAL projects have involved collecting large numbers of tweets about different events. Others also collect about events, so it is important to be able to merge tweet collections. We need a metadata standard to describe tweet collections in order to perform this merge and to be able to store them logically in a database. We were tasked with continued development of a tool for archiving tweet collections, merging collections, and preparing summaries about the respective collections, their union, and their overlap, etc. Preliminary work on this was carried out by Michael Shuffett (see his report from CS6604 in VTechWorks) and we were asked to develop upon his code, test it, extend it, apply new technology, and further document it where necessary.\nWe met with our client, Mohamed Magdy, a Ph.D. candidate working with QCRI and Dr. Fox and came up with the following project deliverables: (1) a standard for tweet sharing and for tweet collection metadata, (2) a method for merging such collections,  (3) a report, and (4) a web-application tool, putting all of these things together.\nThe expected impact of this project will be having increased collaboration between researchers and investigators all trying to use tweet metadata to find insights into everything from natural disasters to criminal activity and even stock market trends. A tool of this type will help ease the process of merging archives of tweets between researchers which will then lead to better analysis and less time spent trying to re-organize information that could be sifted through by this tool.\nOur team was able to develop upon Michael Shuffett’s code, improve it, and set up new and improved wireframes for the website. We were able to start framing out a tool that allows more than two types of files to be merged, which previously had to be in a single format. In the future, the required formats wouldn’t be as strict, making it easier to upload different types of files, thus making it even easier on the user.','[\"Twitter\", \"Metadata\", \"tweet collections\", \"events archive\", \"CTRnet\", \"IDEAL\"]','N/A',6,'Twitter',NULL,NULL,NULL,NULL,NULL),(227,'Grickit Movie Decision Project','This project focused on Grickit, the Movie Decision Making tool created by William Vuong and Michael Long with the guidance of Dr. Steven D. Sheetz, the client. The final report has three main sections: the user’s manual, the developer’s manual, and our lessons learned.\nThe user’s manual is a guide for a person who intends to utilize the Movie Grickit that was created, and wants more information about the Grickit and its purpose. Included is a general overview as well as an in depth look at the roles available for users of the Grickit system. We explore how each user affects the Grickit system to ultimately calculate and display the intended results. Included are screenshots so that the reader can easily identify the features referenced in the report.\nThe developer’s manual is a guide for future developers to understand our design choices and the structure of our Grickit implementation. It begins with a high level overview of the technical workflow. Then is information about the RESTful web API created to store the Grickits and handle calculations. We discuss the various technology choices utilized in creating this API. The accompanying live documentation is discussed and shown to the developer through example use cases. Validation of user entered data and regression testing were done to ensure a good user experience.\nThe developer’s manual also contains information about the front-end implementation. This section starts with the general design and thought process for creating the Movie Grickit. We describe technologies chosen and how they were utilized. Examples are shown with code snippets, about how the technology is used in the scope of this project. Interesting considerations taken with the creation of the front end have been noted and discussed. Included throughout the developer’s manual are screenshots to give a visual representation of the design choices.\nThe lessons learned section contains the issues addressed while developing this tool, and our solutions. The main issues arose when deploying on the server identified by our client. The hardware was older (Windows Server 2003) and therefore we had versioning issues with the technologies we were using and the minimum OS supported. Our database solution required a newer OS, so we pivoted to a different storage solution, that utilizes a similar noSQL structure where data is stored in a file. This is not the optimal solution but was appropriate given the circumstances. We also had issues with the website needing to be retrieved through HTTPS. Therefore any calls using HTTP were routed through our HTTPS server.\nOur choice in separation of responsibilities for the AHP algorithm and our presentation layer were due to the goal of later producing a generalized Grickit that does not depend on a specific domain set. Our back-end solution has no notion of the movie data in our Movie Grickit. We suggest that modifications are made on top of our solution instead of starting from scratch when expanding, as our back-end will be a good base for when a generalized front-end is created.\nThe Movie Grickit was an interesting project that had many considerations to take note of in both the back-end handling of the AHP algorithm and data storage, as well as the front-end user experience and design choices. Considerations were kept in mind for the future generalized solution with the back-end already ready to support a generalized solution. We were able to deploy on the intended hardware despite some minor obstacles. Overall the client was happy with the end result and will be moving forward with the project to expand its use cases.','[\"grickit\", \"decision support\", \"movie database\", \"AHP algorithm\"]','N/A',6,'grickit',NULL,NULL,NULL,NULL,NULL),(228,'Seventeen Moments in Soviet History','This describes results of the Seventeen Moments in Soviet History semester Capstone project in CS4624 Multimedia, Hypertext, and Information Access course. The majority of the work done with the website focused on shoring up security flaws and issues. This was first started by identifying all elements of the website that interacted with the database. Next, the corresponding code in the website was found and work was done to correct it. This began by identifying how CakePHP handles SQL queries and recommended ways to sanitize SQL queries in CakePHP. Next, flow of control in querying the database was changed to ensure that the recommended changes could be implemented. Once these change were made, the website was first tested to ensure that functionality was not damaged in anyway. Once it was confirmed that the website was still as functional as before the changes, testing was undergone to ensure that the SQL issues were fixed. This was done by attempting to make an SQL injection on the website. The database was then checked to ensure that no changes were made to the website and that the database was in the same state as before the injection attack.\nIn addition to fixing the security issues associated with the website, general database changes were made as well. First, user registration was changed to ensure that new users were not listed as moderators. Next, all moderators were dropped, with the clients representing the only moderators. Then, the website was modified to no longer store passwords in plain text and changed to only store the hashed passwords. This was confirmed by making new users and testing to see if their plaintext passwords were stored. In addition, all plaintext passwords were removed from the database.\nResearch was also undertaken for notifying the users of the Soviet History website that the website was operational again. First, a script for emailing users was considered but determined to be unacceptable due to a limit on number of emails sent by an address and the fact that the scripts are dependent on the running computer’s configurations. Next, a mass email service was considered, but determined to be undesirable as they operate on a monthly subscription fee and the service would only be used once. It was then determined that the best course of action was to determine which users should be emailed and only email them so as to not broadcast to the original hackers that the website was back up.\nFinally, work is being done to fix the subtitles not appearing on the audio sections of the website. However, currently they are not working, although, test code is being run to see if it improves the subtitles issue.','[\"Soviet History\", \"Website security\", \"database\"]','N/A',6,'Items included: \nSovietHistory-Final (1).docx - Editable Word Document of Final Report.\nSovietHistory-Final (3).pdf	 - PDF version of final report.\nSovietHistory.pdf - Final Complete requirements report.\nSovietHistoryReport.pdf - PDF version of our report for easier viewing.\nSovietHistoryPresentation.pdf - PDF version of our PowerPoint presentation for our project.\nSovietHistoryPresentation.pptx - PowerPoint version of our project presentation.',NULL,NULL,NULL,NULL,NULL),(229,'YMCA at Virginia Tech Multimedia Marketing Advertisements','This repository contains the deliverables for our efforts on the YMCA at Virginia Tech Multimedia Marketing Advertisements project. This project is for the CS4624 capstone course entitled “Multimedia, Hypertext, and Information Access” offered by the Computer Science Department at Virginia Polytechnic Institute and State University, which is located in Blacksburg, VA. This project was established in accordance with requirements from Mr. Ryan Martin, Executive Director of the YMCA at Virginia Tech. The goals of this project were to create five, thirty-second long video advertisements to be aired at the Regal Cinema complex in Christiansburg, VA and to develop other cross-functional multimedia that could be strategically used online and in person to help promote the Y at Virginia Tech. Additionally, we wanted to help shift the community perception of the Y at Virginia Tech from a place for exercise towards a place to participate in volunteerism and community service. We also wanted to use our videos to strengthen the Y’s community outreach programs while enhancing the Y’s presence in the community. We began this project by meeting with key Y staff members, discussing their areas of the Y, and asking what they thought made the Y unique in the New River Valley. From these discussions, we synthesized five video themes which would help to promote the Y at Virginia Tech. We then scheduled and filmed formal interviews with five staff members and filmed many Y programs and events. We logged all of the footage we had collected, selected background music, and designed introduction and closing graphics for our videos. Finally, we edited the interviews down to thirty seconds of audio, selected interesting cutaway clips, and rendered all of the videos in the correct format. In addition to the five promotional videos, we also delivered five multimedia presentations to be used at community events, a social media calendar and associated guidelines, and a video clip log, with metadata from all of the raw footage that we collected.','[\"Y@VT\", \"YMCA\", \"Virginia Tech\", \"YMCA at Virginia Tech\", \"Multimedia Marketing\", \"Video Advertisements\"]','N/A',6,'This repository contains a final report and presentation in Microsoft Word and Microsoft PowerPoint respectively, as well as both in PDF format. It includes final rendered video files in both H.264 and Apple ProRes. It includes five presentation files in PowerPoint format and a social media calendar and guidelines in PDF, Microsoft Word, and Microsoft Excel formats. It also contains ZIP files with the interview audio and introduction and closing graphics from our promotional videos.',NULL,NULL,NULL,NULL,NULL),(230,'21st Inventory','Currently, Network Infrastructure & Services (NI&S) takes inventory of equipment assigned to employees (computers, laptops, tablets, tools) and sends reports of higher value items to the Controller’s Office. All items have a VT tag number and a CNS number, which can currently only be matched up via an Oracle Forms interface. An inventory clerk must personally verify the existence and location of each piece of equipment. An improvement would be an app that scans an inventory number or bar code and the GPS location where it is scanned and the custodian of that equipment. This data could then be uploaded to a more accessible Google spreadsheet or similar web-based searchable table.\nThe 21st Century Inventory app aims to solve this problem by employing barcode scanning technology integrated into a mobile app which would then send the accompanying asset ID to a CSV formatted output file.  By directly tying a product’s asset ID to the user and their information, along with having the capability to scan a product’s barcode to simplify inventory lookup, saving product information to a CSV file, and giving the user the ability to edit the current information of a product in the application, we are providing a significant upgrade to a system that currently solely relies on an Oracle Forms interface.','[\"Scanner\", \"Barcode\", \"Barcode Scanner\", \"NI&S\", \"21st Inventory\"]','N/A',6,'Scanner',NULL,NULL,NULL,NULL,NULL),(231,'SearchKat','SearchKat is a searchable database built on Apache Solr. The database was created with the purpose of being able to search Hurricane Katrina narratives that were gathered from survivors. The project was carried out in Spring 2015 in CS4624 (Multimedia, Hypertext, and Information Access) with Dr. Edward Fox as the professor and Dr. Katie Carmichael as the project client.\nSearchKat has user specified word groupings which were determined by Dr. Carmichael in order to make the database thematically searchable, with each of these word groupings being considered a theme. The system can both search by these word groupings as well as by generic word associations such as synonyms.\nThe database has customized fields displayed when a query is performed. We have created a CSV file which stores the filename, line number, and line content for each line through all of the files. We use this information to display this relevant data whenever a search is performed.','[\"SearchKat\", \"Hurricane Katrina\", \"Database\", \"Multimedia, Hypertext, and Information Access\"]','N/A',6,'Class project for Dr. Fox\'s CS4624 Spring 2015 class Multimedia, Hypertext, and Information Access.\nThe uploaded files are 2 .zip files. Unzipping \"CS4624Spring2015solr-5.0.0\" will give a directory named \"solr-5.0.0.\" It contains all of the files needed to run the SearchKat database. The presentation (included as a PowerPoint and PDF) and the final report (included as a Microsoft Word document and PDF) are also included. The second .zip file contains the remaining files.  \nInstructions to run SearchKat are in the files \"CS4624Spring2015SearchKatFinalReport\" provided in both PDF and DOCX formats. Various other details including a history of the project can also be found in these files. Explanations/directions for other files included are also in this report.\n\"CS4624Spring2015KatrinaPresentation\" is the file used for the final presentation our team gave in Dr. Fox\'s class.',NULL,NULL,NULL,NULL,NULL),(232,'Arabic News Article Summarization','This project involves taking Arabic PDF news articles to produce results from our new program that indexes, categorizes, and summarizes them. We fill out a template to summarize news articles with predetermined attributes. These values will be extracted using a named entity recognizer (NER) which will recognize organizations and people, topic generation using an LDA algorithm, and direct information extraction from news articles’ authors and dates. We use Fusion LucidWorks (a Solr based system) to help with the indexing of our data and provide an interface for the user to search and browse the articles with their summaries. Solr is used for information retrieval. The final program should enable end users to sift through news articles quickly.','[\"Arabic\", \"Fusion\", \"Solr\", \"Classification\", \"Newspaper articles\", \"Named Entity Recognizer\", \"LDA\"]','N/A',6,'Arabic',NULL,NULL,NULL,NULL,NULL),(233,'Text Transformation','The purpose of this project is to assist the VTTI in converting a large citation file into a CSV file for ease of access. It required us to develop an application which can parse through a text file of citations, and determine how to properly put the data into CSV format. We designed the program in Java and developed a user-interface using JavaFX, which is included in the latest edition of Java.\nWe came up with two main tools: the developer tool and the parsing program itself. The developer tool is used to build a tree made up of regular expressions which would be used in parsing the citations. The top nodes of the tree would be very general regexes, and the leaf nodes of the tree would become much more specific. This program can export the regex tree as a binary file which will be used by the main parsing program.\nThe main parsing program takes three inputs: a binary regex tree file, a citation text file, and an output location. Once run, it parses the citations based off of the tree it was given. It outputs the parsed citations into a CSV file with the citations separated by field. For any citations that the program is unable to process, it dumps them into a failed output text file so.\nWe also created an additional program as an alternative solution to ours. It uses Brown University’s FreeCite parsing program, and then outputs parsed citations to a CSV file.','[\"Citation\", \"Parse\", \"Regex\", \"Java\"]','N/A',6,'Citation',NULL,NULL,NULL,NULL,NULL),(234,'Lincoln In Our Time: Online Museum Exhibition Website','Commemorating the 150th anniversary of Abraham Lincoln\'s assassination, the Virginia Center for Civil War Studies presented a series of programs, including a 2-day symposium of internationally-renowned scholars and a six-week-long exhibition of Lincoln-related artifacts in Newman Library. The exhibition included both objects and video presentations by students in Hist 2984, Abraham Lincoln: The Man, the Myth, the Legend.\nOur client, Kim Kutz, wanted a website to accommodate the exhibit. It would be a WordPress site that exists within the current Virginia Tech Lincoln WordPress site. Our original plan was to include a 3D virtual tour of the Lincoln exhibit in the Newman Library. However, once the exhibit was set up and we visited it, we decided it was too small to be suitable for a virtual tour. So instead, we took pictures of it from various angles to include in an image slider on the homepage of our website.\nWe decided our website would contain two pages, since it was just an addition to the already existing Virginia Tech Lincoln website. These two pages would be a homepage and an artifacts page documenting all the artifacts relating to Lincoln that the Newman Library owns.\nThe homepage included the image slider with the various pictures we took of the exhibit as well as a short description of the purpose of the exhibit and what it contained. In this description, we included a link to the artifacts page. The website has two tabs, one for each page, as well as a search bar so a user can search within the site.\nThe artifacts page contains a photo grid that uses the Mural Theme currently available as one of the themes Wordpress provides. With the Mural Theme, each square photo when hovered over displays text that describes the title of the photo. Additional text can be added to describe the photo. We just included the title of each photo and our client can add text as she wishes in the future.\nEach of the photos in our photo grid is of one of the Lincoln artifacts that was displayed in the Lincoln exhibit. Therefore, the titles were the names of these artifacts. When one of these photos is clicked, the user is transported to a separate page where he or she can view the student presentations explaining these artifacts. The presentations were done by students in Dr. Kim Kutz’s history class. She provided us with these presentations and specifically requested that they be included in our site. The presentations consisted of YouTube videos and Prezi presentations. We allowed both the YouTube and the Prezi presentations to be shown in WordPress’s lightbox feature. In addition, clicking the title of each video takes the user to the YouTube page where the video exists.\nThis submission contains a zip file of the website files, Word and PDF versions of the final report, and PowerPoint and PDF versions of the final presentation.','[\"Lincoln\", \"Online Museum\", \"Online Exhibit\", \"Civil War\", \"Website\"]','N/A',6,'This submission includes a final report in Word and PDF format, a final presentation in PowerPoint and PDF format, and a zip file of the website files.',NULL,NULL,NULL,NULL,NULL),(235,'News Event Website','This report gives a detailed overview of the archiving services developed by the News Event Detection Website Group in the Virginia Tech Multimedia, Hypertext, and Information Access capstone. Our group developed a framework to help facilitate the collection of large amount of news data for the project client, Mohamed Magdy Gharib Farag. The paper begins by giving the reader an overview of the project background and the importance of our final product. Following the overview are several sections on the design stages of our product. These sections include our system overview and architecture. Then we discuss the data, component, and interface design procedures that took place. The paper then covers the developmental stages of the project. We discuss our implementation and prototyping plan as well as the testing procedures and results we used on the product.\nNext the paper will instruct how the user is to conduct a query through our web interface. Following the user manual is a developer’s manual documenting in full detail the tools and procedures used to create our product. These tools include PHP, Apache, and Flask. This developer manual will assist any individual looking to analyze the product or make future modifications.\nFinally, the paper addresses the lessons the group has learned while working on this project. This includes some of the challenges that were faced, solutions to those challenges, as well as future changes that could be made.','[\"News Archiving Automation\", \"Hypertext\", \"News Event Detection Website\", \"News Website\", \"News Archive\"]','N/A',6,'This contains the written report that documents the project details, the PowerPoint presenting the final product, the various programming files used to implement the final product, and the editable report file.',NULL,NULL,NULL,NULL,NULL),(236,'Virginia Center for Civil War Studies website redesign','Rebuilding the digitized sources website for the VCCWS. The website was previously hosted somewhere else, and used third-party services to operate. This project aimed to create a working program running on VT hosting, and didn\'t rely on other servers to function. These reports/presentations outline the design of the website.','[\"website\", \"Civil War\", \"code\", \"redesign\"]','N/A',6,'The report + presentation + source code for the VCCWS digitized sources website redesign project. Inside the ZIP file is our source code. The contents are as follows: Project folder: The actual PHP/javascript code are in here. UI folder: This folder has the reference material used in the CSS of the website. The UI folder contains libraries that have CSS code from other sources as well (Like Twitter bootstrap). gulpfile.js: The gulp file allows the website to use the gulp utility. package.json: This package contains some necessary json files for the website to operate properly. vccws.sql: This is the master sql file that needs to be run one time upon initial startup of the project. This will create the necessary tables/data to be referenced by the website. vendor.json: This also has necessary json data needed for proper operation. README.md: The readme should be read before any additional work is done.',NULL,NULL,NULL,NULL,NULL),(237,'Giles Animal Rescue Project','Our clients, Christine Link-Owens and Helen Gillespie have asked us to upgrade and modify their website atgilesanimalrescue.org. They have been maintaining a Drupal website with the help of previous Virginia Tech students.\nBluehost is a web hosting company, which offers free web to Giles County Animal Rescue through the organization called Grassroots. Our client provided us their account to improve the website.\nThe website we have been working on contains news and events for the Giles County Animal Rescue organization as well as available animals for adoption, donation links, memorials for animals and contact information.\nThe main goal in this project is to make the website running Drupal to be easy to maintain, and to make the information on the website concise for the user to view.\nOur secondary goal is to give back a product that the client will be able to maintain by themselves.','[\"Drupal\", \"Website\", \"animal rescue\"]','N/A',6,'In addition to the final report (in Word and PDF) and final presentation (in PowerPoint and PDF),  there are manuals for site operation and site development.',NULL,NULL,NULL,NULL,NULL),(238,'Blacksburg to Guatemala Archive','The primary objective of the Blacksburg to Guatemala Archive Project is to create a medium for cultural exchange between Christ Church, Blacksburg, and their sister parish in San Andres Itzapa, Guatemala.  This project will create a website and will allow for these two parishes to remain in close contact following the recent visit of a delegation from Christ Church, Blacksburg, to Guatemala in January 2015.  We are completing this project as soon as possible in order to minimize the delay between their visit and the establishment of such a cultural exchange.  Both parishes will benefit from our project, satisfying a desire to remain in-touch and embrace cultural differences.\nWe decided to use WordPress to offer the simplest possible solution. WordPress will allow the client to easily maintain stories on the website and will also give readers an easy way to enjoy them. We chose a simple theme and then further perfected it to reduce any complications. We decided to only display aspects that were absolutely necessary to the project. Aside from the stories, categories and search are the only other modules that are visible to the readers. The website also allows users to leave comments on every story so they can interact with the parishes in an easy way.\nAnother big request from the client’s end was to allow for stories written in Spanish.  We incorporated WordPress’s bilingual tools to support this functionality. Stories can be written in either language and they will be formatted appropriately. A search engine has also been implemented to display results in both English and Spanish.\nOverall, the website was a success. The primary focus was usability and the tests we ran proved the website was easy to use. The client was also happy with the results and can see the website being very useful to both parishes.','[\"Blacksburg\", \"Guatemala\", \"cultural exchange\", \"church website\", \"archive\"]','N/A',6,'Blacksburg2Guatemala website allows the Christ Church parishes to share stories and culture with one another. This is an easy way to archive stories that would otherwise be lost. The website can be viewed at: http://blacksburg2guatemala.wordpress.com. The separate file for the testing report (BburgGuatemalaJoshiTestingEdited.pdf) summarizes work prior to the final report.',NULL,NULL,NULL,NULL,NULL),(239,'TrackPunch','TrackPunch is a music streaming platform that redefines music discovery through social network influence using Big Data from Facebook.\nThe big data indexing and analysis was designed to run in PHP and is capable of running reliably at throughput rates of around 200 Web requests per minute without causing any outages due to rate limits. This data is provided to the user in a polished interface, making the overall experience easy and highly enjoyable.\nThis project involved the design and implementation of technology supporting TrackPunch.','[\"Music\", \"Streaming\", \"Discovery\", \"Social\", \"Friends\", \"Trends\", \"Feed\"]','N/A',5,'TrackPunch is a music streaming service that turns music discovery into a social experience. Our platform provides users with an elegant way to find new music based on their friends\' listening habits. We aggregate music data from top music sources to index who influences each user individually. Our users can search, listen to, and share high quality content from GrooveShark, and SoundCloud, and YouTube for free. On TrackPunch, your next favorite song awaits.',NULL,NULL,NULL,NULL,NULL),(240,'Website Redesign Project: Creating Intuitive Content Managment','Our client, Christine Link-Owens, is the president of the non-profit Giles Animal Rescue, which helps homeless, neglected, and abused pets in Giles County. The organization has a Drupal website,www.GilesAnimalRescue.org, that several groups of students have worked on, but it still needed bug fixes and feature expansion. Our goal was to make all the changes she asked for, as well as create a website content management environment that is easy to maintain, as well as thorough instructions of common tasks.\nMany of our changes to the site were text formatting issues and adding images to pages, but also include:\n•	Updated the version of Drupal to resolve security issues\n•	Added pictures to many of the pages to showcase work of Giles Animal Rescue and its volunteers\n•	Added the ability to enlarge images by clicking on them\n•	Enabled gallery formatting to create image slide shows\n•	Changed settings in Drupal to make adding and editing content easy, by creating tailored content types with specific fields\n•	Resolved text formatting issues by making Filtered HTML default on all text content boxes\n•	Accessed newsletter subscribers\n•	Created a separate tab for Giles County Animal Shelter for information about the shelter\nAdditionally, we migrated hosting from Go Daddy to Bluehost. While updating the Drupal core when hosted at Go Daddy, we experienced many technical difficulties including limited control of our files and difficult menus for tasking such as backups. We moved to BlueHost because we were able to find an organization, GrassRoots, which offers free webhosting through BlueHost. Bluehost has a very intuitive cPanel that displays all the options on one page to help navigate through the website with ease.\nOverall, our client has been very pleased with our changes and is excited about the future of her website. We feel that we have put forth our best effort over the course of this semester. We are proud to have assisted such a great organization that helps save unfortunate animals. We hope that our changes will aid them with their mission.','[\"Website\", \"Drupal\", \"Content Management\", \"Intuitive\", \"CS 4624\", \"Capstone\", \"Hypertext\", \"Multimedia\", \"BlueHost\", \"GoDaddy\", \"Giles Animal Rescue\", \"Giles County Animal Rescue\", \"CS4624\"]','N/A',5,'A Drupal website, www.GilesAnimalRescue.org, redesign project for Giles Animal Rescue organization. We have attached our poster for VTURCS Symposium 2014 as well as our final and midterm presentations, where we presented our progress to our class. The final report is our full documentation of what we have accomplished over the course of the semester.',NULL,NULL,NULL,NULL,NULL),(241,'Contemplative Campus at Virginia Tech','A website and a video has been created for the Contemplative Campus group at Virginia Tech.  This is a hub for students and community members to seek knowledge of what these practices are about and how they can get involved.  The website is located athttp://www.contemplativecampus.dlib.vt.edu/and the latest video(s) can be found athttp://www.contemplativecampus.dlib.vt.edu/videos/.  Inside this submission are the presentation materials along with a manual on how to use and maintain the website and the video.\nThanks to Dr. Douglas Lindner for the opportunity to work with him and his contacts in this area.\nThe website was created for ease of use and management, using WordPress, and the video has been created using professional grade software provided by Innovation Space.','[\"Contemplative Campus\", \"Virginia Tech\", \"Contemplative Practices\"]','N/A',5,'Contemplative Campus',NULL,NULL,NULL,NULL,NULL),(242,'Virginia Tech Themed WordPress Templates','Virginia Tech has a WordPress instance available for blogging for VT students, faculty, and staff. They have installed several commercial themes that allow for a variety of customized blog sites. However, they do not have any themes that are developed in line with the Virginia Tech Style Guide. Specifically, the colors and the general layout should be similar to what is available in the university’s current guideline. This project would be developing these themes that could be made available for faculty and departments.\nOur project deliverables are three Virginia Tech customized themes, with all themes using a drag-and-drop feature. Drag-and-drop is a feature that allows the user to select a virtual object such as textbox, link, video, etc. and drag it into the theme layout. This allows for a more intuitive and easier interface for editing themes.\nOur client is Dr. Jennifer Sparrow, Sr. Director, Networked Knowledge Ventures and Emerging Technologies, Technology-enhanced Learning and Online Strategies (TLOS). Dr. Sparrow also introduced to us Crystal Farris and Brian Broniak who would be working with us on this project. After our first meeting with our client, they requested a template similar to thewww.vt.eduwebsite. This is to present a more unified look across the university. So for our first theme, we created a template based on the Eclipse theme in WordPress. We changed the layout and color to match thewww.vt.eduwebsite. For our second template, we decided to use a VT WordPress provided theme. We decided to use the Computer Science courses website to model the template and after looking at the available themes, we decided to use Headway to create the child template. After much consideration and research, we decided to redo our first Eclipse child theme in Headway.\nFor the tools we had at hand, we used BitNami, a WordPress client, together with NotePad++ and Google Chrome Developer tools. For the second template, we used the VT WordPress Blog and Headway visual editor. The developer languages we used were HTML (for the design of the web template), PHP (for the functionalities such as index, posts, images, etc.) and CSS (for creating the style of the web template)','[\"Wordpress child theme\", \"Wordpress template\", \"Virginia Tech Wordpress template\"]','N/A',5,'Files are provided for themes and templates, as well as for project presentations and the final report.',NULL,NULL,NULL,NULL,NULL),(243,'DLRL Cluster','The Digital Library Research Laboratory is a group focused on researching and implementing a full stack Hadoop cluster for data storage and analysis. The DLRL Cluster project is focused on learning and teaching the technologies behind the cluster itself. To accomplish this, we were given three primary goals.\nFirst, we were to create tutorials to teach new users how to use Mahout, HBase, Hive, and Impala. The idea was to have basic tutorials that would provide users with an introductory coverage of these modern technologies, including what they are, what they’re used for, and a fundamental level of how they’re used. The first goal was met by creating an in-depth tutorial for each technology. Each tutorial contains step-by-step instructions on how to get started with each technology, along with pictures that allow users to follow along and compare their progress to ensure that they are successful.\nSecond, we would use these tools to demonstrate their capabilities on real data from the IDEAL project. Rather than have to show a demo to each new user of the system firsthand, we created a short (5 to 10 minute) demo video for each technology. This way users could see for themselves how to go about utilizing the software to accomplish tasks. With a video, users are able to pause and go back at their leisure to better familiarize themselves with the commands and interfaces involved.\nFinally, we would utilize the knowledge gained from researching these technologies and apply it to the actual cluster. We took a real, large, dataset from the DLRL cluster and ran it through each respective technology. Some reports were generated, focusing on efficiency and performance, and an actual result dataset was generated for some data analysis.','[\"Mahout\", \"Impala\", \"HBase\", \"Hive\", \"Hadoop\", \"IDEAL\"]','N/A',5,'Demonstration and comparison of capabilities of Hadoop tools: HBase, Hive, Impala, Mahout. Contains tutorials and video demos. \nThis project was possible thanks to National Science Foundation IIS - 1319578 support  of the Integrated Digital Event Archiving and Library (IDEAL) grant.',NULL,NULL,NULL,NULL,NULL),(244,'Collegiate Times Grades Database','The Collegiate Times grades database project is the public dissemination of the Virginia Tech grades database amassed by the Collegiate Times, the student newspaper of Virginia Tech, on their new website. The Collegiate Times has collected Virginia Tech grades data over the past decade using Freedom of Information Act requests and then provided this data to users for free on their website. However, the Collegiate Times has recently changed websites and there is no longer a way for the public to see or manipulate this data. Working with Alex Koma, managing editor of the Collegiate Times, this project provides a simple and maintenance-free solution for the Collegiate Times to put this data back in the public sphere by creating a web page that facilitates public access to the data.\nThe system is made up of a series of four dynamic dropdown menus that are filled using information from the database itself. The first dropdown menu is populated when the page loads and the remaining dropdowns populate once a selection is made, giving users some direction.  Users search the database by specifying a subject and course number for a class at Virginia Tech. If they prefer a narrower set of results, they can also specify a year and semester for their chosen course. The program then provides the average grade point average and the percentage of students that received each letter grade for every year and semester section of the course requested. The data is both sortable and paginated. The project was programmed using HTML, PHP, SQL and Javascript.\nThe project is available athttp://database.collegemedia.com/databases/grades/grades.html. Students can use this project to look up the average grades in courses they are thinking about taking during course registration and add/drop. Other potential users could use the data to research grading trends by year or to investigate the grade distribution in Virginia Tech courses. There are numerous other uses for the data and providing this data in a public setting has increased the information and knowledge available on course grades at Virginia Tech.','[\"CS 4624\", \"Multimedia, Hypertext, and Information Access\", \"Collegiate Times\", \"Public database\", \"Capstone project\"]','N/A',5,'CS 4624',NULL,NULL,NULL,NULL,NULL),(245,'Electronic Theses and Dissertations Database Errors','Detecting and correcting coding errors in the Electronic Thesis and Dissertations Database, or ETDdbErrors for short, is a Java based application created primarily for XML file editing and debugging. ETDdbErrors was inspired by the need to fix XML errors in the Virginia Tech Thesis and Dissertation Library Database, where many documents required significant editing to be valid XML and still had visual display bugs. This application’s purpose is to permit the user to easily debug and edit XML files so that they can be correctly displayed.\nETDdbErrors  is Java based so that it can be used across a wider array of platforms and systems. The application uses a configuration file, a parser, an output file, a log file, and an input file. The user can edit the configuration file to specify what kind of errors he is encountering, and what solution options he wants for each find of error. Users may specify any number of solutions, and if there is more than 1, the program will display the context of the found error, and prompt you for a decision on which solution to implement. The application also automatically removes control characters that are illegal in XML. This allows users of ETDdbErrors to quickly and efficiently edit and fix problematic XML files.\nETDdbErrors also has a secondary version, called VT Thesis and Dissertation Database Parser, or VTTDP for short. This is an older version of ETDdbErrors  that was made specifically for the VT Thesis and Dissertation Database. This application is not interactive and not usable beyond fixing the errors specific to the VT Thesis and Dissertation Database, but provides a faster and more targeted fix to the problems encountered there.','[\"Electronic Theses And Dissertations\", \"Database\", \"ETD\"]','N/A',5,'Our client for the Project is Zhiwu Xie, an Associate Professor and Technology Development Librarian at Virginia Tech. \nThe files provided include: the Project files, Midterm Presentation files, Final Presentation files, and Final Report files.',NULL,NULL,NULL,NULL,NULL),(246,'CS4624 IDEAL Spreadsheet','The IDEAL proposal encompasses an incredibly vast infrastructure of technology intended to be used by people of varying backgrounds. The analysts and researchers who will be familiar with the data presented through many aspects of the IDEAL project may not be familiar with the means of accessing it from the differing resources. The purpose of this project is to provide non technically-skilled personnel with the ability to access data in a easy to use and intuitive way.\nThe data this project focuses on are tweets, photos, and webpages found on web-archive files, or ‘warc’ files. These warc files are comprised of a few, to several hundreds of gigabytes, making a manual search to find specific information near impossible. Instead, we use a Cloudera VM as a prototype of the cluster used in IDEAL, and demonstrate how to load WARC files for Hadoop processing. That allows parallel big data processing with several software tools, supporting database and full-text searching, text extraction, and various machine learning applications.\nOur project goal to present relevant data in an attractive, useful, and intuitive way was achieved through the creation of a web based spreadsheet-like service. While the exact use goes on in greater detail below, the overarching plan was to provide the user with an easy to use spreadsheet, which takes input from the user and returns the relevant data in spreadsheet cells. The other functionality requested by the client for special jobs such as ‘all images’ or ‘word count’ led to other features.\nTo summarize, this project intends to provide a web service to provide IDEAL researchers with the means to retrieve relevant information from warc files in an intuitive and effective manner. The project called for several technologies and frameworks which will be elaborated on below, and this project paves the way for increased future development in the IDEAL project mission.','[\"IDEAL\", \"Spreadsheet\", \"CS4624\", \"Hadoop\"]','N/A',5,'Connecting the IDEAL database to a spreadsheet interface. Source code developed is in the zip file provided. Our clients were Mohamed Magdy, a Ph. D. student at Virginia Polytechnic Institute and State University, and the Integrated Digital Event Archiving and Library (IDEAL) Team, supported through NSF IIS - 1319578.',NULL,NULL,NULL,NULL,NULL),(247,'Xpantrac Connection with IDEAL','Title: Integrating Xpantrac into the IDEAL software suite, and applying it to identify topics for IDEAL webpages\nIdentifying topics is useful because it allows us to easily understand what a document is about. If we organize documents into a database, we can then search through those documents using their identified topics.\nPreviously, our client, Seungwon Yang, developed an algorithm for identifying topics in a given webpage called Xpantrac. This algorithm is based on the Expansion-Extraction approach. Consequently, it is also named after this approach. In the first part, the text of a document is used as input into Xpantrac and is expanded into relevant information using a search engine. In the second part, the topics in each document are identified, or extracted. In his prototype, Yang used a standard data set, a collection of one thousand New York Times articles, as a search database.\nAs our CS4624 capstone project, our group was asked to modify Yang’s algorithm to search through IDEAL documents in Apache Solr. In order to accomplish this, we set up and became familiar with a Solr instance. Next, we replaced the prototype’s database with the Yahoo Search API to understand how it would work with a live search engine. Then we indexed a set of IDEAL documents into Solr and replaced the Yahoo Search API with Solr. However, the amount of documents we had previously indexed was far too few. In the end, we used Yang’s Wikipedia collection in Solr instead. This collection has approximately 4.2 million documents and counting.\nWe were unable to connect Xpantrac to the IDEAL collection in Solr. This issue is discussed in detail later (along with a future solution). Therefore, our deliverable is Xpantrac for Yang’s Wikipedia collection in Solr along with an evaluation of the extracted topics.','[\"xpantrac\", \"expansion\", \"extraction\", \"Solr\", \"IDEAL\", \"topics\"]','N/A',5,'The files provided include our Midterm presentation (PowerPoint, PDF), Final Presentation (PowerPoint, PDF), Final report (Word, PDF), and zipped file containing all of the code used to run the various versions of Xpantrac.\n\nWe would also like to acknowledge our client, Seungwon Yang, and NSF IIS - 1319578: Integrated Digital Event Archiving and Library (IDEAL) for supporting and aiding in the completion of this project.',NULL,NULL,NULL,NULL,NULL),(248,'Developing an improved focused crawler for the IDEAL project','The IDEAL (Integrated Digital Event Archive and Library) project currently has a general\npurpose web crawler to find articles relevant to a set of URLs the user can provide. The\nresulting articles are return based on frequency analysis of user provided keywords. The goal of\nour project is to extend the web crawler to return articles related to user provided events and\nother relevant information. By analyzing an article to identify key event components, such as\nthe date, location, and type of natural disaster, we can construct a tree representation of each\nwebpage. Next, we compute the tree edit distance between that tree, and the event tree\nconstructed from the user’s original input. With this information we can predict webpage\nrelevance with a higher certainty than frequency of keyword analysis provides.','[\"web crawler\", \"IDEAL\", \"Python\", \"natural language processing\", \"tree-edit distance\"]','N/A',5,'CS 4624 capstone project. The client is Mohamed Magdy Gharib Farag.\nSupport was provided through NSF IIS - 1319578: Integrated Digital Event Archiving and Library (IDEAL). \nFiles provided have the final report, midterm and final presentations, a poster presented at VTURCS, and related software.\nOur source code can be found at: https://github.com/wbonnefond/focused-crawler',NULL,NULL,NULL,NULL,NULL),(249,'IDEAL Pages','The purpose of this project was to simplify the process of unzipping web archive files, parsing them, and indexing them into a Solr instance.  These tasks are only a subset of tasks that are part of a greater project called IDEAL. The main goal of that project is to build an 11-node cluster that will take roughly 10TB of webpages collected from various events, and ingest, filter, analyze, and provide convenient access to these webpages through a user interface.  The IDEAL Pages portion of this project is critical to the overall success of the mission.  In order to provide desired services with the data, the data needs to be successfully delivered to Solr.  However, working with nearly 10TB of compressed data proves to be a rather difficult task.  Our primary objective was to create a Python script to convert all of the web archive files into raw text files containing the text found on these web pages.  Through the use of multiple existing tools, and software developed in the process, the task was accomplished.  A tool for working with web archive files called Hanzo Warc Tools was incorporated into the Python script to unpack the files.  To extract the text from the HTML we made use of a program called Beautiful Soup which was able to create files that could be easily indexed into Solr.  A key element to making the IDEAL project timely, however, is to distribute this process through the use of Hadoop.  Hadoop grants the ability to run the same process on multiple machines concurrently to effectively reduce the overall runtime of a task.   To accomplish this, it was necessary to split the script into multiple pieces and run them through Hadoop with the use of Map/Reduce.  Using one of the IDEAL Project machines and Cloudera, it was possible to work with Hadoop and Map/Reduce.  The outcome of this project resulted in an efficient way to process web archive files and remains extendable to optimize distribution of the tasks involved.','[\"Digital Archives\", \"Solr\", \"Hadoop\", \"Web Archives\", \"IDEAL Project\"]','N/A',5,'The IDEAL Pages project is part of the IDEAL (NSF IIS - 1319578: Integrated Digital Event Archiving and Library) project and specifically focuses on the processing and indexing of Web archives.',NULL,NULL,NULL,NULL,NULL),(250,'NRV Tweets and RSS feeds','The goal of this project was to associate existing data in the Virtual Town Square database from the New River Valley area with topical metadata. We took a database of approximately 360,000 tweets and 15,000 RSS news stories collected in the last two years and associated each RSS story and tweet with topics. The open-source natural language processing library Mallet was used to perform topical modeling on the data using Latent Dirichlet Allocation, which was then used to create a Solr instance of searchable tweets and news stories. Topical modeling was not done around specific events, instead the entire tweet data (and entire RSS data) was used as the corpus. The tweet data was analyzed separately from the RSS stories, so the generated topics are specific to each dataset. This report details the methodology used in our work in the Methodology section and contains a detailed Developer’s Guide and User’s Guide so that others may continue our work.	The client was satisfied with the outcome of this project as, even though tweets have generally been considered too short to be run through a topical modeling process, we generated topics for each tweet that appear to be relevant and accurate.','[\"nlp\", \"natural language processing\", \"lda\", \"latent dirichlet allocation\", \"mallet\", \"open source\", \"tweets\", \"rss\", \"nrv\", \"new river valley\", \"blacksburg\", \"IDEAL\"]','N/A',5,'This collection contains the source code, programs, documentation, and example data used in the project. Please review the \"Final Report and Technical Manual\" for a comprehensive overview of the project. The open source library Mallet was used and is referenced here: McCallum, Andrew Kachites.  \"MALLET: A Machine Learning for Language Toolkit.\"\n    http://mallet.cs.umass.edu. 2002.',NULL,NULL,NULL,NULL,NULL),(251,'Russian Flu Capstone Project','This report includes a detailed description of Dr. Ewing’s Russian Flu project.  This project integrated two groups:  a Translation team and a CS team.  The translation team worked to find and translate historical articles documenting the Russian Flu pandemic from 1889 to 1890.  The languages consisted of French, German, Spanish, English and Russian.  The CS team indexed the article metadata into a searchable website.  The desired website allowed a user to search for articles, view a list of corresponding results, and understand the specifics of a given article.\nThe website was implemented using Solr, an open-source search engine platform, and Blacklight, a Ruby on Rails gem.  Solr’s search features include facets, relevance definitions, spell checking, and synonyms.  Blacklight was easily integrated with Solr, in that it displayed the search features and results in a user-friendly format.  Blacklight was configured with a faceted search, drop-down menu search, full-text search box, and spell checker.  The facets defined for the Russian Flu project included Newspaper Title, Infection Location, Reporting Location, Language, Date, and Keywords.  Once a facet is specified, a search is executed, or a combination of the two occurs. Then a list of corresponding results is displayed.  The metadata associated with each article can be viewed as a single document through clicking the Newspaper Title link.\nThe website also includes multimedia resources tracking the Russian Flu pandemic.  An interactive timeline depicts a world map with the flu outbreak (and spread) at different time intervals.  Another multimedia resource is the Google Earth 1889 Russian Flu map overlay.  This interactive Google Map allows a user to see all continents and the flu’s impact through an 1889 world map.\nThe resulting website is:http://russianflu.lib.vt.edu','[\"russian flu\", \"influenza\", \"translation\", \"pandemic\", \"solr\", \"blacklight\", \"final report\", \"midterm presentation\", \"final presentation\"]','N/A',5,'CS 4624 Russian Flu Capstone Project',NULL,NULL,NULL,NULL,NULL),(252,'VT Web Archive Project','VTWebArchive is a project to archive, organize, and make available to the public, historical back-versions of content hosted onvt.edudomains.  This system incorporates several open source software packages to design a publicly utilizable tool for searching and discovering historical versions of content hosted on Virginia Tech websites. These tools include Heritrix, a highly customizable spider and crawler, as well as the Apache Tomcat webserver system and the Wayback Machine front-end.','[\"Archive\", \"Internet archive\", \"Heritrix\", \"Wayback\", \"Crawl\", \"Crawler\", \"wayback machine\", \"WARC\", \"Website archive\", \"vt.edu\", \"IDEAL\", \"Qatar\"]','N/A',5,'In addition to the report and presentation files, included in this repository is a Heritrix configuration file, \'Heritrix Configuration.xml\'. This file contains a customized configuration for crawling the VT.edu domain.\n\nSupport has been provided through:\n1) Virginia Tech\'s Information Technology organization;\n2) Qatar National Research Fund Project No. NPRP 4-029-1-007;\n3) NSF IIS - 1319578: Integrated Digital Event Archiving and Library (IDEAL)',NULL,NULL,NULL,NULL,NULL),(253,'KindaRight','Concept:\nKindaRight is a collaborative, open network for artists of all disciplines. We emphasize collaboration between artists of all disciplines because we firmly believe that art comes from inspiration, and inspiration comes from people. The more you know and the more you see will help artists produce better, more beautiful pieces of art. We believe that the best people qualified to critique art are artists themselves, which is why KindaRight also revolves around a merit system which shows your status as an artist weighted for both how many people have liked you, and the respective merit of those people. Finally, we are first and foremost a network: for connecting those creating art to those buying art; to discover new art and new talent; and where the entire art community can work together.\nCurrent Status:\nAs of the end of this semester we have implemented a full user experience for uploading and sharing photographs. We plan to continue this project and implement a design that is more closely related to our vision. We have included various milestone markers including our midterm and final presentations that detail our status at those points in time respectively. We also included our poster from VTURCS which gives a good overall description of our project and where our future works will be focused. Lastly we have included our final report which is a comprehensive documentation of everything that we have built this semester.\nEventually, our website will be open:kindaright.com','[\"Collaborative network for artists and patrons\", \"website\"]','N/A',5,'Collaborative network for artists and patrons',NULL,NULL,NULL,NULL,NULL),(254,'4624S14DSpaceEmbargo','DSpace [1] is an open source repository application used by many organizations and institutions. It provides a way to access and manage all kinds of digital documents. The 4624S14DSpaceEmbargo project was intended to extend the functionality of the ItemImport command line tool. Specifically the goal was to add the ability to embargo uploaded items until a specified date. This functionality was already implemented for the two web interfaces (XMLUI and JSPUI). DSpace is used by the Virginia Tech library in the form of VTechWorks [2].\nThe project was overseen initially by Keith Gilbertson and Zhiwu Xie who work for the Virginia Tech library. Near the end of the semester we were introduced to another software developer for the library, Jay Chen. We helped Jay set up the DSpace environment on his local computer and demonstrated to him how to use the ItemImport command line tool.\nEmbargoes are used to limit access until a specified date. An embargo can be applied as a resource policy on an item, group, or bitstream level. An item level embargo restricts access to all of the files uploaded for a particular item. A group level embargoes submissions from anyone that is a member of the specified group. By default, the Anonymous group is the group that is used. A bitstream level embargo restricts access only on a specific file that is uploaded. The date format expected for setting an embargo must adhere to the ISO 8601 date format [3], specifically the YYYY-MM-DD, YYYY-MM, and YYYY variations.\nThe deliverables for this project were the source and this documentation. Source code will be available on VTechWorks as well as GitHub. The GitHub repository [4] will be more up to date than the VTechWorks copy because we will continue some work on the project after the due date for this project based on feedback from the DSpace developers. The JIRA ticket for this feature to be implemented in DSpace 5.0 is DS-1996 [5].\n[1] DuraSpace, “DSpace”, 2014,http://dspace.org/[2] Virginia Tech, “VTechWorks”, 2014,http://vtechworks.lib.vt.edu/[3] ISO, “Date and time format - ISO 8601,” 2014,http://www.iso.org/iso/home/standards/iso8601.htm[4] GitHub, “jebschiefer/DSpace,” 2014,https://github.com/jebschiefer/DSpace/[5] DuraSpace JIRA, “[DS-1996] Embargo Support in ItemImport,” 2014,https://jira.duraspace.org/browse/DS-1996','[\"dspace\", \"CS4624\"]','N/A',5,'dspace',NULL,NULL,NULL,NULL,NULL),(255,'Video Accessibility','We worked with Keith Gilbertson to come up with a way to make the videos on VTechWorks more accessible. Not accessible in the sense of where the videos are stored but accessible for searching for the videos and assistance while watching them. Initially, Keith wanted us to focus on both the ability to make the video full-text searchable and to add captions to assist with viewing. They wanted to make the videos easy to find and easy to watch if the video had tough accents or the viewer had a disability. He also suggested used CMU’s Sphinx software, a software that was developed to automatically transcribe videos to a text document. We also worked with Therese Walters of the VT Event Capture Group and Paul Mathers, a UNIX Systems Administrator with the library. Both of whom provided us valuable information and suggestions to move forward and complete this project.\nAfter doing research on the currently available software for manual and automatic transcription and for captioning, we determined that captioning was far beyond the scope of this class project. Many of the automatic transcription tools we found were extremely poor at correctly identifying the words in a video and anything used for captioning was even worse. We even tested YouTube’s captioning and the accuracy was quite low. In the end we decided that working on a process for manual transcription and using Adobe Premiere for automatic transcription; captioning would have to wait for the technology to catch up.\nFor manual transcription we used a standard word document and wrote each word as we heard it. This process proved to take far longer than we anticipated for each video that was transcribed. The automatic transcription was run through Adobe Premiere and proved to be much more time consuming that we thought it would be as well. Typically, it seems to take just as much time to manually transcribe a video as it does to automatically transcribe a video. There appears to be  a large separation between what a computer can process in the english language and what the human brain is able to interpret. There are also many other factors that go into transcribing a video, manually or automatically, which are discussed in further detail below.\nIn the end, there is no easy way to transcribe video, at least that is readily available. However, we believe the processes we researched can still be of great benefit to the library and the VTechWorks groups. We can further shift the focus of the project to focusing more on just the keywords than on transcribing the text word for word. This allows videos to be fully searchable and easily located on the VTechWorks site and can prevent matching the filler words; such as, the, and, it, and so on.\nWe want to thank Dr. Fox, Therese, Paul, and especially Keith for the guidance, support, and cookies given during the project. Below we explain in detail how to use and setup Adobe Premiere, the results of manual transcription research, and what we learned from the project.','[\"transcription\", \"speech-to-text\", \"transcription maker\"]','N/A',5,'transcription',NULL,NULL,NULL,NULL,NULL),(256,'BTDImporter','The BTD Importer is used to importer Bound Thesis Dissertations to the Electronic Thesis Dissertation Database. The process involves taking a hard copy thesis and scanning it into PDF form. Once in PDF form, the importer script would locate the new PDF and extract its library call number, which is located in the PDF file’s name. Using the call number, the importer script would fetch the metadata of the thesis, such as title and author, by scraping the metadata using AirPAC Classic. The PDF would then be uploaded to the ETD database along with its metadata.\nThe BTD Importer deliverables listed a new importer script that would take new PDFs and look up their metadata using the Sierra APIs to access Addison directly, then taking that metadata and constructing an XML file containing the data. The script would then move the PDF and the new XML file to a new output file structure, which would later be read, by another section of new the importer process. That final section would then upload the PDF and XML file to VTechWorks. The project would require PHP skills which Nathanael had and SQL skills which Adam and Scott had knowledge of, so our group felt like we could complete the project satisfactorily. The project spec also listed the project as being high impact as our work would be used to import roughly 13,000 BTDs into VTechWorks.\nWe completed the project by splitting up the work amongst the group and meeting weekly to discuss milestones and our next goals. We decided to stick with using PHP, as that was what the original importer script was written in. The PHP libraries made it very straightforward to construct an XML file and a directory structure.','[\"PHP\", \"XML\", \"Shell Script\", \"DSpace\"]','N/A',5,'This is a project to process bound theses PDF\'s for importing into the VTechWorks system.  The process scans a set of given directories and creates XML files containing metadata for each file in a directory structure set up for importation into the VTechWorks system.',NULL,NULL,NULL,NULL,NULL),(257,'Online VT CS Module: Unity Crash Course for CS 4624','America’s entertainment software industry creates a wide array of computer and video games to meet the demands and tastes of audiences as diverse as our nation’s population. Today’s gamers include millions of Americans of all ages and backgrounds. In fact, more than two-thirds of all American households play games. This vast audience is fueling the growth of this multi-billion dollar industry (Essential Facts About the Computer and Video Game Industry, 2006).\nThe Computer Science Department at Virginia Tech has offered a course to facilitate the future of art and game development. CS 4644: Creative Computing Studio Capstone is an intensive immersion into different approaches to game design and 3D modeling. The course allows students to develop an understanding of the scientific and technological principles associated with the design and development of computer and console games for both entertainment and serious applications. Students are encouraged to use a wide range of game engines as they work in teams to conduct an end-to-end integrative design project, the most popular being Unity.\nUnity is a game development ecosystem: a powerful rendering engine fully integrated with a complete set of intuitive tools and rapid workflows to create interactive 3D content; easy multiplatform publishing; thousands of quality, ready-made assets in the Asset Store; and a knowledge-sharing Community.\nUnity is free to a large proportion of developers and affordable for the rest. For independent developers and studios, Unity’s democratizing ecosystem smashes the time and cost barriers to creating uniquely beautiful games. They are using Unity to build a livelihood doing what they love: creating games that hook and delight players on any platform. It is for this reason that our group decided to work with the professors of the Creative Computer Studio Capstone to deliver a module that will quickly get students up and running with Unity game development.\nVideos are publicly available through the YouTube playlist:\nhttp://m.youtube.com/playlist?list=PLKFvhfT4QOqlEReJ2lSZJk_APVq5sxZ-x\nAll of the code is maintained in the public GitHub repository:\nhttps://github.com/jm991/UnityThirdPersonTutorial?files=1','[\"Unity\", \"computer game\", \"video game\", \"CS4644\", \"Virginia Tech\", \"Creative Computing Studio Capstone\", \"game engine\", \"tutorial\"]','N/A',3,'The purpose of our project is to create a set of online cross-disciplinary tutorials for making games with the Unity game engine. The tutorials will cater to both art and computer science students enrolled in the Creative Computing Studio course. The tutorials also can serve as a guide for anyone interested in game development. Students will gain the knowledge to script game mechanic logic, develop camera devices through which the player views the world, and implement character controllers using preexisting animations through retargeting. By the end of the tutorials students should feel empowered and comfortable enough to begin creating their own games. The course is aimed at an intermediate level to meet the needs of Virginia Tech students who already know how to program or create 3D art, but don’t understand how to apply that knowledge to a game engine. This course is not intended as a step-by-step tutorial series; it takes a troubleshooting-based approach by addressing common pitfalls we encountered taking the gaming capstone in previous semester. Each of the videos can be watched individually to learn the related techniques. Students will learn and apply principles from the course not to a sample file, but to their own game\'s needs. Module Overview Legend of Zelda Wind Waker and Super Mario Galaxy are two examples of innovative platform games that revolutionized the third person cameras in a 3D environment. Both share similar mechanics that are simple, yet extremely robust. This module will recreate the same character locomotion and third person camera used by both games.',NULL,NULL,NULL,NULL,NULL),(258,'SeerSuite at Virginia Tech','Problem Statement:\nA digital library has computer-managed collections stored in digital formats. Although scientists have researched and developed digital libraries since 1991, there has been limited research on multilingual digital libraries.  This project seeks to research the anticipated needs for digital library infrastructure to support multilingual information and how this can best proceed for both Arabic and English digital content. It also will recommend and assemble the necessary tools, SeerSuite and its dependencies, to establish the digital library for the crawled data. We then will display results using a web interface.\nProject Procedures:\n·      The first thing we had to do was to get a working Linux machine running and ready to install SeerSuite. We started with Ubuntu, moved to Red Hat, and after many problems with Java we ultimately switched to CentOS 6.3. Red Hat is recommended by the Seersuite developers, but we did not have a valid license so Java would not work.\n·      After installing the operating system, we then had to configure dependencies for the OS. We installed Java, Perl, and MySQL and configured the variables for system access to these resources.\n·      We then began installing SeerSuite dependencies such as Apache Tomcat, Apache Solr, Apache Ant, and Apache Axis2. These are all required before installing the SeerSuite package.\n·      After all the dependencies were working, and many weeks of fighting with Solr, we began installation of SeerSuite. The installation worked pretty well and we had the web interface, CiteseerX, running then.\n·      The next step was to import pre-parsed data. We tried for many days to get the data imported and searchable, but we could not ultimately get the data to be searchable.\nResults:\n·      We were unable to do much research on multilingual support since we could not get any data imported or searchable.\n·      We have learned that installing SeerSuite is quite difficult and it would have been nice to have Steve Carman help us, but his help was limited and not as frequent as we would have liked. Due to him not replying to emails and us having issue after issue, this installation was much more painful than it should have been.\n·      We had our machine compromised and had to start from scratch towards the end of our project. We then worked day and night for many days and restored the machine to a working state, which shows we have learned how to install these programs, and we have laid out the plans for repeating our work in this document.\nConclusion:\nWith more time, and a responsive contact that is knowledgeable about SeerSuite, we would have been able to complete this installation. We have put a lot of effort into this, and documented all the issues we had and their solutions. With this new documentation, we are confident that future teams will be able to complete our work with more ease.','[\"Crawling\", \"CiteSeerX\", \"Search\", \"Multilingual\", \"Arabic\", \"English\", \"Solr\", \"Axis2\", \"MySQL\", \"Index\", \"Data\", \"Documents\", \"SeerSuite\"]','N/A',3,'SeerSuite is a tool used to index and search documents, that has been developed by Penn State. We were implementing this tool in the hopes to be able to do multilingual research with it, such as indexing documents in Arabic in Qatar and documents in English here, and then searching all of them using either English or Arabic queries. This project was installed on a CentOS machine here at Virginia Tech and was to be ported to a machine in Qatar after we got our installation finalized. This would allow the machines to each work in the same way and be able to index and support searching of multilingual documents and queries.',NULL,NULL,NULL,NULL,NULL),(259,'Focused Crawling','Finding information on the WWW is a difficult and challenging task because of the extremely large volume of content in the WWW. Search engines can be used to facilitate this task, but it is still difficult to cover all the webpages on the WWW and also to provide good results for all types of users and in all contexts. The focused crawling concept has been developed to overcome these difficulties. There are several approaches for developing a focused crawler. Classification-based approaches use classifiers in relevance estimation. Semantic-based approaches use ontologies for domain or topic representation and in relevance estimation. Link analysis approaches use text and link structure information in relevance estimation. The main differences between these approaches are: what policy is taken for crawling, how to represent the topic of interest, and how to estimate the relevance of webpages visited during crawling. We present in this report a modular architecture for focused crawling. We separated the design of the main components of focused crawling into modules to facilitate the exchange and integration of different modules. We present here a classification-based focused crawler prototype based on our modular architecture. We also describe how it can help with a particular event-oriented crawl. Note: Mr. Collins and Mr. Dickerson, in CS4624 in the spring of 2013, extended the prior work by the other co-authors from CS5604, from the fall of 2012.','[\"Information Retrieval\", \"Web Crawling\", \"Web Crawler\"]','Virginia Tech',1,'* FocusedCrawler.py,\nDriver class for this project,\nResponsible for creating configuration and classifier object and calling crawler;\n\n* crawler.py,\nCrawler class responsible for collecting and exploring new URLs to find relevant pages,\nGiven a priority queue and a scoring class with a calculate_score(text) method;\n\n* classifier.py,\nParent class of classifiers (non-VSM) including NaiveBayesClassifier and SVMClassifier,\nContains code for tokenization and vectorization of document text using sklearn,\nChild classes only have to assign self.model;\n\n* config.ini,\nConfiguration file for focused crawler in INI format;\n\n* fcconfig.py,\nClass responsible for reading the configuration file using ConfigParser,\nAdds all configuration options to its internal dictionary (e.g. config[“seedFile”]);\n\n* fcutils.py,\nContains various utility functions relating to reading files and sanitizing/tokenizing text;\n\n* html_files.txt,\nList of local files to act as training/testing set for the classifier (“repository docs”),\nDefault name that can be changed in configuration;\n\n* labels.txt,\n1-to-1 correspondence with the lines of the repository that is assigning numerical categorical labels which are 1 for relevant or 0 for nonrelevant,\nOptional;\n\n* lsiscorer.py,\nSubclass of Scorer representing an LSI vector space model;\n\n* NBClassifier.py,\nSubclass of Classifier representing a Naïve Bayes classifier;\n\n* priorityQueue.py,\nSimple implementation of a priority queue using a heap;\n\n* scorer.py,\nParent class of scorers which are non-classifier models such as (typically) VSM;\n\n* seeds.txt,\nContains URLs to relevant pages for focused crawler to start,\nDefault name that can be modified in config.ini;\n\n* SVMClassifier.py,\nSubclass of Classifier that is representing an SVM classifier;\n\n* tfidfscorer.py,\nSubclass of Scorer that is representing a tf-idf vector space model;\n\n* webpage.py,\nUses BeautifulSoup and NLTK to extract webpage text;\n\n* README.txt,\nDocumentation about the Focused Crawler and its usage.',NULL,NULL,NULL,NULL,NULL),(260,'Port Video and Logo','The goal of this project was to create a logo and video to help promote the Port research space. Port is a digital research commons located on the library side of Torgersen Bridge, in room Torgersen 3320. It has seven computers with a variety of software installed that students might not easily get access to otherwise. The Port space also has a meeting room, appropriate for small consulting sessions and groups. The space can be reserved online through the Virginia Tech Library website. Anyone is free to use Port to utilize the software on the computers. A valid Hokie passport is required to gain entrance to the space.\nIn order to inform and promote Port, Purdom Lindblad, our client, wanted both a logo designed for the space and a video to promote all that Port has to offer. Purdom is one of three library employees in charge of Port. These deliverables are designed and created with the goal of attracting people of all disciplines and levels to use Port, regardless of major or year (senior, freshman, or faculty). The end product promotes Port as a sandbox where one can learn and explore any subject they wish.\nThe logo is designed with the theme of learning in mind and aesthetic appeal. Our client wanted the logo to be easily usable in a variety of situations including a Port website, on flyers, and on notices inside the Port space. Also, two versions of the logo were requested: one full size, used in places such as a banner on a website, and a smaller compact version, used where a logo of square dimensions would be required. The two logos should obviously be similar in design to mesh well when viewed together.\nOver the course of the semester, Stuart was able to design and create a logo that fulfilled these needs. After initial designs were approved by the client, Stuart created the logo in a free vector graphics program, Inkscape. Every few weeks while meeting with the client, we discussed the progress on the logo and got continuous feedback to evolve the logos into what became the final product. Both a small and full size version of the logo were delivered in several formats so that Purdom would have the correct logo for any situation.\nThe video requested was a 3-5 minute promotional video that could be uploaded to YouTube or embedded in the Port website.  Throughout the semester, Stuart and Josh used Innovation Space equipment to film the Port lab, the meeting space, and the surrounding areas in and outside of Torgersen Bridge.  They also collected screen casts of library faculty showcasing software offered in Port.  Josh wrote narration for the video which Stuart recorded in the Innovation Space sound booth.  The video was compiled and edited by Josh using Adobe Premiere Pro CS6 (http://www.adobe.com/products/premiere.html).  Music for the video was retrieved fromhttp://www.jewelbeat.com.  Josh completed several drafts throughout the semester, adding to and changing the video as more footage was captured and feedback from the client was taken into consideration.  The final video is 3 minutes and 49 seconds long and can be viewed athttp://www.youtube.com/watch?v=4QtfpyOC6q0','[\"research\", \"commons\", \"lab\", \"promotional\", \"humanities\", \"digital\", \"purdom\", \"lindblad\"]','N/A',3,'Logo PDF files should be accessible by any PDF reader such as Adobe Reader. SVG files of the logo are vector graphics accessible by programs such as Inkscape or Adobe Illustrator. PNG files are image files of the logo that should be able to be opened by any operating system\'s default image viewer.\n\nThe final report is submitted in both .doc (Microsoft Word) and .pdf formats. The video is submitted in .avi format and can be viewed with Windows Media Player or VLC. Audio .wav files are also accessible through Windows Media Player and VLC.',NULL,NULL,NULL,NULL,NULL),(261,'FlickrIDR: A web-based multimodal search interface based on SuperIDR','In many educational fields, it is important to be able to recognize by sight, such as identifying a fish’s species in wildlife sciences.  To develop these types of skills, it is important to be able to look at databases of images pertaining to the topic of study and to be able to examine relationships of images.  For example, if one is doing an assignment on identifying a fish’s species by its picture, it may be useful to be able to search for images similar to the one given, taking note of certain unique aspects. FlickrIDR allows a user to specify a Flickr Image Group pertaining to their field of interest and to search through that group for images, for those similar to a given image and/or text.  The goal of this project is to develop a web-based interface and system to enable multi-modal (image and annotation) search on Flickr image collections. The project will build upon the SuperIDR prototype and will provide SuperIDR’s functionality, focusing on search, in a web-based environment.The project was separated into phases. The first phase consisted of doing research regarding the Flickr API and Flickr groups.  In this phase, the Flickr API was used to gather information about Flickr groups including total number of images, number of images with annotations, and average number of annotations per image.  The second phase involved creating a website as well as enabling searching of the Flickr SuperIDR image group.  This was the most time and effort-intensive portion, as the website had to be designed and implemented as well as the ability to search a Flickr image group.  In the third phase, the Flickr image group search capability was to be generalized.  Whereas the website in phase 2 could search only the SuperIDR image group, the goal of phase 3 was to be able to indicate any Flickr image group for indexing so that any indexed group would be used in searches.  As stretch goals, the project was to be released as open source, and with any extra time, research would be done on improving search algorithms.\nThe team was able to complete effectively the 3 primary phases, with some work still left to be done.  Phase 1 was completely finished on-schedule after only a few weeks.  Phase 2 was completed, though later than anticipated, and also the UI designed and developed in this stage was inadequate for the functionality to be provided in phase 3.  The web development proved to be quite difficult due to the need to use javascript to implement subimage selection capabilities, which are still not fully functional.  Phase 3 further required of the UI that the user be able to specify a Flickr image group for indexing.  This change to the UI was not implemented.  The functionality to accomplish this task exists in the code base, however it will need to be integrated with the UI to allow the user to dynamically select groups for indexing. The general functionality of indexing a Flickr image group beyond the SuperIDR image group is complete.  However, the ability for the user to specify that image group was not completely provided.\nThe client’s goal was sufficiently fulfilled; however there is still a significant amount of work left to be completed in the future.  For example, due to unexpected difficulties in learning new technologies during phase 2, the schedule had to be altered, leaving no time to create a sufficient automated test suite.  The subimage selection in the user interface requires further work, as does image selection via URL.  The UI and backend functionality need to be updated to enable the user to specify Flickr groups for indexing, though the backend code does have the capability to index Flickr groups.  It also may be interesting to consider updating indexed Flickr groups and recognizing groups which have already been indexed to save time, as the indexing process is very time and bandwidth intensive.  This project serves as a proof of concept and base for further development in creating a tool which allows users the kind of multi-modal search capabilities available in the SuperIDR in an easy-to-use and generic web interface.','[\"Flickr\", \"multi-modal\", \"multimodal\", \"search\", \"image\", \"annotation\", \"SuperIDR\"]','N/A',3,'The goal of this project is to develop a web-based interface and system to enable multi-modal (image and annotation) search on Flickr image collections. The project will build upon the SuperIDR prototype and will provide SuperIDR’s functionality, focusing on search, in a web-based environment.',NULL,NULL,NULL,NULL,NULL),(262,'CS4624: Environment - Virginia Water Resources Research Center (VWRRC) PDF Documents to VTechWorks','Virginia Tech has many groups engaged in work related to the environment. In an effort to alleviate server strain for the Virginia Water Resources Research Center (VWRRC), we have begun to archive over 300 PDF documents into VTechWorks. This will make more than five decades of Virginia Tech’s water research more searchable and accessible than ever before. This permanent archive supports searching and browsing by issue date, author, title, subject, series, and more. It may lead to other efforts in support of the College of Natural Resources and Environment.','[\"Water\", \"links\", \"vwrrc\", \"pdf conversion\", \"jsoup\", \"opencloud\", \"tag cloud\", \"html parsing\", \"resources\"]','N/A',4,'This submission describes our efforts in moving over 300 VWRRC PDF documents to VTechWorks. We employed mostly Java code to do this, using the third-party libraries OpenCloud and JSoup for metadata tagging and procurement, respectively. Additionally, PDFBox by Apache was used to pull textual information out of PDF documents dating back to the 1970\'s.',NULL,NULL,NULL,NULL,NULL),(263,'Virginia eSports Association Video Project','The Virginia eSports Association (VeSPA) held a League of Legends tournament from January 27 - February 12, 2013 called The Hokie Grail. The tournament consisted of 24 teams, and over 75 games were played with over $1000 cash prizes and raffles. This project group set out to use both footage from the live event as well as in-game footage in order to promote the organization. The promotional material generated by the project is meant for two distinct audiences. The first is the collection of the companies that VeSPA contacts in order to obtain sponsorships for future events. To appeal to this audience the project group created a promotional video consisting of footage from the live event. This footage highlights the sponsors for the Hokie Grail and shows attendees interacting with resources provided by those sponsors. The second audience is the gamers who the organization hopes will attend future events. The live event promotional video achieves this goal to some degree by exciting players, but a top 10 plays video also was created to better appeal to this audience. The top 10 plays video consists of in-game footage that highlights some of the best players who competed in the tournament. This is meant to excite and inspire gamers, as well as provide a forum for high level play to be recognized.','[\"league of legends\", \"vespa\", \"virginia esports association\", \"video\", \"video games\", \"hokie grail\", \"esports\", \"game\", \"gaming\", \"tournament\", \"promotional video\", \"top 10 plays\"]','YouTube',4,'This archive contains the two videos that were the deliverables for this project, with music that is licensed under the Creative Commons license. One video file is the promotional video used by VeSPA to get sponsors for future events, containing footage from the live event. The other is the highlight reel video used to excite players about future tournaments, using in-game footage. The additional files are archives of the iMovie projects used to create the deliverables. The projects with the suffix CC are for the videos with Creative Commons licensed music, while the ones without the CC suffix are the projects without Creative Commons licensed music.',NULL,NULL,NULL,NULL,NULL),(264,'Mobile Tickets Dashboard','The Virginia Tech Network Infrastructure and Services Field Engineers play a vital role in the school and community. They are the ones that respond to network and telephone trouble calls. Since the majority of their work is done on the go, it is important for them to be able to communicate with the rest of their team efficiently. It is much easier to send updates by way of a smart phone or tablet rather than carrying a laptop everywhere. Since the engineers would be utilizing touch screens it is essential that they have an interface that is easy to interact with, hence the need for a mobile web application.\nThe utilization of JIRA in the mobile web application was implemented by using the JIRA REST API version 4.3.3. The development of the back-end was handled using Python2 and Flask. Using both the language and framework, respectively, all necessary integrations were met. The front-end of the mobile dashboard was done using Twitter’s Bootstrap. This front-end framework was essential in meeting the design expectations such as menu driven lists and forms. In addition, Bootstrap provides a simple way for the front-end to be responsive to interface changes, which was important for our front-end. The capacity to provide aspect support for multiple resolutions was vital for the mobile web application because of its ability to be run on the web and be utilized via a tablet, smartphone, or laptop. Due to the breadth of devices that can be used, the dashboard must cater to each type of user. The user interface of the dashboard is supplied with large buttons and large text to be easily selected and read, respectively.','[\"jira\", \"application\", \"web\", \"bootstrap\", \"python\", \"html\"]','N/A',4,'The files included in this submission include a zip archive of all of the source files for the web application. When extracted, the resulting folder includes a runserver.py file for testing purposes and a mobiletickets folder where everything else is held. Inside this mobiletickets folder there is a folder called static, which holds all of the front-end bootstrap files. This same directory includes a template folder where all of the HTML files are kept. Alongside these folders are all of the Python files needed for the back-end. Please make use of the README file.',NULL,NULL,NULL,NULL,NULL),(265,'CTRimages','CTRnet (Crisis, Tragedy, and Recovery network) is an NSF funded project that focuses on crawling/scanning the Internet regarding tragic events and creating digital libraries of information on those crises. CTRnet downloads webpages in regards to these events to ensure that this information is saved. As an example, CTRnet has over 440 gigabytes of webpages saved just for the Hurricane Sandy event.\nOur group was assigned with creating a script to walk through the downloaded webpages, finding relevant images, and downloading them. We also researched gallery modules to create a Drupal gallery for our downloaded images.','[\"python script\", \"image parsing\", \"image filtering\", \"CTR\", \"Drupal gallery\", \"Crisis, Tragedy, and Recovery Network Project\"]','N/A',4,'parse_images.py – a Python script that finds all URLs inside of HTML image tags and creates a text document with URLs and another with ALT tags.\n\nbannedUrls.txt – a list of URLs from which no images will be downloaded.\n\nctrfilter – a bash file that runs the script on all .html and .htm files in the current directory and its subdirectories.\n\nfilter_images.py – a Python script that filters our URLs for download based on banned URLs, image dimensions, ALT tags, and file types. It also downloads the images into a specified folder. \n\nCS4624_Documentation.docx – documentation for the project.\n\nImageProperties.xlsx – Excel spreadsheet that has information on all images on the group of webpages we were provided with.\n\nFinalPresentation.pptx – the final presentation given during class.',NULL,NULL,NULL,NULL,NULL),(266,'LucidWorks Vectorize Module for the Digital Library Curriculum Initiative','The goal of our project was to create a learning module for students who are interested in converting a large number of documents of data into a usable form for machine learning, information retrieval, and related purposes. In order to complete this task, we wrote a module that gives information about how LucidWorks Big Data software handles the task of vectorizing documents using a workflow. This module details the approach that LucidWorks implements, and gives detailed instructions on how to create a collection, start the workflow, check the status of the workflow, and finally access the results after the workflow completes.\nUpon completion of our module, users will be able to test their understanding using the example documents provided by the LucidWorks software, and be familiar with Hadoop’s distributed file system. After users are familiar with how the software works, they will be able to create their own vectorized representations of documents. Our module also provides information about the installation of LucidWorks software on a virtual machine; if the users have no access to the software they will then be able to create their own instance of it.\nThe module will be available also throughhttp://en.wikiversity.org/wiki/Curriculum_on_Digital_Libraries.','[\"LucidWorks\", \"vectorize\", \"module\", \"workflow\"]','N/A',4,'This is a module for students wishing to learn about the LucidWorks Vectorize workflow. It walks them through the steps to use this tool and teaches them the purpose of the vector space model. There are also suggestions for discussion upon completion of the module. The LucidWorks Overview Module should be completed prior to starting this module.\n\nThe ModuleIntro.(pdf/pptx/mp4) is an introduction to the module. It gives an overview of the concepts as well as learning goals for students.\n\nThe FinalReport.pdf is an informative piece about the creation of this module. It includes user and developer manuals, as well as descriptions of some of the problems we encountered.\n\nThe VectorizeModule.(pdf/docx) is the LucidWorks module itself. It includes all of the information concerning the module, as well as prerequisites and references.\n\nUser\'s should either watch the introductory video or PowerPoint prior to starting on the Vectorize Module pdf file.',NULL,NULL,NULL,NULL,NULL),(267,'Database Creation and Information Extraction from ETDs for CRA-E','This project was in support of the educational activities of the Computing Research Association (CRA-E). The main point of the project was to collect data associated with electronic theses and dissertations (ETDs) to allow determination of why graduate students in computing go into computing research. The deliverables include a database of the data extracted from the ETDs analyzed and a framework for machine learning and manual approaches to this data extraction.\nTo accomplish these objectives, ETDs from North Carolina State University (NCSU), Florida State University (FSU), Auburn University (AU), Wake Forest University (WFU), and Virginia Tech (VT) were analyzed and results were inserted into the database. The Extensible Markup Language (XML) was decided upon as the structuring format for the data extracted from ETDs, and a tag structure was created utilizing biographical, educational, and institutional data from each ETD. Some of the tags included: author name, title of the paper, year published, undergraduate institution of the author, etc. XML was chosen because of its prevalence in the ETD field, its structural properties, and its ease of use. These tags were used to create the attributes for each entry in the database in Microsoft Access. Access was chosen mostly because of convenience and easy porting of tags into the system. However, the database could be moved into another system quite easily. Challenges that arose included missing data or insufficient information in various areas.\nThe second deliverable took the form of instructions (pg. 4 in the report) that could be given to an Amazon Mechanical Turk user in how to extract information. These instructions were created and provided in order to increase speed and decrease errors in manual data extraction. It was found that the basic structure of most ETDs is similar and is normally in this approximate order (dependent on institution of origin): title page, table of contents, abstract, actual content, biography, acknowledgements, and resume (not normally present). In these, all but the table of contents and the paper itself contains required information for the database. The instructions provide the most common locations for each tag/attribute and alternate locations (if any were found). They also instruct the Mechanical Turk user what to do in case of missing data for each attribute.','[\"Amazon Mechanical Turk\", \"CRA\", \"Microsoft Access\", \"Machine learning\", \"XML\", \"ETD\", \"Thesis\", \"Dissertation\", \"Database\", \"Computing\", \"Research\", \"Joseph Luke\", \"Lamont Banks\"]','N/A',4,'Microsoft Access 2010 or 2013 is needed to view the database (*.accdb). An XML file is also provided (*.xml); this can be viewed in Notepad or an equivalent program. The Microsoft Word document (*.docx) can be opened in Microsoft Word 2013. All *.pdf files can be opened with Adobe Reader X and higher. Microsoft PowerPoint 2013 is needed for the *.pptx file.',NULL,NULL,NULL,NULL,NULL),(268,'Project MELT: The Middle English Translator','Project MELT is the Middle English Language Translator. For this project, our team created a functional, responsive, web-based application that converts Middle English (or English) text into a .wav file with correctly pronounced Middle English audio. A part of this project was the creation of a Middle English phemones table that uses machine learning components to \"train\" a server how to pronounce different terms in Middle English. In the end, we have found that we created a completely modular language system that can be ported to other languages to re-create and archive them digitally forever.','[\"english\", \"middle\", \"project\", \"melt\", \"M.E.L.T\", \"translator\", \"text-to-speech\", \"audiobook\", \"audio book\", \"speech\"]','Published to the internet on website',4,'To view the project or for more information, please visit http://project-melt.org. This project demonstrated the ability for portable digital audio archival and recreation. For this specific project, the Middle English language was used as an example. We were able to create a full audio transmission from any text file. The project uses 100% Unix sources such as espeak from CMU.',NULL,NULL,NULL,NULL,NULL),(269,'IDRgeneralization:  Music Appreciation','When instructors teach courses, they break up the material into components. The students need to fully understand these components in order to understand the entire course. This concept led Uma Murthy to create the SuperIDR. The SuperIDR is a tool that was formatted for Wildlife Sciences. It allows users to search for a species of fish, annotate selected parts of the fish, and compare annotations of two different species of fish. This tool improved students’ in Wildlife Sciences ability to learn. We sought to generalize the SuperIDR for other disciplines in order to improve more students\' learning experiences.\nThe discipline we focused on was Music Appreciation. There are many components to Music Appreciation:  scores, audio, and historical context. To ensure that students have a grasp on all of Music Appreciation’s components, we enhanced the annotations feature and added a feature to handle audio files. The annotations feature now handles different types of annotations, and the “Song Details Form” has the ability to play an audio file. The rest of SuperIDR’s features remain the same, but have been augmented to handle time periods, composers, and songs as opposed to families of fish, genuses of fish, and species of fish.','[\"learning tool\", \"annotations\", \"song details\"]','N/A',4,'In this submission, we included our copies of our Midterm and Final presentations. The presentations discuss the progress of our project. We also submitted our final document, which features a user manual and a developer manual. We also included a zip file featuring the database and source code need to run our project.',NULL,NULL,NULL,NULL,NULL),(270,'Feeding America Southwest Virginia','As a result of our course work we have made a fully functional website that is ready to be moved over to FASWVA’s host and made live. Additionally, we have completed a Virtual Food Drive. A Virtual Food Drive is a way for the user to experience the act of shopping for food for others rather than donating a flat amount. FASWVA personnel will be able to update content as well as encourage companies and individuals to hold food drives using the Virtual Food Drive.\nThe report discusses that Drupal leads to easier editing and that the Virtual Food Drive will improve the user’s experience while donating. The next step is to perform analytics after the website goes live. Recommendations discussed include:','[\"Drupal\", \"FASWVA\", \"Feeding America\", \"website\"]','N/A',4,'To transfer the original content from the Joomla! site to Drupal, we first had to install the Insert and WYSIWYG Module. That allowed easy modification for editing text and multimedia content. We manually copied page by page the site over to Drupal by using the Drupal forms. We would add content and fill in the appropriate information. We found that for some sites that didn\'t have a lot of multimedia information on it originally we could just copy the entire original site and paste it over to Drupal. Drupal would then be able to read the HTML from the original site. However that worked for a limited amount of pages due to the fact that a lot of the original site had a heavy amount of multimedia on it. One problem we ran into was that the ways Joomla! and Drupal handle embedded  videos are different. For Drupal you can not just add the script tag to the HTML of an existing page. The work-around we did was instead to link the videos to the original source.',NULL,NULL,NULL,NULL,NULL),(271,'VeSPA Website Enhancements and VeSPAbot','For our term project, for our Multimedia/Hypertext/Information Access capstone, our group took the task of revamping the Virginia eSports Association’s website,www.vespa.org.vt.edu.  After looking at their website extensively, we decided the focus of our updates would be on providing better publicity for the organization and making a much more usable interface with a few key features.  VeSPA wanted us to keep the website based on a Wordpress blog to promote simple upkeep and updates.  In addition, they wanted us to create a “Bot” to help with the chat monitoring and manipulation during streaming sessions.','[\"IRC bot\", \"WordPress\", \"website  redesign\", \"chat\", \"moduler\", \"forum\", \"Twitch Youtube Player\", \"WordPress membership\"]','N/A',4,'Technical report and accompanying files for our project to update the VeSPA website and create the VeSPAbot.',NULL,NULL,NULL,NULL,NULL),(272,'“More Hack, Less Yak” an Oral History of the Digital Humanities','The purpose of this project was to create a short promotional video advocating for the Digital Humanities. The video is intended to not only shed light on the advent of Digital Humanities but also demonstrate to viewers how they could become involved and also add to a chain of videos by Digital Humanities professionals documenting this technology and its benefits to the humanities. We produced this promotional video by interviewing three individuals in the Digital Humanities to whom our contact Purdom Lindblad has introduced us. Upon completion of the interviews, we compiled the footage and produced the promotional video highlighting on the topics that seemed most relevant. The proliferation of Digital technologies has allowed for easier information retrieval and better investigative techniques.','[\"Digital\", \"Oral\", \"Humanities\", \"Video\", \"Digitization\", \"Archieve\", \"Archieves\", \"History\", \"Histories\", \"Videos\", \"Interviews\", \"Jim Glanville\", \"James Glanville\", \"Scott Pennington\", \"MATRIX\", \"Gail McMillan\", \"Digital Archieves\", \"Professor Emeritus\"]','N/A',4,'This project includes:\n* Promotional video for the Digital Humanities.\n* Edited Interviews of professionals in the Digital Humanities.\n* Raw videos of Interview footage.\n* Report on the project.\n* PowerPoint about the project.',NULL,NULL,NULL,NULL,NULL),(273,'Giles County Animal Rescue','Giles County Animal Rescue is a volunteer organization located in Giles County, Virginia. This group of volunteers assists the Giles County Animal Shelter in placing animals in homes. They also campaign for awareness of the importance of spay/neuter. Since most of their information is accessed on the web, our client Christine Link-Ownes believes that it is important to have a website that is easy to use and update. For this project, we worked with our client and Giles County Animal Rescue to redesign their website, fix bugs, and add new functionality using Drupal. This included recreating the Giles County Animal Rescue website and adding features such as newsletters and animal statuses.','[\"Drupal\", \"Shelter\", \"Web Design\", \"Giles County\", \"Animals\", \"Video\"]','N/A',4,'This submission contains several files, aside from the final report and the final presentation. Several videos are included, which were used as an extension of the Developer\'s Manual in the final report to help our client use her new site. These files can also be found on the website and may be viewed by administrators. The website can be found at www.gilescountyanimalrescue.org.',NULL,NULL,NULL,NULL,NULL),(274,'Contemplative Video','Contemplative practices are a collection of various techniques that are focused on the betterment of the individual through various self-improvement techniques. There are various forms of contemplative practices such as Tai Chi, Qigong, Yoga, Mindfulness, and Meditation.\nThe goal of the project is to develop a documentary on the Contemplative Practices Conference that happened at the Virginia Tech Conference Center on April 11-13. The extent of our work was to generate the content to make this documentary, edit and publish the interviews, and successfully hand off the content for further development at a later date. So far, the videos have been edited and published on the Contemplative Video YouTube channel, the link to which is posted below, and our professor, Dr. Fox, is in possession of all the content generated by our work, including the raw footage.\nThe YouTube channel featuring our work can be found here:http://www.youtube.com/user/ContempVideo','[\"contemplative\", \"conference\", \"interviews\", \"video\"]','N/A',4,'This project deals with the documentation of the Contemplative Practices Conference that happened at the Virginia Tech Conference Center on April 11th-April 13th. The project involved three distinct phases. The first phase was to gather interviews from various members within the Contemplative Practice field living in the New River Valley region. In this phase, 5 people were interviewed. The second phase of the project was to film the events of the conference, while at the same time gathering interviews from various individuals attending the conference. In total, we have interviewed 18 practitioners. A full listing of the materials including the names of those interviewed can be found in the report. The third phase of the project involved editing the interview footage, and uploading the interviews to YouTube.',NULL,NULL,NULL,NULL,NULL),(275,'CS Seminar Videos','Computer Science seminars are a very educational and interesting aspect that the CS department offers to the students attending the university. There are numerous occasions in which many students are not able to attend the seminars. Taking this into consideration our group was responsible for the digital video recording and editing of the CS Seminar talks. This allows for students to re-watch a seminar if they can’t remember all the information. Each seminar is held on Friday at 11:15am in Torg 2150. We recorded each CS Seminar, edited it, and posted it to the VTechWorks website that is located at this link:http://vtechworks.lib.vt.edu/handle/10919/19036. This project has been an ongoing project as seminars and talks are held every semester. The main purpose, along with what was described above, is to attract more people to Computer Science and support the Computer Science department based on research discussed in the seminars. Having these seminars uploaded onto a website that is easily accessible will generate interest in the computer science field as well as give people already in the field something potentially new and interesting to think about.','[\"CS Seminar\", \"Distinguished Lecture Series\", \"Video Project\", \"Video Editing\", \"Camtasia\", \"iMovie\", \"VTechWorks Uploading\"]','N/A',4,'Main site for our project can be found at this URL: http://vtechworks.lib.vt.edu/handle/10919/19036.  From here you can find videos of all the CS seminars and distinguished lectures given this semester.  Each video has its own abstract and description.  The files attached in this section are a final report in both raw Word Document and archival PDF formats and a presentation in both raw Powerpoint and archival PDF formats.',NULL,NULL,NULL,NULL,NULL),(276,'Food Safety 100 Game','The Virginia Tech Food Safety Game will allow for the users or the new dining employees to learn about food safety in an interactive and more engaging way. The employees will be able to play this game on a personal or a public computer through the use of a browser and familiarize themselves with the food safety material that is necessary to take a food safety exam. The food safety game allows for the training coordinators to make the learning more enjoyable and guarantee food safety in their dining centers. When the game is complete, it will replace the PowerPoint presentation that is currently in place for the new employees to learn about food safety at Virginia Tech.','[\"food safety\", \"food safety 100\", \"game\", \"handwashing\", \"serving\", \"cooking\"]','N/A',4,'In order to teach the users about the importance of food safety in the dining centers, there is a need for a comprehensive training course. However, due to the lack of interactive material, it has become difficult to interest the employees to learn about food safety. At present, the employees do not have access to an interactive way of learning about the importance of food safety. And the material that is available does not aid the employees in helping them remember all the important material. This limits their ability to take proper precautions and/or steps to sustain a healthy environment in the dining centers at Virginia Tech.',NULL,NULL,NULL,NULL,NULL),(277,'Disaster Video Gallery Project','The goal of this project was to collect YouTube videos for carefully selected events. The videos were manually collected and verified to be relevant to the specific events. The collection together with short description included with each video can later be used to automate the process of collecting videos pertaining to past disasters. We hope that the sample video collection presented here can help build a successful model relating metadata with relevance of video.',NULL,'N/A',2,'Project Report.docx\nProject Report.pdf\nProject Presentation.pptx\nProject Presentation.pdf\nSample_YouTube_Videos_Raw.txt\nSample_YouTube_Videos_Readable.txt',NULL,NULL,NULL,NULL,NULL),(278,'Catawba Multimedia Website','The website for the Catawba Sustainability Center (CSC) was in its infancy, and it needed to be expanded with descriptions for onsite land demonstrations, showcases for student and faculty projects, and spotlights of the businesses on site.  The lead content director for the site is Christy Gabbard, and the head of website development is Joe Gabbard','[\"catawba\", \"Sustainability\", \"center\", \"csc\", \"website\"]','N/A',2,'catawba',NULL,NULL,NULL,NULL,NULL),(279,'Save the Penguins','The purpose of this project was to create a promotional video for Studio STEM\'s after school \'Save the Penguins\' program.  This program was created to get middle school aged students interested in critical thinking and performing experiments using the scientific method.  Students would have an ice cube which represented a penguin, and they would construct a house for it out of different materials to protect it from the sun (a heat lamp).  The materials would be tested to see which are the most effective at insulating the house from the heat lamp.  The students would design their house based on data they obtained from experiments, and then further refine their design based on how well it fared in tests.','[\"Save the Penguins\", \"scientific method\", \"after school program\"]','N/A',2,'Save the Penguins',NULL,NULL,NULL,NULL,NULL),(280,'FFMPEG on the IBM Cloud','This module aims to introduce FFMPEG to students in a linux environment (IBM Cloud)','[\"ffmpeg\", \"ibm cloud\"]','N/A',2,'This module aims to introduce FFMPEG to students in a linux environment (IBM Cloud)',NULL,NULL,NULL,NULL,NULL),(281,'Valley Interfaith Child Care Center CMS','The project consisted of revamping Valley Interfaith Child Care Center\'s website to be more modern and feature media. The goal was to cater to two diverse audiences: the families that needed their services and the investors who helped them keep running. This system is the result of efforts to do that.\nTo run this software locally requires: Ruby 1.9.2 or newer, the bundler gem and either SQLite or PostgreSQL. The software can also quite easily be run from a Heroku instance.\nThe website can be viewed in any browser, but the best experience will be had in a recent version of Chrome, Safari or Firefox.','[\"ruby on rails\", \"refinerycms\", \"viccc\", \"cms\"]','N/A',2,'Included files: viccc.zip, viccc2.zip, viccc3.zip, viccc_final_paper.doc.',NULL,NULL,NULL,NULL,NULL),(282,'AnimalRescue',NULL,'[\"GIles County Animal Rescue\"]','N/A',2,'This is Neal Schneier and Zachary Highman\'s submission for our final project.',NULL,NULL,NULL,NULL,NULL),(283,'Dynamic Optimizations of Irregular Applications on Many-core Architectures (CS Seminar Lecture Series)','Enhancing the match between software executions and hardware features is key to computing efficiency in terms of both performance and energy consumption. The match is constantly complicated by emerging architecture features in computing systems and has become a continuously evolving problem. In this talk, I will present some recent findings in the implications of three prominent features of modern systems: the heterogeneity, the rapid growth of processor-level parallelism and the increasingly complex interplay among computing units. In particular, I will focus on how to streamline computations containing dynamic irregularities for General Purpose Graphic Processing Units (GPGPUs), a broadly adopted many-core architecture. The talk will begin with the theoretical foundations of GPGPU program-level transformation techniques, and further describe a runtime optimization system, named G-Streamline, as a unified software solution to irregularities in both memory references and control flows. The system enables on-the-fly elimination of irregularities through adaptive CPU-GPU pipelining and kernel splitting schemes. Working in a holistic fashion, it maximizes whole-program performance by resolving conflicts among optimizations. In the end, I will briefly describe my other work which includes a study of the influence of shared cache on multicore and a new paradigm, named shared-cache-aware optimizations, for parallel software locality enhancement.\nBio:\nZheng (Eddy) Zhang is a PhD candidate at the Computer Science Department of the College of William & Mary. She received her M.S. in Computer Science at William & Mary with a Computational Operations Research (COR) specialization. Her research generally lies in the area of compilers and programming systems, with a focus on revealing and exploiting the implications of emerging hardware features on the development, compilation, and execution of software. She is the lead author of a paper that won the Best Paper Award at PPoPP\'10, and a recipient of a Google Anita Borg Memorial Scholarship.\nThe Computer Science Seminar Lecture Series is a collection of weekly lectures about topics at the forefront of contemporary computer science research, given by speakers knowledgeable in their field of study. These speakers come from a variety of different technical and geographic backgrounds, with many of them traveling from other universities across the globe to come here and share their knowledge. These weekly lectures were recorded with an HD video camera, edited with Apple Final Cut Pro X, and outputted in such a way that the resulting .mp4 video files were economical to store and stream utilizing the university\'s limited bandwidth and disk space resources.','[\"Computer Science Lecture Series\"]','N/A',2,'CS Seminar.pdf - A pdf file further documenting the purpose of the project, and how it was achieved.\n\nMarch 2 Lecture - Broadband Low.mp4 - A video file containing a lecture given by Zheng \"Eddy\" Zhang on Many-Core Architectures. This is one of ten videos in the lecture series; a collection of DVDs containing videos of all of the lectures were given to Dr. Edward Fox, who has agreed to provide access to the videos upon further request at some point in the future.',NULL,NULL,NULL,NULL,NULL),(284,'Boy Scout Medical Record System for Blue Ridge Mountain Council','For this semester project, our team decided to partner with the Boy Scouts of America in Pulaski County.  Our coordinator, Gregory W. Harmon, works for the Boy Scouts and manages all of their camping facilities.  Since they serve over 120,000 users per day, they were looking for ways to improve their medical recording procedures for filing injuries and accidents.  For them, currently everything is written by hand into a log book and supplemented with various forms.  Our project is basically a web-based digitalization of this recording procedure.  This system has one main form that goes into a database.  This main form has the ability to create arbitrary reports with electronic signatures (for legal reasons) as well as the ability to auto populate other form fields.\nThe technologies we used for this project include object-oriented PHP, MySQL, JavaScript, jQuery, phpass, CSS, and HTML5 (appcache/localStorage).  The website that we developed has a home login page.  After the user has successfully logged in with his or her user account information, there are multiple things he or she can do.  The user can create a new user account with user information, delete an existing user, change the password of the currently logged in user, file an injury report (and upload photos of the injury), view previous injury reports, search reports (which can be downloaded and printed), manage backups (manually and automatically), access forms offline, and contact support for help.  Some of the other features of the website include automatic output minification (for CSS, HTML, JavaScript, and fonts), client- and server-side input validation, and robust error handling.\nOur final website solution ended up being 7,141 lines (158 pages) of code long.  Our website is divided up into nine directories (root, backend, backups, css, fonts, form-templates, images, js, and photos), and the code is split up across 55 files.  The root folder contains all of the website views and controllers.  The backend folder contains all of the website models.  The backups folder stores all manual and automatic backups in gzip format.  The css folder stores all CSS.  The fonts folder stores all custom web fonts.  The form-templates folder stores RTF templates for each of the output forms.  A user can easily modify these RTF templates, which have variable placeholders, to change the way the report forms look.  The images folder contains all of the icons and images used by the website.  The js folder stores all of the front-end JavaScript and jQuery code.  The photos folder contains all of the photos that users have uploaded with injury forms.\nOur database stores user account information and injury forms.  We developed and normalized the database design in MySQL Workbench.  We ended up with seventeen tables.  Each injury form is broken up across a series of tables.  A report table stores foreign keys to each of these injury tables.  We managed our tables in phpMyAdmin, a web control panel.  We perform database backups using mysqldump, a binary executable that comes with MySQL.\nTo make the website secure, we used the phpass library, which effectively combats rainbow tables and password crackers by using salted, per-user bcrypt password hashes.  We also prepared SQL queries to prevent SQL injections.  Finally, we sanitized output to prevent cross-site scripting (XSS) attacks.\nOverall, the website we developed provides a nice alternative to the current paper solution that the Blue Ridge Mountain Council is using.  It is our hope that the Blue Ridge Mountain Council can continue to use and modify our system for the years to come.','[\"boy scouts\", \"blue ridge mountain council\", \"medical records\", \"injuries\", \"reports\", \"photo records\"]','N/A',2,'We developed a web site for the Blue Ridge Mountain Council of the Boy Scouts of America.  The website serves as a medical record system.',NULL,NULL,NULL,NULL,NULL),(285,'Upward Bound and Talent Search Video','Upward Bound and Educational Talent Search are both national programs housed at Virginia Tech to encourage and assist high school students in college admission preparation. This project was assigned with the goal of updating and revamping their very first recruiting video for current high school students.','[\"Video\", \"Upward Bound\", \"Talent Search\", \"Recruiting\"]','N/A',2,'Includes a 6:17 minute .mov format video, the final project report and the in-class presentation.',NULL,NULL,NULL,NULL,NULL),(286,'Carilion Case Simulator Project',NULL,NULL,'N/A',2,'N/A',NULL,NULL,NULL,NULL,NULL),(287,'Apple Ridge Farms Corporate Retreat Video','Apple Ridge Farms, a NPO in the Roanoke area, sponsors an academic summer camp for underprivileged youths in the Roanoke area. They also host corporate retreats and other events on their grounds in the off-season. They requested a short video for internet distribution to increase revenue from their corporate retreat portion of their business. We filmed the grounds on April 27th, 2012, and created a video for them using captured video, images, and audio, as well as images they provided.','[\"Video\"]','N/A',2,'Video',NULL,NULL,NULL,NULL,NULL),(288,'Library Projects - Secure Streaming','The main purpose of this project was to find a secure streaming solution for audio files within the VT Library, specifically regarding the recital collection audio files that were donated by the Department of Music.  Within the context of this project, the definition of secure streaming is a method of online streaming that ensures that copyrighted audio files are streamed while respecting copyright law.','[\"Library\", \"HTTP Live Streaming\", \"Secure Streaming\"]','N/A',2,'DLA Library Audio Collection Midterm Update.pptx\nInstructions for HTTP Live Streaming Final.pdf\nM4H01715.MP4\nSecure Streaming Solution for VT Library.pptx',NULL,NULL,NULL,NULL,NULL),(289,'Group1- ADS Photo & Video','The files included showcase our work done on the ADS Photo & Video project that we undertook for the Virginia Tech Adult Day Services organization. We created written and video tutorials for them to assist them in accomplishing their needs for video conferencing, tagging pictures, and sharing files.','[\"ADS Photo\", \"ADS Video\", \"Picasa\", \"DropBox\", \"Skype\", \"Tutorial\", \"Group 1\"]','N/A',2,'CS4624Group1Document.doc, \nGroupMembers.txt, \nDropBoxInstructionTutorial.pdf, \nDropBoxVideoTutorial.avi, \nPicasaInstructionTutorial.pdf, \nPicasaVideoTutorial.avi, \nSkypeInstructionTutorial.pdf, \nSkypeVideoTutorial.avi,',NULL,NULL,NULL,NULL,NULL),(290,'LectureCapture','The following problem was addressed by our project:  How can we easily visualize the content of a body of text without manually analyzing its content?  The initial goal was to be able to visualize captioned college lectures, but ended up being any section of text.\nOur client, Mr. James Barker of Aptigent, supplied us with a large collection of captioned news reports for us to create visualizations.  These television news reports were a good examples for us, since they can usually be summarized with just a few key words and relationships between words.  This obviously makes them optimal for visualizing.\nThere were a few specifics about our task for this project.  We had the ability to use a clustering program which would take a given body of text and generate, among other things, a list of keywords, which we called \'concepts,\' and a list of tags.  The concepts were words that the clustering program believed to have more importance, while the tags were generally words or phrases that were tied directly to one or more concepts.\nOur solution needed to be web-based.  In order to best accomplish this task, we chose to design our solution using HTML and JavaScript. We choose to use Raphael, a JavaScript library, to draw the visualizations.\nOur solution puts a heavy emphasis on the proximity between each concept and tag. Whether or not the two appear in the same sentence is also taken into consideration.','[\"Lecture Capture Cluster Analyze Visualization\"]','N/A',2,'Lecture Capture.docx\ndraw.js\nraphael-min.js\nstyle.css\nwebpage.html',NULL,NULL,NULL,NULL,NULL),(291,'Mathematics Education Recruitment Video','This video was created to help recruit graduate students for the Mathematics Education program at Virginia Tech.','[\"Math Education\"]','N/A',2,'Final version of Mathematics Education recruitment video.',NULL,NULL,NULL,NULL,NULL),(292,'VTechWorks Interface Proposal and Prototype for Media Records','This project is a prototype/proposal for how to present media (particularly) records in VTechWorks. It must be placed on a webserver to run, with its file structure preserved. It provides an AJAX prototype of VTechWorks that demonstrates a number of suggested changes to the interface, including the inclusion of an HTML5 video player, lightbox display of images, and asynchronous loading of data to prevent unnecessary page loads.','[\"vtechworks\", \"video\", \"html5\", \"javascript\", \"prototype\"]','N/A',2,'css\n     -lightbox.css\n     -style.css\n\nfiles\n    -demoDocument.jpg\n    -fullsize.jpg\n    -thumbnail.jpg\n\nimages\n    -bullet.gif\n    -close.gif\n    -closelabel.gif\n    -donate-button.gif\n    -download-icon.gif\n    -image-1.jpg\n    -loading.gif\n    -nextlabel.gif\n    -prevlabel.gif\n    -thumb-1.jpg\n    -video_thumb.jpg\n\n-js\n    -builder.js\n    -communities.js\n    -dynamiccommunity.js\n    -effects.js\n    -expansion.js\n    -lightbox.js\n    -lightbox-web.js\n    -prototype.js\n    -record.js\n    -scriptaculous.js\n    -search.js\n    -searchresults.js\n    -xml.js\n\n-xml\n    -communities.xml\n    -records.xml\n\n-AdvancedSearch.html\n-bg_footer.jpg\n-bg_sp_main_nav.jpg\n-Capture_20120411_2.mp4video.mp4\n-Capture_20120411_2.webmvp8.webm\n-ProtoCommunity.html\n-ProtoMain.html\n-ProtoResults.html\n-ProtoVideoRecord.html\n-sp_header.jpg\n-VTechWorks Interface Proposals and Prototypes.pdf',NULL,NULL,NULL,NULL,NULL),(293,'OpenDSA Mergesort Visualization and Exercise','This is a snapshot of the OpenDSA project taken to exemplify the work done on mergesort-av.html and mergesort-proficiency.html as part of a semester-long project for CS-4624 Multimedia / Hypertext.  More information about OpenDSA can be found athttp://algoviz.org/OpenDSA/and the up-to-date source code is available athttps://github.com/cashaffer/OpenDSA.\nTo view the work completed for CS-4624 please see OpenDSA/AV/mergesort-av.html and OpenDSA/AV/mergesort-proficiency.html.  The help file for the proficiency exercise was also created as part of the semester-long project and can be accessed at OpenDSA/AV/MSprofHelp.html or through the proficiency exercise interface.','[\"Mergesort\", \"OpenDSA\", \"Open source course materials\"]','N/A',2,'The following is a complete list of the files included in this archive:\n\nalgoviz_entry.txt\nbreakid_final_report.doc\nInterviewProtocolMS.docx\nREADME.txt\nUserReview20120406.txt\n\n./Final Presentation:\nalgoviz.png\nbreakid_final_presentation.ppt\nmergesort_exer_1.png\nmergesort_exer_2.png\nmergesort_exer_3.png\nmergesort_exer_4.png\nmergesort_vis_1.png\nmergesort_vis_2.png\nmergesort_vis_3.PNG\n\n./Links:\nAlgoViz.org - The Algorithm Visualization Portal.url\nCreating a Profiency Exercise with JavaScript Algorithm Visualization API (JSAV) Version 0.3.url\nData Structures and Algorithm Analysis Book, 3rd Edition.url\nHow to Think Like a Computer Scientist - How to Think Like a Computer Scientist - Learning with Python - Interactive Edition.url\nJavaScript Algorithm Visualization (JSAV) API documentation.url\nKhan Exercises - GitHub.url\nOpenDSA Project Homepage.url\nThe OpenDSA Developer\'s Getting Started Guide - OpenDSA Active-eBook Project.url\nThe State of the E-Textbook - PCWorld.url\n\n./Midterm Presentation:\nbreakid_midterm_presentation.ppt\nMergesort_visual.PNG\nmergesort_with_pseudocode_and_pseudo-stack.PNG\nThumbs.db\nversion_1.PNG\nversion_2.PNG\nversion_3.PNG\nversion_4.PNG\nversion_5.PNG\nversion_6.PNG\nversion_7.PNG\nversion_7_2.PNG\nversion_7_3.PNG\nworking_version_1.PNG\nworking_version_2.PNG\n\n./OpenDSA:\nAV\nbuild\nDemos\nDoc\nExercises\nJSAV\nlib\nModules\nODSAkhan-exercises\nScripts\nBinaryTreeChapter.pptx\nMakefile\nMIT-license.txt\nmodules.csv\nmodules.json\nREADME.txt\n\n./OpenDSA/AV:\nhashing\nxml\nbinaryheap-buildheap-proficiency.html\nbinaryheap-delete-proficiency.html\nbinaryheap-insert-proficiency.html\nBirthday.html\nheapsort-proficiency.html\nHuffmanCoding.html\nHuffmanCoding-alt.html\ninsertionsort-av.html\nkd-interact.html\nkd-treeAV.html\nmergesort-av.html\nmergesort-proficiency.html\nMSprofHelp.html\nopendsaAV.css\nselectionsort-av.html\nshellsort.html\nShellsortAlt.html\nshellsort-av.html\nShellsortPerformance.html\nShellsortProficiency.html\nSSavHelp.html\nSSprofHelp.html\n\n./OpenDSA/AV/hashing:\nhash.html\nintroduction.html\n\n./OpenDSA/AV/xml:\nbinaryheap-buildheap-proficiency.xml\nbinaryheap-delete-proficiency.xml\nbinaryheap-insert-proficiency.xml\nBirthday.xml\nheapsort-proficiency.xml\nshellsort.xml\nshellsort-av.xml\nShellsortPerformance.xml\nShellsortProficiency.xml\n\n./OpenDSA/build:\nImages\nBinNodeSpace.html\nBinTree.html\nBinTreeExam.html\nBinTreeImp.html\nBinTreeNodeADT.html\nBinTreeTraversal.html\nBST.html\nBST_alt.html\nBubbleSort.html\nCompleteTree.html\nComposite.html\nExchangeSort.html\nFFT.html\nFinalExam.html\nFullTheorem.html\nglossary.html\nHeap.html\nHeapSort.html\nHuffman.html\nHuffProof.html\nImpossible.html\nInsertionSort.html\nKDtree.html\nknowledgemap.html\nLimComp.html\nMergeSort.html\nMIT-license.html\nmodules.csv\nNPComplete.html\nNPCompletePilu.html\nNPCoping.html\nopendsaMOD.css\nOtherSpatial.html\nPRquadtree.html\nQuickSort.html\nRadixSort.html\nReduction.html\nSelectionSort.html\nShellsort.html\nSorting.html\nSortingEmpirical.html\nSortingLowerBound.html\nSpatial.html\n\n./OpenDSA/build/Images:\nBEgraph.png\nBinArray.png\nBinDiff.png\nBinExamp.png\nBinLink.png\nBintree.png\nBlackBox.png\nBST1-tn.png\nBSTAdd.png\nBSTCheckFig.png\nBSTShape.png\nBubSort.png\nCC88x31.png\nComplex.png\ndashboard-icon-active.png\nDecTree.png\nDelMin.png\nDiffNode.png\nexpand-left.png\nexpand-right.png\nfield_1.jpg\nfield_2.jpg\nfield_3.jpg\nfield_4.jpg\nflag.png\nFullComp.png\nFuncBin.png\nFuncDiag.png\nHeapBld.png\nHeapInd.png\nHeapsort.png\nHProof.png\nHuffCode.png\nHuffTree.png\nInCirc.png\nInsSort.png\nKDtree.png\nKDtree2.png\nlight-page-bg.png\nmap-target.png\nMrgSort.png\nnode-challenge-complete.png\nnode-challenge-not-started.png\nnode-challenge-suggested.png\nnode-complete.png\nnode-not-started.png\nnode-review.png\nnode-suggested.png\nnon-repeating-sprites.3.png\nOneLeaf.png\nPairing.png\nPairingBox.png\nPartit.png\nPRexamp.png\nPRinsert.png\nPtQuad.png\nPtrSwap.png\nQsort.png\nRadExamp.png\nRadSort.png\nRemove.png\nSales.png\nSelSort.png\nSiftPic.png\nUnity.png\n\n./OpenDSA/Demos:\nautonodes.html\nsmallnodes.html\n\n./OpenDSA/Doc:\nModuleAuthoring.html\nopendsadocs.css\n\n./OpenDSA/Exercises:\nxml\nHash_displayTable.html\nHashingMC.html\nka-jsav.html\nODSAindex.html\nShellsortMC.html\nShellsortSeries.html\n\n./OpenDSA/Exercises/xml:\nODSAindex.xml\nShellsortMC.xml\nShellsortSeries.xml\n\n./OpenDSA/JSAV:\nbuild\ncss\ndoc\nexamples\nextras\nlib\nsrc\ntest\ntools\nChangelog.txt\nJakefile\nMakefile\nMIT-license.txt\nREADME.txt\n\n./OpenDSA/JSAV/build:\nJSAV.js\nJSAV-min.js\n\n./OpenDSA/JSAV/css:\nimages\nJSAV.css\n\n./OpenDSA/JSAV/css/images:\nbackward.png\nbegin.png\nend.png\nforward.png\nsettings.png\n\n./OpenDSA/JSAV/doc:\napi.html\napidoc.css\nexercise.html\n\n./OpenDSA/JSAV/examples:\nbintree.html\nopendsaAV.css\nShellsortAlt.html\nsimple-exercise.html\nsudoku.html\n\n./OpenDSA/JSAV/extras:\nbinaryheap.js\n\n./OpenDSA/JSAV/lib:\njquery.transform.light.js\nraphael.js\n\n./OpenDSA/JSAV/src:\nanim.js\narray.js\ncode.js\ncore.js\ndatastructures.js\neffects.js\nexercise.js\nfront.js\nfront1.txt\nfront2.txt\ngraphicals.js\nmessages.js\nquestions.js\nsettings.js\ntree.js\nutils.js\nversion.js\nversion.txt\nversion1.txt\nversion2.txt\n\n./OpenDSA/JSAV/test:\nunit\nutils\nindex.html\n\n./OpenDSA/JSAV/test/unit:\nanim.js\narray.js\ncode.js\ncore.js\ngraphicals.js\ntree.js\nutils.js\n\n./OpenDSA/JSAV/test/utils:\narrayutils.js\nqunit.css\nqunit.js\n\n./OpenDSA/JSAV/tools:\nyuicompressor-2.4.6.jar\n\n./OpenDSA/lib:\nImages\nkhan-exercises\nmap\nall_js.js\nkhanacademy.css\nknowledgemap.html\nODSA.js\n\n./OpenDSA/lib/Images:\ndashboard-icon-active.png\nexpand-left.png\nexpand-right.png\nfield_1.jpg\nfield_2.jpg\nfield_3.jpg\nfield_4.jpg\nflag.png\nlight-page-bg.png\nmap-target.png\nnode-challenge-complete.png\nnode-challenge-not-started.png\nnode-challenge-suggested.png\nnode-complete.png\nnode-not-started.png\nnode-review.png\nnode-suggested.png\nnon-repeating-sprites.3.png\n\n./OpenDSA/lib/khan-exercises:\nexercises\n\n./OpenDSA/lib/map:\nfastmarkeroverlay.js\nknowledgemap.js\n\n./OpenDSA/Modules:\nImages\nBinNodeSpace.odsa\nBinTree.odsa\nBinTreeExam.odsa\nBinTreeImp.odsa\nBinTreeNodeADT.odsa\nBinTreeTraversal.odsa\nBST.odsa\nBST_alt.odsa\nBubbleSort.odsa\nCompleteTree.odsa\nComposite.odsa\nExchangeSort.odsa\nFFT.odsa\nFinalExam.odsa\nFooter.txt\nFullTheorem.odsa\nHeader.txt\nHeap.odsa\nHeapSort.odsa\nHuffman.odsa\nHuffProof.odsa\nImpossible.odsa\nInsertionSort.odsa\nKDtree.odsa\nLimComp.odsa\nMergeSort.odsa\nMIT-license.html\nNPComplete.odsa\nNPCompletePilu.odsa\nNPCoping.odsa\nopendsaMOD.css\nOtherSpatial.odsa\nPRquadtree.odsa\nQuickSort.odsa\nRadixSort.odsa\nReduction.odsa\nSelectionSort.odsa\nShellsort.odsa\nSorting.odsa\nSortingEmpirical.odsa\nSortingLowerBound.odsa\nSpatial.odsa\n\n./OpenDSA/Modules/Images:\nBEgraph.png\nBinArray.png\nBinDiff.png\nBinExamp.png\nBinLink.png\nBintree.png\nBlackBox.png\nBST1-tn.png\nBSTAdd.png\nBSTCheckFig.png\nBSTShape.png\nBubSort.png\nCC88x31.png\nComplex.png\nDecTree.png\nDelMin.png\nDiffNode.png\nFullComp.png\nFuncBin.png\nFuncDiag.png\nHeapBld.png\nHeapInd.png\nHeapsort.png\nHProof.png\nHuffCode.png\nHuffTree.png\nInCirc.png\nInsSort.png\nKDtree.png\nKDtree2.png\nMrgSort.png\nOneLeaf.png\nPairing.png\nPairingBox.png\nPartit.png\nPRexamp.png\nPRinsert.png\nPtQuad.png\nPtrSwap.png\nQsort.png\nRadExamp.png\nRadSort.png\nRemove.png\nSales.png\nSelSort.png\nSiftPic.png\nUnity.png\n\n./OpenDSA/ODSAkhan-exercises:\ncss\nexercises\nutils\nKAthJax-77111459c7d82564a705f9c5480e2c88.js\nkhan-exercise.js\nREADME.md\nREADME.txt\n\n./OpenDSA/ODSAkhan-exercises/css:\nimages\nkhan-exercise.css\nkhan-site.css\n\n./OpenDSA/ODSAkhan-exercises/css/images:\ncontent-border.1.png\nface-sad.gif\nface-smiley.gif\nlight-page-bg.png\nnon-repeating-sprites.3.png\nstreak-meter-active.png\nstreak-meter-empty-challenge.png\nstreak-meter-separator.png\nthrobber.gif\nx-repeating-sprites.png\n\n./OpenDSA/ODSAkhan-exercises/exercises:\nkhan-exercise.html\nkhan-site.html\n\n./OpenDSA/ODSAkhan-exercises/utils:\nangles.js\nanswer-types.js\nast.js\ncalculus.js\ncongruence.js\nconvert-values.js\nd3.js\nderivative-intuition.js\nexercise_maker.html\nexponents.js\nexpressions.js\nfunctional.js\ngraphie.js\ngraphie-geometry.js\ngraphie-helpers.js\ngraphie-helpers-arithmetic.js\ngraphie-polygon.js\ninteractive.js\njquery.adhesion.js\njquery.mobile.vmouse.js\nmath.js\nmath-format.js\nmath-model.js\nmean-and-median.js\nparabola-intuition.js\npolynomials.js\nprobability.js\nraphael.js\nscratchpad.js\nslice-clone.js\nspin.js\nstat.js\nsubhints.js\ntime.js\ntmpl.js\nunderscore.js\nunit-circle.js\nword-problems.js\n\n./OpenDSA/Scripts:\npreprocessor.py',NULL,NULL,NULL,NULL,NULL),(294,'Virginia Techniques Gymnastics Promotional Video Project','This project contains seven different videos.  Two for the preschool programs offered, three for the instructional programs, and two for the open gyms.  Each video contains clips of the specified class in action showing what is involved in each class in a hyped manner.  The videos advertise what each class can gives you, and what can be learned in their different classes.','[\"Gymnastics\", \"Virginia Techniques\", \"Agility\", \"Flexibility\", \"Strength\", \"Balance\", \"Coordination\"]','N/A',2,'Gymnastics',NULL,NULL,NULL,NULL,NULL);
/*!40000 ALTER TABLE `projects` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `semesters`
--

DROP TABLE IF EXISTS `semesters`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `semesters` (
  `semester_id` int NOT NULL AUTO_INCREMENT,
  `year` int NOT NULL,
  `semester` varchar(45) NOT NULL,
  `status` varchar(10) DEFAULT NULL,
  PRIMARY KEY (`semester_id`)
) ENGINE=InnoDB AUTO_INCREMENT=39 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `semesters`
--

LOCK TABLES `semesters` WRITE;
/*!40000 ALTER TABLE `semesters` DISABLE KEYS */;
INSERT INTO `semesters` VALUES (1,2012,'Fall','close'),(2,2012,'Spring','close'),(3,2013,'Fall','close'),(4,2013,'Spring','close'),(5,2014,'Spring','close'),(6,2015,'Spring','close'),(7,2016,'Spring','close'),(8,2017,'Spring','close'),(9,2018,'Spring','close'),(10,2019,'Spring','close'),(11,2020,'Spring','close'),(12,2021,'Fall','close'),(13,2021,'Spring','close'),(14,2022,'Fall','close'),(15,2022,'Spring','close'),(16,2023,'Fall','close'),(17,2023,'Spring','close'),(18,2024,'Spring','open');
/*!40000 ALTER TABLE `semesters` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `students`
--

DROP TABLE IF EXISTS `students`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `students` (
  `username` varchar(45) NOT NULL,
  `first_name` varchar(45) NOT NULL,
  `last_name` varchar(45) NOT NULL,
  `password` varchar(100) NOT NULL,
  `project_id` int DEFAULT NULL,
  `semester_id` int NOT NULL,
  PRIMARY KEY (`username`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `students`
--

LOCK TABLES `students` WRITE;
/*!40000 ALTER TABLE `students` DISABLE KEYS */;
INSERT INTO `students` VALUES ('aaron.gomez','Aaron','Gomez','$2b$12$PndZ0MBeH2YwU1BAaCgVjO8l6S/32M/qRrv/I8N9/NTpUQOtc9VMi',25,18),('abishek ajai.satnur','Abishek Ajai','Satnur','$2b$12$4cH/AXD9OBPAxtEexF4tDeweKXpBP4L.lM/E2hi7ya7TXQC6FUlaS',12,18),('adam.kanaan','Adam','Kanaan','$2b$12$zx/tQakD.r1iwRSTZKHln.4Fdp/OqbnDwicplzIABQP6j2cIJ.T0.',1,18),('adharsh.jayaseelan','Adharsh','Jayaseelan','$2b$12$R4Scab/q3BePdrfqendQbOF/7qFGfV18TUO57t3jubE5WwdrIb2D2',53,18),('adi.hanumaiah','Adi','Hanumaiah','$2b$12$8zlkJcNEdy7upq6dxlaPi.X9WMrpVa2lLxq7ehHQFiDgvI1mSNKqu',26,18),('adrian.shirazi','Adrian','Shirazi','$2b$12$zx9CA7I7TP.t7smmjmr7yObtjo8HxmQK41hAkl3UI38jrSyoexJ7u',43,18),('aidan.simms','Aidan','Simms','$2b$12$dX047teNT7KUiPR62VdKquqCb3uImyk4ZBMYvPIsct3awqOtuUrZm',39,18),('akshath.majumder','Akshath','Majumder','$2b$12$dke36H/JmQUH4uq2YOfwcut/0H35EgCkteLx/tO8zclF9aXGQdWwq',21,18),('alan.devera','Alan','Devera','$2b$12$AMpARAzUvww49IhUPcXIeOFPf2uZgoMxtc9fkpFMeiy7Xgsz9c04.',41,18),('alex.issing','Alex','Issing','$2b$12$LBvUkUJqiJk3cAky05Y/WOHRA4YbaOHd6mZ72rIhF7vfXP9sqOJVq',3,18),('alex.lin','Alex','Lin','$2b$12$lgFcohKyTSbP3c4E4eY24Ob7c4ZstbwHJwDppSeEqzL3yLY1.CC0e',35,18),('alex.sigua','Alex','Sigua','$2b$12$NLRpoOPcm6pn6jX1CyM6BeRihlk8sG.iUsY746.n6IBj22PRbZOyG',8,18),('alex.zhang','Alex','Zhang','$2b$12$wR5EYcQovtqxbImkp5zXs.OxF64.M0END4rg61hjEk122WU2ThsJi',30,18),('alexander.georgiev','Alexander','Georgiev','$2b$12$lY7ZUH2o1T4IKqynhx6P/.srFC2YpKC4e9.DkwoI/.dyMqQfcMdqi',32,18),('alfred.premkumar','Alfred','Premkumar','$2b$12$tAk68QyHueL8DZBM645xtuL/DrnOtlVo1YKlpFx.GGKYW42O8yz4q',55,18),('an.truong','An','Truong','$2b$12$n5wfcTVPJ9sFdYY8qN95bub8bh3M69lNWc1tEq1OefbYkbZDW/GJ.',17,18),('andrew.neeser','Andrew','Neeser','$2b$12$khYTOhr8y5VOqCcc4CHzXeTgk2zf/qBSqHfQEpK2/6HNBqwUpdg5S',8,18),('anhtuan vuong.tran','Anhtuan Vuong','Tran','$2b$12$udKoTHj2YrCROn4Yt9O0MunTelqKsCM/ihodgDI.PRKOqIqFbXPFi',13,18),('aniket.sonnakul','Aniket','Sonnakul','$2b$12$y1AZkvcjHnfLrvQSyzI2L.W9sheBTwXOeKHiFTUNCZa7vrCcw.zwS',4,18),('anish.dhondi','Anish','Dhondi','$2b$12$SC3Y7ByAIhWhFFF2fnIX9.VtEppRhOY3n1laBQ7B3vtYdn8oFOrDu',40,18),('anthony.nguyen','Anthony','Nguyen','$2b$12$aIFwkveL2wuPf6wC1.2yPeQE0azl4BdQVsTSljeaGBgb.i/r67XFK',35,18),('anthony.tran','Anthony','Tran','$2b$12$YMbd4Wnq1j5HZiho2nyqaOulHQfVrtFE9nO5Tzsz473uusARzN/Ta',5,18),('antonio.lau','Antonio','Lau','$2b$12$tKJaCHQ3Bd8AvlM.XVVl1uPlMtl0KrVBqUJ6bAQoqDe0.Z4erqRqO',26,18),('aren.waheed','Aren','Waheed','$2b$12$rehsrJ74BMllJzYUIAFZoeoHmlfBEi4dRHl26fYgYmel/sgxDqtU6',20,18),('arjit.singh','Arjit','Singh','$2b$12$Ho3ovkfm.5o92F1CQsI0OeDr54HI9CwuhRVppw0OKv//hzVrnOjR2',28,18),('arjun.vellanki','Arjun','Vellanki','$2b$12$2O81dfpEQfZj/gh8E.p/w.PcR.YYUB1Nnc3Aa6mvjth3b0qxnnUh.',27,18),('bailey.mckelway','Bailey','McKelway','$2b$12$opbaetAGJvlIUqmoyIwquOOFPD1xVkNdk1irq2kZoOfvy5NKfxguq',32,18),('bharathi.ganesan','Bharathi','Ganesan','$2b$12$jIkz/i7F7wa5Pyskl9iev.Gvxb9tbIZrZaKlFM6J16eNxbAVsVg9.',36,18),('bhargava.elavarthi','Bhargava','Elavarthi','$2b$12$da1XJSl83qTKfRkbC1OVV.ksV6OpVRkCF.myekcqqUUPWK1QtZmwm',34,18),('bhavya.patel','Bhavya','Patel','$2b$12$bioAPXnGF6sz/a.7RCGo9e/nQY9YWv7A/8IZU..hVDy/K5JYf/3PK',28,18),('bradley.freedman','Bradley','Freedman','$2b$12$/dXaYefq5X6h20zIRLEwO.FrWuBncPGbL5W9ydEq0SfzE.GlmSG1i',36,18),('brandon.hoang','Brandon','Hoang','$2b$12$ILPQ8w8wT9EucBAtBpvMO.UrztZYfSatCxkwGUgDrraZ.1brE3esC',24,18),('brian.hays','Brian','Hays','$2b$12$822L1Af0iRoNYWZspajw2.k57EdSkXcyP6LI2nRNvSUxZsdUXZKPi',30,18),('brianna.brown','Brianna','Brown','$2b$12$gc6HawSQYoIQEnEczYZaXu.RoCiKaVShS9eWS40o6KmE86N2TRoqq',2,18),('bryan.tran','Bryan','Tran','$2b$12$pkFEzEDTgSBGB.I2fHOKA.xP/r7t2cmVGTOzla.NnXZRCdEFuMFcC',6,18),('caleb.mcirvin','Caleb','McIrvin','$2b$12$4CB9n5e6xZMxLDGE5xLP3.6pN/Z8h1znFx4fzEC8YhRWhHhpADpfa',31,18),('catalina.lemus','Catalina','Lemus','$2b$12$aMCDpvxRpKVY5huPFzWhFeXOSrIo1NASDbraPJU8U7me8AxajUUhy',44,18),('cedric.mohbat','Cedric','Mohbat','$2b$12$1evQlzNejIUy4D4LaEfKKegz6JiE6aS5eLWvavtHxhmyp5pjFkkbm',8,18),('charan.srinivasan','Charan','Srinivasan','$2b$12$NyUqVMQFTiPd/0v6DJq8yOy8gs1PncXi.QqNzCJr36sKvHn9/7aku',55,18),('charles.bruner','Charles','Bruner','$2b$12$8pDI3LJR1FxUrsGxXY00m.mhfluabaTUu40kwg.yBQ5EBGoygc23S',12,18),('chris.lam','Chris','Lam','$2b$12$fpZ64iAD7RiySUf3jvzWTekh50nR02Ju.9WlgHmW2B9bLXRe5xW4W',18,18),('claire.holmes','Claire','Holmes','$2b$12$9YUflhOYr2hbrLRnZVip1u0e5czyxtMxNHtLmxQ7fhX0r13gRN72u',32,18),('corbin.strine','Corbin','Strine','$2b$12$O02gUjWLdw46qY50jJANiOP15kCm7./VpVHtq0xUoQ8yPy3fepOtS',20,18),('daniel.lanigan','Daniel','Lanigan','$2b$12$TNPMMBGc12hzqY7BnIU5IOjktbywluBxoO9zkz/T/pWKCgLSQnYg.',15,18),('daniel.sabanov','Daniel','Sabanov','$2b$12$lRKqaHl1TvH6hpH6Y7q0xO93lVWLC78VQ4u7AciXHp6AWy7vuv70q',57,18),('daniel.shin','Daniel','Shin','$2b$12$NWAR.EwNAuRuHJu7cihYEuoS7sPcecGOz9ySaT4I3O6hsPQesIWKG',24,18),('danny.chung','Danny','Chung','$2b$12$XUPnJyLrjq3/hG3N4v8DnuQB.Vdnok36xr/iadRNnkEa9qsLCxj62',6,18),('danny.pham','Danny','Pham','$2b$12$0D91N471EL5nHJEyIUptoerllRIrRdDgR3wlDcXBDpMn3cVdPv7O6',6,18),('danny.yang','Danny','Yang','$2b$12$AL2lZw7rfRz08nyOUO7EoeIHhcPGU.iJj7F.KwQU3fbIfwwRJUF/S',22,18),('derek.hackenbracht','Derek','Hackenbracht','$2b$12$doHWHIBF7Y5C1LBM92Wrg.WyrBdM1763vGAhQ//yMGtxLYWup5Gze',52,18),('devin.toms','Devin','Toms','$2b$12$igEa5NQzivtR9a3VKlcF0eDoxFfam2LNrvRwGmYRhQpIeXNGYQJz2',57,18),('dhruvil.shah','Dhruvil','Shah','$2b$12$FRKrmCDjnIXkkSf/2izaOu7uXVk.gBxk.ULdc2L9SET79rtQoAYUu',33,18),('dillon.domnick','Dillon','Domnick','$2b$12$2e1CfzFddRRQkvJqpBMcAe6U7PV4XJgoMIRAPQAtslWbk4Q72PKHa',46,18),('dong.xiao','Dong','Xiao','$2b$12$uMoFEQ5z4gQK39E1r4/JpOXNrZUUX.qBPITRTJpOQ4l1T/SK0N22u',28,18),('douglas a. jr..bowman','Douglas A. Jr.','Bowman','$2b$12$BBd7TWdwxVMLk.hsTzy1b.v82bHS3ftwduNiMhpQzXU0H8I.GN73y',56,18),('dylan.harrison','Dylan','Harrison','$2b$12$LjygLBuhvdpIt0o04A/mAeqq9ft9t1WErKavEo/CwuHMR4QWlh9kq',7,18),('earl.carter','Earl','Carter','$2b$12$vdhU7k0uh868XbBGYPAcKe2EUqKaOkSxkCaYdJN9XbOzbN53dGLva',51,18),('egor.lukiyanov','Egor','Lukiyanov','$2b$12$JyQBAANdX97jFdFUu1SpveRe0Dadbut3Vyo5QrDyBcGFPILC2kglS',27,18),('eitsaam.rabbani','Eitsaam','Rabbani','$2b$12$EUwxqNOp3g1Jv7TCRK2f7extqIdrqk6U//s.UOBio8.HAi5mmwyWK',2,18),('eliza.fields','Eliza','Fields','$2b$12$CEvkM9d8QA0gUQeW.xaoNeH2l1q6zVRMSo29MBESwUr/BE9c8p5F6',14,18),('elizabeth.keegan','Elizabeth','Keegan','$2b$12$ffDUXzroA1xRFSm1V8CYsOQC9weHQAyJr4gXb7eYvP/9G5R9QxEz.',41,18),('emmanuel.aninye','Emmanuel','Aninye','$2b$12$DijHsMea1ahY7T/Skci4V.TCOu37iWMSBvl2cbKUZzdXsMC6qlTIS',33,18),('enk.narantsatsralt','Enk','Narantsatsralt','$2b$12$evl41.0Svtrrd/PsUVTTSuN.u52Fdwx1gJqq2dlANMOUf0d7AIfXS',18,18),('eric.wallace','Eric','Wallace','$2b$12$4cSp4peCyPmO6a1/WUU4k.PBUhq5LZpykQ1COT7v6RiY3M6a7C/ny',7,18),('ethan.springs','Ethan','Springs','$2b$12$1AE1/sg11ZcuOWN0eMM21u81gTlPaepiEBxIKyAmwNnVlMQwOzI2S',56,18),('eugene.kwon','Eugene','Kwon','$2b$12$GyDT4qw.clgAQlVUaS1OGeVECTljpBvZJEohTebcofKI4XX9VzQye',26,18),('eunice.hong','Eunice','Hong','$2b$12$wVDw30uHqJWyRf/h8rR4ReMoKYCg4ZTIEVnN3iXeWAeUnB9VIQh6G',6,18),('faizan.moshin','Faizan','Moshin','$2b$12$DJU1dKDfxz9zh.ahLyavi.PqllkhxFVTXdVrYiUFd8e40EXC/l4TS',1,18),('farhan.mohammed','Farhan','Mohammed','$2b$12$A.ojGb0h09aA0ghwTjDEquCKdeAn25p4WNCXGaRVstqS.zzXkq3KW',4,18),('fiona.mustard','Fiona','Mustard','$2b$12$v2WoFvZExfUoFknARgEfzu3xr8z29EBI9WKK1q/c4JTHRxFJ.uuXy',9,18),('francesco.crisafulli','Francesco','Crisafulli','$2b$12$eekFc3HfCV9WMQKGBqoQUeqQG.wW7PLAOccGhjKzP8ktuHq2c4OB.',4,18),('gabriel.shin','Gabriel','Shin','$2b$12$XsVZj06YwzbrdxI7npZsrOMupKiQ03WrmafjMjyvhw/xAIzEJhDOm',24,18),('gabrielle.nguyen','Gabrielle','Nguyen','$2b$12$jS21oPE1F02AlkdEUPfywuC5OQNjWfmK.a19T3gcZpKQj9v6rQgWW',41,18),('geethika.abhilash','Geethika','Abhilash','$2b$12$FDyC3tXHz7Lci19DO3gDseoljBZ5eWuyRpV54ij3YPC4dQ6Abn8Gi',5,18),('hamza.jaldi','Hamza','Jaldi','$2b$12$s5Y5xlJgsJJwNJy8J5BTcev.kIwll87YkLE0Dj0gLbMxk5RT4csym',14,18),('hamza.saif','Hamza','Saif','$2b$12$xtDsWlgFQSXuTsbfHP0MqOGTxHlI1I4rk7JTMAb5AYy2z6uBqjTMi',9,18),('haoran.xing','Haoran','Xing','$2b$12$9x3t2WPuqARcfX09EQtlBOghFhD8dCNpRqlYP3yBYnTKuNnZEI8LW',16,18),('harris.naseh','Harris','Naseh','$2b$12$v.wapiohBiRtEyKohp11H.1JgX28Zc5x8Fivwv3TPdWRvviezVV4a',28,18),('harshil.goel','Harshil','Goel','$2b$12$bgXBBJJWJV7oVwDCid6XtObKOEM1s4p61ea4SJw9EmginqFTUCTQO',40,18),('heewoon.bae','Heewoon','Bae','$2b$12$SOmtm/6QFJoghWk/y6mO2ejNQt2MogemsgprU7v/eL2AOx64rrQDe',11,18),('henry.macht','Henry','Macht','$2b$12$5k.BLoQHeHPWjwZi/pcWF.fO5YtB5X.a0ah904lU9CCRraNRivR7m',43,18),('hiwot.alemu','Hiwot','Alemu','$2b$12$2cX7g6/eTizu6qewxa9XSOrH/WeVFySGeziZQnt3Of2PDv9ZujeRC',7,18),('ilya.mruz','Ilya','Mruz','$2b$12$bACB0yIJWHKxkfEm0MMgGeT0e6C07eWJ.lx2FV8vHZWxbQSdoLZfy',18,18),('jack.golden','Jack','Golden','$2b$12$ssFCMyYffvHwXP8M0xQLnOSNOjqqF13U5C1h2zNmaKYUivWrPWS9S',55,18),('jackson.todd','Jackson','Todd','$2b$12$1batu7mGebvcWupLduXVN.InUmvqHZqY4JYFyzzSBv768S6DnxGAW',54,18),('jae.lee','Jae','Lee','$2b$12$QnhSs7IPnNu3FRnYtGKw5eEcglbSrvw4lVkHUHBm.XRHG26Ch9hqy',49,18),('jakob.ashworth','Jakob','Ashworth','$2b$12$iWNwBxk4BxCNkkKyg7VDHuKDL9cRL9EEOzzNvaeJ0Qc.A24S3scmK',50,18),('james.de chutkowski','James','de Chutkowski','$2b$12$y.VU46BtNiEKjhLNRqUKuucptYjKC4ouxq5DHYNAzCnIAkaugUnQy',5,18),('james.deloach','James','DeLoach','$2b$12$OH11YUdsx10OVWWbOVJnfuy3djKObKd4JI4N/8cpQKjpn0xYqgj7.',33,18),('james.do','James','Do','$2b$12$uXXSHP8YpSodNoIG3S7ohuS2l48x1hZ5i5plc3X.SXERVEakokjhy',11,18),('james.liu','James','Liu','$2b$12$DkxQhq1G0IxcnJNQwoVZueASu9nbgKuUz4UO7XsFcCZOP5pbkgMky',8,18),('jared.melini','Jared','Melini','$2b$12$u6LbwNJuLESYVYKyzQQPZOAGNHflv5jyHJSMNsTLymOxcrXIeGOJ6',25,18),('jason.lao','Jason','Lao','$2b$12$YChYzK9ZpHbo4Buv2Bf9PuUePDbuuj3nM.TsjlLKu44PhfGUY.ChC',57,18),('jiayue.lin','Jiayue','Lin','$2b$12$R5oKRNpqRYwUaTTOa.A3iel66XrA6qee0S25mipSYoMpbWpPVbeXe',37,18),('jillian.ylagan','Jillian','Ylagan','$2b$12$U46VOmOEt5RjlgW202cxleT.ZEQaZo5bsrbayX251gTbng5H3Pq5C',23,18),('joe.grilli','Joe','Grilli','$2b$12$NMwEnCCVGnr.g4/LefgYDuku/jxhOkivrGcCa.fa.kBMK2QZHovHi',39,18),('joey.desai','Joey','Desai','$2b$12$9totaRf/DyA/BncPbmcUxOzF.0xHU.uXgh/hxtgSe7DysxImL.Gka',3,18),('johan.lee','Johan','Lee','$2b$12$jruXT73nVjWCgZEo4K3yB.E1VwZ0qD/j1k2RylEtEPsieO16RwjQC',23,18),('john.beutner','John','Beutner','$2b$12$sAwUfgQxrzpRG7Vc9u8kg.XB2wfrenqQzaoIDrXCuN9XDq7AZ4gNC',33,18),('john.siegel','John','Siegel','$2b$12$n4U.ilI4r9348pTVmhq0IumyEqqmNxFWqFBbW6cLhPwDhjcSUCfPK',35,18),('jonathan.woody','Jonathan','Woody','$2b$12$hshgqwiaPguV6bDO62kQ9.kHSsJwSTNWOELTCK4xEDdmJcgxwknAG',14,18),('jordan.teaford','Jordan','Teaford','$2b$12$RceZyugvxvy9vWq53W9ihORYHD6z/uu6LgILxq/FW7njiUB2hq/zK',10,18),('joseph.o\'such','Joseph','O\'Such','$2b$12$mpOR5bNfTnmrw.ee3.wXEea5LWz.2qMnJ5Uc4byOzz4eJFgGp5Ccm',14,18),('josh.marcelin','Josh','Marcelin','$2b$12$SR6d46oOdY.UMTQVEEZonO6Sncpoq1UgwCQND4z7g9Kb2sv/8QC4q',26,18),('josh.martin','Josh','Martin','$2b$12$Q32vGjNXUusGt5JO6uqAPObHBDNJcmQuLY16aal73SwBAtfcHzGbO',50,18),('julius.colby','Julius','Colby','$2b$12$XK5qQLO42ESxg9XxwgTV1uk6Jsy.nLSVmyxlnibcmE.1Q0piEZjyC',11,18),('justin.nelson','Justin','Nelson','$2b$12$7J9GZvA.ActWaaVre8pKEu7oWZ5rH87Waktaf8r8G2H4caCBT12im',52,18),('justin.turkiewicz','Justin','Turkiewicz','$2b$12$ka3MVpIa0.CJNhXCqvSfduP4d4Cy6txvKYhee4KI5r9.fW6ua5sgu',5,18),('karan.shah','Karan','Shah','$2b$12$61UbZMYGPm9jmOcgJOFGPOdFzI6CWMMU9.mJTTapVkslEQzpzJOh6',3,18),('katherine.fichera','Katherine','Fichera','$2b$12$q3LlKscZycCxhQMoQItQr.ApJ8KoNSVn2y6519NGukr9WwkfUk9Ba',46,18),('katie.geibel','Katie','Geibel','$2b$12$PDmAi3H5AM6eCQqR6ejhlu76MZrI1HaPSubKzRAy8wKKna59Jluku',45,18),('kavya.polina','Kavya','Polina','$2b$12$5BBgDS5QdxmoraQ4JYFAsewR2IG0Q7Cta/aMkmL1m8a5IhNJ8b09m',27,18),('kevin.d\'alessandro','Kevin','D\'Alessandro','$2b$12$JH0aiUYYQDbZXG64kXoVhOyzv.AjaqeI48XpByUgApYOzqf/lTjG6',18,18),('kevin.he','Kevin','He','$2b$12$HK5L7vWSa/BRWWY/fNLj.un6GpDR6xitF6Y4eJ2T10qI3N2mhfaO2',26,18),('khanh.pham','Khanh','Pham','$2b$12$2lUMGIVnY84LhiNyXZrga.nQCfQ6hERgdM40VHEUNyhilVfnMsH0S',44,18),('kieran.siek','Kieran','Siek','$2b$12$odHPM9DJRqxYvO937hM7VOyxwl5REjj1PglX9crh//scOxUxUIusK',33,18),('kristian.braun','Kristian','Braun','$2b$12$qwOjhGaU5Fg7/kcEzTnx6e2i7vnUSXylw9MWcJLMJnlNL/Qbtup6i',10,18),('kunal.nakka','Kunal','Nakka','$2b$12$gEt94H01xYdZTHeEPb5gkuB4tgrvOdFmkAwc9ESSi0a/dvCQTMQge',34,18),('kurt.grzybowski','Kurt','Grzybowski','$2b$12$51fFhcgYMjz1IO3xBf3Bbu2.AROXwQIkEMQQBtXtV4hdkXDLRQw2a',57,18),('kyle.hilgenberg','Kyle','Hilgenberg','$2b$12$zUj42FPkVs.H/yyECmWybuQk9f3eN5PjW2dGWVup91kw57ZNXuEg.',17,18),('kyra.forest','Kyra','Forest','$2b$12$hWIw07XiKFwC2kj9jXvEouGTpq5Nx9lR9dK66nfbNOcdXHRxOI4Y6',9,18),('liam.mcbride','Liam','McBride','$2b$12$i0.QA5yuZKIDHaSChRkj3uCf3WitDV7qmK9z4OD50Wdu.mb3baO0a',15,18),('lily.chiang','Lily','Chiang','$2b$12$ktF4bfPbdK6ww2E9jundpeICFtFNiUe2q1iPzy2gneQ4k06l0qd7y',27,18),('lily.khochareun','Lily','Khochareun','$2b$12$uke9zvvtKgmLemHd5DijyO.2/N6gxpZTzhEWrN8QN7zypkbGnns4u',56,18),('lnatysin','Logan','Natysin','$2b$12$Xdt6RkLeYECWvy6usq5aLOLO5bvZnmOY2iHa7wUcB7EhbdUmqz912',NULL,18),('long.phan','Long','Phan','$2b$12$gAiYeqw4aRSG1E0uMqhuxessCZVZ.v7LEzsAwK1WA1xMbttZCn3la',42,18),('luke.digiovanna','Luke','DiGiovanna','$2b$12$IR.vcReEcQGNC0.ZvCDDEOVSvCksyP4uWv6xB.DK/JRMU0yjdXvLy',35,18),('luke.marks','Luke','Marks','$2b$12$pqriywLjG4D1jmoENfxBje8fZ9zjdUbby5aX3xyXCPLoxiILwXyL.',21,18),('luke.wevley','Luke','Wevley','$2b$12$L.pOrF9iDH1Ngmyy2j51suHnBUjEZAEqfpljlAGd/pxey5dofyYTu',41,18),('maddie.gonzalez','Maddie','Gonzalez','$2b$12$hINcmO3sgYVC/OP08zVMKumradXBAR6j.HuL0lh1csPFkI1gEIxxi',1,18),('mahlet.zemui','Mahlet','Zemui','$2b$12$/132rDATk.lCvmH/5dlVBezynASIsWAsz2gQ6LC.G17SQB5q5SjkS',32,18),('manasi.peta','Manasi','Peta','$2b$12$ONkE2y6HzmN1icd0WLmPF.NZb1fEU9m9qBUMwt9SNBs2ns9Lxoucu',39,18),('mark.fuentes','Mark','Fuentes','$2b$12$L4UU4CjPB8F7Fwkx5LXiQefLrjrnrHP1oYasFryrR8AgtFyVCg2Z2',1,18),('mathew.padath','Mathew','Padath','$2b$12$FFANlJ.jwWZhhoQQzXbMR.AlpnUrdGueUz7CjXOfAwjY6UAlLNFAO',29,18),('matthew.eberhard','Matthew','Eberhard','$2b$12$0UWTT8NXJjf1fHhISwrPHeJ1DdSAtsAzm4imw2et8EpaLTkk6HHz.',52,18),('matthew.walters','Matthew','Walters','$2b$12$7ocyuUPb73rNxFNM.ysaPewy1wlIrYJYofY70nA5PzbW1DYz7tFpy',5,18),('mauricio.lovelace','Mauricio','Lovelace','$2b$12$zLmXbx.iWcOMa0ADKP1g.u7RTEUbkcao2L3zkJ7k7uGZ/2WWjEMW.',51,18),('mia.hagood','Mia','Hagood','$2b$12$W9qDtRKYjzDZEK5dG/74kuS2PGTtBZkcAOQ0Cj1S7SQpvkHr4DHS.',13,18),('michael.blair','Michael','Blair','$2b$12$d5BEX7mor9WSjpyk2pZUfuPq./LLgsQvpfEufmxFcNeF4zgDN40AG',47,18),('michael.nader','Michael','Nader','$2b$12$vksdV4LT8rSqrRdzq8ycfuMf.QwebrqdCKSSQ2Eh6xhc.n4T5y2E.',41,18),('michael.shi','Michael','Shi','$2b$12$FnnZP24sjT9CvjnulVYfBuiUPccOjwrsF.YQhOkTxq3LDaDTTKrGq',17,18),('michelle.hong','Michelle','Hong','$2b$12$q/SOn9BDZxug1G9yrLTZmuWdX9plYK6Y2N.WZr203CcYz/Mv0mXri',10,18),('miguel.lopez','Miguel','Lopez','$2b$12$sxUB29EMiDGBYMgnhHlTHevRZ6TB6g4pKZQvCSOWPTpE2QKklf1c6',22,18),('mihir d..gathani','Mihir D.','Gathani','$2b$12$lZ8KIGV4eHqn8KwN5U3ZdumV7tjptIBRg4ILq4K3hvywe6Nmju3SS',38,18),('miles.jackson','Miles','Jackson','$2b$12$A8uScJv2xDPivVT3oUH05.2XD9RbHOdTL7Rzp7WmZbKU8rQYYJZ3a',19,18),('mingkai.pang','Mingkai','Pang','$2b$12$B14tUKE/F8XtDSCFFg0d9uyTjuNCWTy/GugYnkQfwwWDEYswWT8CS',37,18),('minwu.kim','Minwu','Kim','$2b$12$YWtUPt4tS.zIFH7c6LIrtub33VNb47vXlfW1O/JimLMAPtab852YO',23,18),('mitchel.rifae','Mitchel','Rifae','$2b$12$eQ99NdNjJloxAxyx/WNwc.SCLiV2.7/TzqNpUphm0.dJ9L1/b0a2m',30,18),('mitchell.campbell','Mitchell','Campbell','$2b$12$3pI2V7Jw/Dj2fo2QFkxjJO9wR1bVesGL3.f6FI7OfGlHmtEYND/hu',43,18),('mohamed.farag','Mohamed','Farag','$2b$12$5iQHnOQ69fiMLDdbQhg9A.uNFAi4snIVmaWe9DNI280NFJkQKk/pG',26,18),('mohamed.kanu','Mohamed','Kanu','$2b$12$XGXoxOsjXBQJfOwFy.C6pOubCK7oZKYfUMrbpIHBBwR4J1i6ko6ga',53,18),('mohammed al.ansari','Mohammed Al','Ansari','$2b$12$1nB0P9c1pZZ/lngC.TTRYernfc3lyEzkqZHoRSUPnP/MpHZR1pr3m',44,18),('nandha.chokkan','Nandha','Chokkan','$2b$12$0qGCkKGFqdUftpvgPn7QoecW1VvIkW41FtLbAmZjrlsSvdn9o4LEm',39,18),('nicholas.cho','Nicholas','Cho','$2b$12$ZlTdUxltfnDXZzGrT2DM0uuGjeMZLjQW7xj1joD16XAyuuHqA7IGa',55,18),('nihar.satasia','Nihar','Satasia','$2b$12$MRX6HmuTcudUFpZ2bGW3.OWQa/AEhYnv3OKLZQqPMwA0LogDaOEXa',21,18),('niko.olson','Niko','Olson','$2b$12$w8kX7werynYGDDfO1G7HUuS7K8xrKOsHe8VZ.K/WjpyPTxBuIZo9e',51,18),('nishesh.saikrishna','Nishesh','Saikrishna','$2b$12$Ak55E7A1MZCvfYPrRiP1m.27SdflNhWYTWf28dAGCG8yySDf5c9Ve',2,18),('parsa.nikpour','Parsa','Nikpour','$2b$12$hrewcbZSra.QBnMI7Ds7eu0TX5t0niS4VwzjTSYlGzMi/lruygPRG',30,18),('parth.desai','Parth','Desai','$2b$12$5DfgQz1qadtst.mwHDyiNe2fl4m6krFTMMjWNiy3wM35N5nXSy1ce',40,18),('patrick.johnson','Patrick','Johnson','$2b$12$QBtiKI8MoZmjzMfEuikYUO7.CGVDKDhDDz7gGr./W4FzLbyZC6lu6',25,18),('patrick.warner','Patrick','Warner','$2b$12$qVqWzwPFo4etlzDCDwlKGOfXEaCBwmvPxvjrpMl73Iu0rPmAC3Tq6',13,18),('phu.nguyen','Phu','Nguyen','$2b$12$MmDQ2E2Q7AsxBmsDhBt7rubHXnPgtvNqBRnis7RZ590hLUWi9B6Va',3,18),('pierre.tran','Pierre','Tran','$2b$12$n4pgWXdDDxwRq5pg2Ckv6.YlkdBpi/FpRQarFPC6a9ezR8Sdj3XGy',6,18),('puchuan.song','Puchuan','Song','$2b$12$m9HMf.qEs13f8p8c/cqkPOxObAYiNs/iEU9hEh5ciKLxuMRpF0tQ6',22,18),('qianxiang.hao','Qianxiang','Hao','$2b$12$7HOxk4wfqkvtTLy5mf8znOvbT2FVfat.xMppJXuq1.vctAEBZIl3G',16,18),('rahul.aneja','Rahul','Aneja','$2b$12$eJ/k25MbqJ/SecaHBo./ReJoVSASGZ4stRzlH8HvA.1VTgZZOOshm',32,18),('rayyan.hashmi','Rayyan','Hashmi','$2b$12$ZnPegnH742uAKhOHPRoTqO79N.ZhXmPrYUWpn6Eyb6i1D2Md39UT.',7,18),('reema.daniel','Reema','Daniel','$2b$12$9LpESuRBs/kRFETrJAuG7emU4XPL6ufqsgbEKTHTknxzrlMA8YXaq',38,18),('renzo.neps','Renzo','Neps','$2b$12$Nx14Nm7I9tSZ2lkPBR8P9en7SVB2UGNVJXF9nGtN1j.9gn0BXJZfG',15,18),('roshan.ravindran','Roshan','Ravindran','$2b$12$oTudC0g5RIe5qspEMLda7uIlvGaiPlhyvvCpU1f8E8QA/Dibd0rGW',36,18),('ryan p..gniadek','Ryan P.','Gniadek','$2b$12$PO.iFNtCLNTN1DdVRBKdMOlkJf83BXY.K31wnc4eXuLVJVzoa5hia',48,18),('ryan.knight','Ryan','Knight','$2b$12$b7a26Ryj4Acqbn7wn3nRQuhqcQYbR9Aov8XnnobZRxC5ujL5v4Fsq',3,18),('ryan.mcgovern','Ryan','McGovern','$2b$12$/PqRULoffpZ1w1piVULaAu2lrSuY7rHu81PcW1yp42YH5Km/NwvWa',29,18),('rylen.bass','Rylen','Bass','$2b$12$BknQYIEGuaWsypLMsQcqK.ZbXRmIDrrwEdmyGgHvfA1qMxCSpmTJW',23,18),('safa.kamran','Safa','Kamran','$2b$12$PcG/VFiBOysnOIHloC2HqOnBvBinVHz.cD9jaDXigSDPdrR8rDHNW',47,18),('sahar.farzanehpour','Sahar','Farzanehpour','$2b$12$SyQ6XT1uUTRsacvLzb2vl.gm7Uc03D05D3zrcIV6.3JVvqatGQJcS',56,18),('sai.duduru','Sai','Duduru','$2b$12$duwodZ.TsctM4McDwxXjMO.TinMMw3J4CCdxi0jPCJ9M3ersPIXLC',9,18),('saketh.rajesh','Saketh','Rajesh','$2b$12$Kbz.wb78NyncAYwMV1XTx.ylK0.dmKxoHoJjCnKYT5HXkMkwgI8u2',17,18),('santiago.guadiamos','Santiago','Guadiamos','$2b$12$Ze2YiXx8ri2OwOe1H4Ln5uVESStC5cvNpNYAgJp15TjAcrPppaEqi',4,18),('sarah.burnett','Sarah','Burnett','$2b$12$ht4ZEzq4XPAzew517FGgw.Zoa9zyQ9PozZJ2eUZxu3QixuC1JHAZW',36,18),('seth.robertson','Seth','Robertson','$2b$12$Ks7wEVwNu98xXXNmfxZMDu6i9fpg6v.2iJJw.VHjkIWUseOckIXXK',53,18),('shayne.perryman','Shayne','Perryman','$2b$12$p5xltEo0tQQDBBnvFntEaef/Dzbhc7jbPIArTMShvw7n4VKN8.xsq',1,18),('shreya.mallamula','Shreya','Mallamula','$2b$12$M7z6Hk5Uh3Dgn8siyHZv3eR41qsb2F/TL4ijVil3qDeedeaJh89zi',53,18),('shreyas.sakhalkar','Shreyas','Sakhalkar','$2b$12$p79yiiByeJwBuEMjO6yk2ORbbnLdAAGitwvvWY3UA0CDgNBtuURZW',21,18),('smera.sheik','Smera','Sheik','$2b$12$4eqMwMK3RuoJe/CrYraQduM0M0/RSL8pq1TznfpmDRElkcIL7C/BG',20,18),('sondra.rathje','Sondra','Rathje','$2b$12$McqHA0F8iFISypo4k7wbWe2woVymDBq4b47VuZIX/csVGX.IKq5u6',10,18),('sraavya.gudavalli','Sraavya','Gudavalli','$2b$12$TZHNXtoHmqxqu6T1y2X0se2SyH6eIujh.w0yi22Hb/Putw4lADsGG',48,18),('srikaran.bachu','Srikaran','Bachu','$2b$12$HrRKShWPmARnd7rFINAvW.952819mafxxC0ObCk45oKstazN54.my',34,18),('stephen.angeley','Stephen','Angeley','$2b$12$wpyfoaDkD7XUyvfAFRRX3e0yP.7/koN6gAYXLXFwgadQ4/ClPpiV2',10,18),('steven a..ruckert','Steven A.','Ruckert','$2b$12$0pE3u43flcC3p4zrxynipuL/x8qRG8CXEvyEGNr6odeDzuD2zGpI6',48,18),('sully.gregory','Sully','Gregory','$2b$12$CBwV0luFpOUBsh/A2X3oTuSjYsQU3vDmNWq2aV8fzy/9enQIZMkFS',2,18),('sungjeon.choi','Sungjeon','Choi','$2b$12$UB9i5zgXlXH4cNdwSMiPcO3LMu1i.wNRWDLRLs4MuP0ub1sR3fHzG',47,18),('sushen.kolakaleti','Sushen','Kolakaleti','$2b$12$EXh1pVl8cTJOdK11UCAowuU/zu5aptJgTQXSf9dLMkip32gD9dE7e',18,18),('tai.nunez','Tai','Nunez','$2b$12$fkOqpvWXTsT92ejiMJzZO.S7LLe/ZQVNlZUIKL9k8hIjfk0xPYVW2',28,18),('talia.blakemore','Talia','Blakemore','$2b$12$Lv9nA5dddsiE9m5xGJYJ5OFNKB1MwC6Vjn4Ta8.gpBPPYZYquDw8G',42,18),('tarek.shah','Tarek','Shah','$2b$12$alWmqUQODXD.GBAXmqKTfOUw9fqhagHcEOHy8U7e5h5TtEXjmkLs6',4,18),('theo.ouzhinski','Theo','Ouzhinski','$2b$12$PehGg0VRbFkhwKCmlnM0r.SgJOxXTuS45ur4eNcx406n.qRltr8YS',45,18),('theodore.gunn','Theodore','Gunn','$2b$12$VyGJVUowtYwi.R3MdCdzpueWiLftPqXg.45APSN6ESv7qqvtzxGYW',41,18),('thrilok.shivaraman','Thrilok','Shivaraman','$2b$12$JBIt6X6mcbI8W26WJMC6L.ooV4C/Iaj42FpOmDegR5X78pyhejy5K',45,18),('tolu.akinyemi','Tolu','Akinyemi','$2b$12$.H5benaQzsXPxgmoFlKkL.LemrJlnm8D4PaOlSmnXWQoBhD1Mdxpy',49,18),('trevor.kappauf','Trevor','Kappauf','$2b$12$zGBU9Ia8s5lBgMjOv7490uRojT57Fg4kPvZP2WV4q.zaKWiqWE2R.',30,18),('tsion.woldeab','Tsion','Woldeab','$2b$12$JxjHhW.fRxDOAUIUf320HuO7PBItE2sPWN3jww.7BiVetpqjgKJXu',49,18),('tuan.chau','Tuan','Chau','$2b$12$S/okilkcNseeYNKDMTKgsu8W623/GKk0oBeAimD68bEB3Rmh4w9BC',27,18),('tyler.buxton','Tyler','Buxton','$2b$12$ihWFetNofcocUpu23Z.szOwNSpN93NZCYt/8fjWzNqPdciSiUizny',25,18),('vaasu.agadkar','Vaasu','Agadkar','$2b$12$8I4hipw8UdRoRHYRx6aEmudRds5u/v6/tMm4AbFS.5PUZal6cgwAS',9,18),('vaishali.ramesh','Vaishali','Ramesh','$2b$12$yamncdfeqHuFGyCdqzQetuiQWbrA1rsQ.jt7sxL0NoZLRkDj7xqt2',38,18),('vaishnavi.alavala','Vaishnavi','Alavala','$2b$12$vftCwiQ1.tK1NG8x4z/QCugI.CPgtjRJ/oi.Z/8gG9tmG2LTFi6wO',20,18),('van ha.nguyen','Van Ha','Nguyen','$2b$12$svt4IykGf2uvX35M/YnQY.boqnRcdhGbZ3f037g.M6RcECBMRJqxi',36,18),('varun.choudhary','Varun','Choudhary','$2b$12$V/gnFS3gsy9FCfRrY5Gz8ekXOwYSyJVRUorDO3HG0lpESPKjjURnK',40,18),('vedant.shah','Vedant','Shah','$2b$12$nSvSRuviYaSTrXhxI0rMSuwJUqfAszOmIqlLHLR161FVYqqd932y6',38,18),('victoria f..hardy','Victoria F.','Hardy','$2b$12$NfH.O1/7IgkNtnVDd6ZCFOis8IQ5gXtQOniVIcNcaEYPzxVPNkUcu',48,18),('vincent.diperna','Vincent','DiPerna','$2b$12$49D2.YUpQlUJGBkGjr0hae48.SRuaYsadyxbWqNivLGzliyQX9SAe',47,18),('wenmiao.wang','Wenmiao','Wang','$2b$12$UQIsqboOqi5Idw37LIgw9ODWyCO9QtekSQXJkc518DoJ6kxOB6e8i',29,18),('westin.jiang','Westin','Jiang','$2b$12$F3ZL4zlzVdHkCiF.A8xoD.il28PWvkxh.temCEUAiXDX4FTHPHpV.',29,18),('will.hillhouse','Will','Hillhouse','$2b$12$4dL/xyECAeoSPsjqiOd/a.ViP/Jtay3GW76W8fk5UuOSYERgLW/6a',52,18),('will.spies','Will','Spies','$2b$12$klY27LXODylpSVDGLwpiTOLlik/rTZiI2HPCixGR8unDMpqzTbrdO',2,18),('william.bogusz','William','Bogusz','$2b$12$GwSXYpXzB2HMGCf9UUoCNe3Qs58KzkA9ar/hFLzRPqmD6vKTBveP2',8,18),('william.denman','William','Denman','$2b$12$vYh1sCsYmwP3jYkKKCpR3.ZWC1PfuYbPjyubP9HvzQl8FGA2OiNK.',45,18),('yifei.wan','Yifei','Wan','$2b$12$vR6PxZcAiVqHHm/jKgP3DumWkoI7UMhK9jV9OcyuRtFEeIfBgq6Ty',29,18),('yinhjie.zhao','Yinhjie','Zhao','$2b$12$esIW6Ql/CFwmu1yd4Mklq.7O.QwcC5Y/Rhoip5.m6vxUyhH1alTIq',19,18),('yulong.liu','Yulong','Liu','$2b$12$kVt1/zKeDr.BoZhyBEANzuEa2s4TZ0BZC2xvxb9TDnmw4fLvkPpLu',37,18),('zachary.shaffer','Zachary','Shaffer','$2b$12$xWlwxhpC3IO8tqueh0RELeJKqBQGX2n4VYcafmTVNF27AS.Xd2HEG',43,18),('zehua.zhang','Zehua','Zhang','$2b$12$HUJy5R0GdCtYiukvQgC0oOm25E5wvA/Vfc/Y3DzCzq/hioLF8TpXO',41,18),('ziyan.wang','Ziyan','Wang','$2b$12$p9bvL/15aeCzbMNFL7PiMOgPYrkQS9YIdqz4HxMvMoNqLqCseVBwq',24,18);
/*!40000 ALTER TABLE `students` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2024-11-14 15:02:38
